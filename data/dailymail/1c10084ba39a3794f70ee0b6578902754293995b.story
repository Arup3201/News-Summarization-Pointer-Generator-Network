Facebook has issued tough new guidelines on research using its system following outrage from users over experiments on the social network.

The firm today tightened its research guidelines following uproar over its disclosure this summer that it allowed researchers to manipulate users' feeds to see if their moods could be changed.

At issue was study in which Facebook allowed researchers to manipulate the content that appeared in the main section, or 'news feed,' of small fraction of the social network's users. 

Scroll down for video 

The California-based firm carried out the experiment during a week in 2012. During that time, negative posts were deprioritised in the data feeds of 689,003 users, to see if it generated a more positive response.

The California-based firm carried out the experiment during a week in 2012.

During that time, negative posts were deprioritised in the data feeds of 689,003 users, to see if it generated a more positive response.

Posts were determined by the experiment to be positive or negative if they contained at least one positive or negative word.

The experiment affected around 0.04 per cent of users - or 1 in 2500.

According to Facebook, nobody's posts were 'hidden,' they just didn't show up on some feeds.

It found that negative posts elicited a swell of positive responses, but also that a reduction in positive news led to more negative posts.

‘When positive expressions were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred,’ said the researchers. 

During the weeklong study in January 2012, data-scientists were trying to collect evidence to prove their thesis that people's moods could spread like an 'emotional contagion' depending on what they were reading.

'Although this subject matter was important to research, we were unprepared for the reaction the paper received when it was published and have taken to heart the comments and criticism,' Mike Schroepfer, Facebook's chief technology officer, wrote in a blog post.

'It is clear now that there are things we should have done differently.'

In the past three months, Schroepfer said, Facebook has given researchers clearer guidelines on research procedures and has created an internal panel that will review projects. 

But there will not be an external review process and Facebook will continue to encourage researchers to study how people use its site.

'We believe in research, because it helps us build a better Facebook,' Schroepfer wrote. 

'Like most companies today, our products are built based on extensive research, experimentation and testing.'

‘The reason we did this research is because we care about the emotional impact of Facebook and the people that use our product,' Facebook data scientist Adam D. I. Kramer said at the time of the study.

‘We felt that it was important to investigate the common worry that seeing friends post positive content leads to people feeling negative or left out.

The experiment affected around 0.04 per cent of users - or 1 in 2500.

‘At the same time, we were concerned that exposure to friends' negativity might lead people to avoid visiting Facebook. We didn't clearly state our motivations in the paper.

'Having written and designed this experiment myself, I can tell you that our goal was never to upset anyone.'

During the experiment, Facebook deprioritised content in News Feeds, based on whether there was an emotional word in the post.

Tests affected around 0.04 per cent of users - or 1 in 2500 - for a week, in early 2012.

According to Kramer, nobody's posts were 'hidden,' they just didn't show up on some feeds.

'Those posts were always visible on friends' timelines, and could have shown up on subsequent News Feed loads.'

It found that negative posts elicited a swell of positive responses, but also that a reduction in positive news led to more negative posts, according to the results of a study published in PNAS Journal. 

 

 

@highlight

California-based firm carried out the experiment during a week in 2012

@highlight

negative posts were deprioritised in the data feeds of 689,003 users, to see if it generated a more positive response

@highlight

Experiment caused uproar among users 