By
Daily Mail Reporter

PUBLISHED:
  
  
      07:57 EST, 17 May 2013
    
  
 | 
  UPDATED:
  
  
      10:55 EST, 17 May 2013

A study by a top university has claimed internet giant Google’s search facility 'perpetuates prejudices'.

The investigation from Lancaster University found that results from Google’s auto-complete internet search tool produce suggested terms which could be viewed as racist, sexist or homophobic.

The study by at team at Lancaster University’s Faculty of Arts and Social Sciences comes as a German federal court has told Google to clean up the results its search engine suggests.

The re4sult of typing 'why do black' in Google in the UK reveals a list containing offensive answers

Typing 'why do gay people' into Google reveals this list which also includes offensive answers

Autocomplete is an automated system.

As you type, autocomplete predicts and displays queries to choose from.

The search queries that you see as part of autocomplete are a reflection of the search activity of all web users and the content of web pages indexed by Google.

If you're signed in to your Google.

The court has said Google must ensure terms generated by auto-complete, which represent the level of questions people are asking, are not offensive or defamatory.

The FASS study found some shocking results in its UK study, which drew out more than 2,600 questions on the Google search tool and categorised the answers.

And it warns that 'humans may have already shaped the internet in their image, having taught stereotypes to search engines.'

The research revealed high proportions of negative evaluative questions for black people, gay people and males.

For black people, these questions involved constructions of them as lazy, criminal, cheating, under-achieving and suffering from various conditions such as dry skin or fibroids.

Searching for 'why do men' reveals the answers cheat, rape, go blind and get morning glory

Searching for 'why do women' also reveals offensive answers

Gay people were negatively constructed as contracting AIDS, going to hell, not deserving equal rights, having high voices or talking like girls.

The negative questions for males positioned them as catching thrush, under-achieving and treating females poorly.

A Google spokesperson said the system was entirely automated.

'Autocomplete is a feature of Google search that offers predicted searches to help you more quickly find what you’re looking for. '

The search queries that you see as part of autocomplete are a reflection of the search activity of all web users and the contents of web pages indexed by Google.

Autocomplete predictions are algorithmically determined based on a number of factors (including popularity of search terms) without any human intervention.

Google also allows people to ask for offensive terms to be removed from the service.

Professor Baker’s said: 'It seems as though humans may have already shaped the internet in their image, having taught stereotypes to search engines and even trained them to hastily present these as results of ‘top relevance’.

The US according to Google's auto-complete function

While it is disturbing to see negative questions appear for any social group, it is especially worrying to encounter them when they refer to groups who either constitute a minority or have been subject to oppression either now or in the past.

The higher amounts of negative questions (both in terms of proportion and frequency) for black and gay social groups are thus a cause for particular concern.'

The report recommends Google introduces a way for readers to flag up auto-complete suggestions as problematic.

The study by at team at Lancaster University¿s Faculty of Arts and Social Sciences comes as a German federal court has told Google to clean up the results its search engine suggests

Examples of offensive auto-complete options include when typing 'why do black' suggestions of 'why do black people have big lips' or 'why do black people like chicken' are made.

The phrase 'Why do gay' is met with a suggestion of 'why do gay people get AIDS'.

Prof Baker said: 'Many websites offer the facility for Internet users to flag user-generated content as problematic by clicking on an icon.

For example, in the video-sharing website YouTube (owned and operated by Google), people can comment on videos, and then readers can mark those comments with ‘agree’ or ‘disagree’.

'If enough people disagree with a comment, it is hidden and an additional link needs to be clicked on for it to be viewed.'

Europe according to Google's auto-complete feature

The report concludes that advances, which have been put forward in order to improve or personalize people’s experience of the Internet by predicting the sorts of things that people will find interesting or want to search for, have unintended consequences which can result in the perpetuation of negative stereotypes about vulnerable social groups.

Prof Baker said: 'Most people would probably not wish to ask Google about social group stereotypes. However, enough people are doing so to cause auto-complete algorithms to offer these questions.

'Should content-providers ‘protect’ their users or should they simply reflect the phenomena that people are interested in?

'And if content providers do choose to manually remove certain suggestions from auto-completion algorithms, who decides which suggestions are inappropriate?

'Decisions to remove certain questions could be interpreted by some Internet users as censorship and result in a backlash.”

@highlight

Team analysed more than 2,600 questions on the Google search tool and categorised the answers

@highlight

The research revealed high proportions of negative questions for black people, gay people and males