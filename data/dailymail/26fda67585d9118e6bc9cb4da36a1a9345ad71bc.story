By
Victoria Woollaston

PUBLISHED:
  
  
      06:32 EST, 7 March 2014
    
  

 | 
  UPDATED:
  
  
      10:50 EST, 7 March 2014

A breakthrough technology that helps blind people 'see' the world around them using sound is now available in a smartphone app.

The vOICe system converts images taken on a mobile into 'soundscapes' by assigning different musical notes and pitches to different shapes. For example, a diagonal line - such as staircase - is converted to a string of rising musical notes.

The app, available on Android, uses the phone's camera to record scenes and landscapes, and teaches users how to identify which sounds go with which shapes.

There is even an accompanying app called EyeMusic that adds colour to these shapes.

The vOICe system converts images into 'soundscapes' by assigning different musical pitches to different shapes. For example, a diagonal line is converted to a string of rising musical notes. The app is available on Android, pictured, and explains how users can train themselves to identify the shapes

During tests, head-mounted cameras were attached to the blind participants before being wired up to a computer and microphone.

As the participants moved around the room, the camera took a photo and converted into a 'soundscape'.

It uses pitch for height and loudness for brightness by scanning the room from left to right. 

A rising bright line is converted into a rising tone, a bright spot - such as a lamp - is a beep, a brightly filled rectangle - such as a window during daylight - becomes a noise burst and a vertical grid  - such as waffle or trellis - is converted into a rhythm.

Users are trained to identify which pitch relates to which height, for example. They then listen to, and interpret, the visual information 
coming from the camera - effectively 'seeing' with sounds.

After
 being taught how to use the technology, blind people could use the devices to
 learn to read, with sounds representing visual images of letters, for example.

The EyeMusic app uses the same principles, but additionally assigns different pitches to colours.

The vOICe system was created by Dutch engineer Peter Meijer in 1992.

It uses pitch for height and loudness for brightness by scanning the room from left to right.

A rising bright line is converted into a rising tone, a bright spot - such as a lamp - is a beep, a brightly filled rectangle - such as a window during daylight - becomes a noise burst and a vertical grid  - such as waffle or trellis - is converted into a rhythm.

Since 2007, neuroscientist Professor Amir Amedi from the Hebrew University of Jerusalem has been training blind people how to use this technology.

With around 70 hours training, Professor Amedi's participants recognise the presence of a human form.

They are also able to 
detect the exact posture of the person in an image, and imitate it.

Using brain scans, Professor Amedi was able to establish the outlines and 
silhouettes of bodies cause the visual cortex in his participants' brains - an area specifically dedicated in sighted people to processing body 
shapes - to light up with activity.

The vOICe for Android app makes this technology widely available. It uses augmented reality to convert live camera views to soundscapes for the totally blind through sensory substitution and computer vision.

Using vOICe, researchers have also created EyeMusic for iOS, pictured. It translates black and white images created by vOICe into colour. Colours are represented by different musical instruments - higher pixels are translated into higher notes while lower pixels are translated into lower notes on the same instrument

Researchers first taught people to perceive simple dots and lines. Then those individuals learned to connect the lines with junctions or curves, gradually working up to more and more complex images, pictured

The app also features a talking colour identifier, talking compass, talking face detector and a talking GPS locator, while CamFind visual search and Google Goggles can be launched from the vOICe for Android app by tapping the left or right screen edge.

In Professor Amedi's most recent work, he has taken this a step further and created the EyeMusic app for iOS devices.

It uses an algorithm to translate the original black and white images created by vOICe into colour.

Colours are represented using different musical instruments - higher pixels of an image are translated into higher notes on a given musical instrument, for example, higher pitches on the piano, trumpet or the violin, while lower pixels of an image are translated into lower notes on the same instrument.

Video: How the vOICe technology appears to blind people 

The
 researchers from the Hebrew University of Jerusalem first taught people to perceive simple dots and lines. Then
 those individuals learned to connect the lines with junctions or 
curves, gradually working up to more and more complex images.

Professor Amedi's team taught congenitally blind 
adults, meaning adults who were born blind, to use what's called sensory substitution 
devices (SSDs).

For
 example, when a person uses a visual-to-auditory SSD, images from a 
video camera are converted into 'soundscapes' that represent the images.

This allows the user to 
listen to and then interpret the visual information coming from the 
camera, in that way 'seeing' with sounds.

After
 being taught how to use the SSDs, blind people could use the devices to
 learn to read, with sounds representing visual images of letters.

This
 skill involved a region of the brain called the Visual Word Form Area 
(VWFA), which in sighted people is activated by seeing and reading 
letters (pictured)

After
 only tens of hours of training, blind people's VWFA showed more 
activation for letters than for any of the other visual categories 
tested.

Pixels closer to the left side of the image are heard before pixels closer to the right side of the picture letting blind people determine shapes, colours and the position of items.

The research has been reported in Cell Press 
journal Current Biology.

'The
 idea is to replace information from a missing sense by using input from
 a different sense,' explained Professor Amedi. 'It's just like bats and dolphins use sounds and echolocation
 to 'see' using their ears.'

'Imagine
 for instance a diagonal line going down from left to right; if we use a
 descending musical scale -going on the piano from right to left - it will 
describe it nicely," continued Professor Amedi's colleage Ella Striem-Amit. 'And if the diagonal line is going up from left to right, 
then we use an ascending musical scale.'

@highlight

The vOICe technology was developed by engineer Peter Meijer in 2007

@highlight

It turns images into 'soundscapes' where shapes produce different noises

@highlight

For example, a diagonal line is converted to a string of rising musical notes

@highlight

Professor Amir Amedi has been training blind people to see using vOICe

@highlight

The vOICe Android app uses a phone's camera to record views and scenes

@highlight

The EyeMusic iOS app uses vOICe to help blind people see colour

@highlight

Colours are assigned different pitches in the same way shapes are