For a child, the world is a heady mix of colours, shapes and words that are constantly being learned and remembered. 

To make sense of many of these experiences, babies and toddlers 'map' names to objects by assessing their surroundings and associating words with the item's location.

And now a new study has found that this association is not only affected by the child's proximity to the object, it can also be influenced by their specific posture.

Scroll down for video 

Cognitive scientists from Indiana University taught a robot (pictured) to associate the name of an object with its location, while sat with a certain posture. When the target and another object were placed in the location, and the robot was positioned in a different way, it failed to recognise the object

Cognitive scientists from Indiana University began by assessing the learning capabilities of a child-like robot called iCub.

They showed the robot an object situated to its left, and a different object on its right. This was repeated so iCub learnt to associate between the objects and its two related postures.  

The robot consistently made a link between the object and its name - and proved this by reaching for the object - during 20 repeats of the experiment. 

But in subsequent tests where the target and another object were placed in both locations and the robot was positioned in a different way, it failed to recognise the  object. 

This experiment was replicated using children aged 12 to 18 months and the researchers noticed only 'slight differences' in the results.

As with the robot, the infants failed to recognise objects if their position and posture changed, or the object was moved.

In steps 1 to 4, objects are placed left or right and are given names. In step 5 of the Baldwin Task, the objects are hidden while in the Interference Task, their location is switched. The items are put back in their correct location in 6 and 7 before being moved again in step 8. The participant is then asked to identify an object

This experiment was replicated using children aged 12 to 18 months (stock image) and the researchers noticed only 'slight differences in the results.' Like with the robot, the infants failed to recognise objects if their position and posture changed, or the object was moved

The study was carried out by Linda Smith, a professor in the IU Bloomington College of Arts and Sciences' Department of Psychological and Brain Sciences. 

She worked with Anthony Morse, a senior post-doctoral research fellow in the Centre for Robotics and Neural Systems at the University of Plymouth and Viridiana Benitez, a post-doctoral research associate at the University of Wisconsin-Madison. 

'This study shows that the body plays a role in early object name learning, and how toddlers use the body's position in space to connect ideas,' Professor Smith said. 

'The creation of a robot model for infant learning has far-reaching implications for how the brains of young people work.

'A number of studies suggest that memory is tightly tied to the location of an object.

'None, however, have shown that bodily position plays a role or that, if you shift your body, you could forget.'

'This study shows  the body plays a role in early object name learning,' lead author Professor Linda Smith said. 'The creation of a robot model for infant learning (iCub pictured)) has far-reaching implications for how the brains of young people work'

The humanoid called iCub has been under development for 10 years. 

It is motivated by goals and can express six emotions on its face, including raising its eyebrows and generating a light-up smile. 

It was originally created by the Italian Institute of Technology as part of the eFAA Project and has since been refined to crawl, walk and dance to music, manipulate objects in its hands, speak and express emotions. 

The robot's goals and emotions change constantly, based on information picked up by sensors and it reacts appropriately

A video recently posted by New Scientist shows it playing a game of paddle war – a simple game where two players each hold a paddle - or in this case a cylinder - and try to score goals using a digital ball, while stopping their rival from scoring. 

The robot's goals and emotions change constantly, based on information picked up by sensors and it reacts appropriately to whether it is winning or losing.

When iCub concedes a goal, it frowns and furrows its light-up eyebrows, sometimes swearing like an ill-tempered human.

When it scores a goal, it smiles and says phrases like 'got it,' although there is not much change in intonation whether a phrase is negative or positive.

The video also shows how the robot can respond to being touched, a little bit like a slightly creepy human. 

When a human strokes its arms it affects iCub's reactions and emotions and the robot says on one occasion in the video: 'I like when you touch me this way'.

Tweaks to its software also mean that the robot is starting to understand language and can follow instructions that require reasoning, such as identifying two objects and moving them to a position, as instructed.

She added that the experiments could provide a new way to investigate how cognition is connected to the body, as well as new evidence that thoughts, words and representations of objects are affected by the 'spatial relationship of the body within the surrounding world.'

The research, 'Posture Affects How Robots and Infants Map Words to Objects,' is published in the journal Plos One.

Professor Smith said additional research is needed to determine whether this study's results apply to just infants, or more broadly to the relationship between the brain, the body and memory. 

The study may also help learn more about developmental disorders, difficulties with motor coordination and cognitive development.   

A video shows iCub playing a game of paddle war (pictured) – a  game where two players each hold a paddle and try to score goals using a digital ball, while stopping their rival from scoring 

 

@highlight

Babies learn the name of new objects by 'mapping' their surroundings 

@highlight

This means they associate a word using the object's proximity to them

@highlight

Study said this applies to their physical location as well as their posture

@highlight

Researchers used a humanoid called iCub and infants to test this theory