By
Ellie Zolfagharifard

Robots may have the upper hand when completing complex, repetitive tasks – but when it comes to human interaction, they still have a lot to learn.

Now one droid, named Charlie, is hoping to change all that by copying the body language of humans to get people to do exactly what he wants.

Standing at five feet (152cm) tall, with two large arms that mimic the range of motion of a human arm, Charlie has now mastered the task of handing an object to someone.

Scroll down for video...

Researchers are programming robots to communicate with people using human-like body language and cues. Pictured is  AJung Moon, a PhD student in the Department of Mechanical Engineering at British Columbia University with Charlie

The $400,000 ($238,000) humanoid robot, picks up a water bottle, briefly glances at the drink and then extends the bottle, with its two camera eyes rising to meet the gaze of the human.

It might sound like a simple task, but the two-second act is the result of months of painstaking research and programming.

Past studies have shown that people have difficulty figuring out when to reach out and take an object from a robot because droids fail to provide appropriate nonverbal cues.

‘We hand things to other people multiple times a day and we do it seamlessly,’ said AJung Moon, a PhD student at the University of British Columbia in Canada.

School can be a daunting place for children with autism – with many struggling to communicate with peers and stay focused. But one school think they have found a solution – robot teachers.

Topcliffe Primary in Birmingham is the first in the country to use humanoid robots to help teach pupils with autism. The knee-high machines, which are designed to move in the same way as children, can be programmed to have conversations, play games, dance and even take classes.

But unlike human teachers, Max and Ben have blank features, no emotions and standardised expressions and responses. It is this, according to head teacher, Ian Lowe that makes the robots much easier for autistic to children to relate to and understand.

‘Getting this to work between a robot and a person is really important if we want robots to be helpful in fetching us things in our homes or at work.’

Ms Moon and her colleagues studied what people do with their heads, necks and eyes when they hand water bottles to one another.

They then tested three variations of this interaction with Charlie and the 102 study participants. Programming Charlie to use eye gaze as a nonverbal cue made the handover more fluid.

Researchers found that people reached out to take the water bottle sooner in scenarios where the robot moved its head to look at the area where it would hand over the water bottle.

People also took the cue to reach out for the bottle when Charlie looked to the handover location and then up at the person to make eye contact.

Understanding the human-robot interaction could inform everything from rehabilitation for ailments like stroke applications such as factory production and manufacturing.

‘We want the robot to communicate using the cues that people already recognise,’ said Ms Moon. ‘This is key to interacting with a robot in a safe and friendly manner.’

Standing at five feet (152cm) tall, with two large arms that mimic the range of motion of a human arm, Charlie has now mastered the task of handing an object to someone

 

@highlight

Humans have 
difficulty knowing when to reach for an object from robots

@highlight

This is because droids fail to provide appropriate nonverbal cues

@highlight

Charlie has been programmed to pick
 up a bottle, briefly glance at the drink and then extend it, with its two camera eyes rising to meet the human

@highlight

Understanding the human-robot interaction
 could help develop better droids for everything from medical rehabilitation to manufacturing