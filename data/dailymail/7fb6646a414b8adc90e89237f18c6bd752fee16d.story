Following reports Facebook manipulated the feeds of almost 700,000 users, the site has issued a statement claiming it 'never met to upset anyone'.

During one week in 2012, the social media giant edited feeds to highlight either positive or negative items, and then monitored responses.

The site has since apologised for the way the paper described the research, and any anxiety that was caused, adding, 'the research benefits of the paper may not have justified all of this.'

Scroll down for video

During one week in 2012, Facebook manipulated feeds of just over 689,000 users to highlight either positive or negative items, and then monitored responses over the course of a random week. The site has since apologised for the way the paper described the research, and any anxiety that was caused

The California-based firm carried out the experiment during a week in 2012.

During that time, negative posts were deprioritised in the data feeds of 689,003 users, to see if it generated a more positive response.

Posts were determined by the experiment to be positive or negative if they contained at least one positive or negative word.

The experiment affected around 0.04 per cent of users - or 1 in 2500.

According to Facebook, nobody's posts were 'hidden,' they just didn't show up on some feeds.

It found that negative posts elicited a 
swell of positive responses, but also that a reduction in positive news 
led to more negative posts.

‘When
 positive expressions were reduced, people produced fewer positive posts
 and more negative posts; when negative expressions were reduced, the 
opposite pattern occurred,’ said the researchers.

‘The reason we did this research is because we care about the emotional impact of Facebook and the people that use our product,' said Facebook data scientist Adam D. I. Kramer.

‘We felt that it was important to investigate the common worry that seeing friends post positive content leads to people feeling negative or left out.

‘At the same time, we were concerned that exposure to friends' negativity might lead people to avoid visiting Facebook. We didn't clearly state our motivations in the paper.

'Having written and designed this experiment myself, I can tell you that our goal was never to upset anyone.'

During the experiment, Facebook deprioritised content in News Feeds, based on whether there was an emotional word in the post.

Tests affected around 0.04 per cent of users - or 1 in 2500 - for a week, in early 2012.

According to Kramer, nobody's posts were 'hidden,' they just didn't show up on some feeds.

'Those posts were always visible on friends' timelines, and could have shown up on subsequent News Feed loads.'

It found that negative posts elicited a 
swell of positive responses, but also that a reduction in positive news 
led to more negative posts, according to the results of a study 
published in PNAS Journal.

Facebook data scientist Adam D. I. Kramer issued a statement over the weekend (pictured). He said: 'The reason we did this research is because we care about the emotional impact of Facebook. Having written and designed this experiment myself, I can tell you our goal was never to upset anyone'

‘When
 positive expressions were reduced, people produced fewer positive posts
 and more negative posts; when negative expressions were reduced, the 
opposite pattern occurred,’ said the researchers.

‘These
 results indicate that emotions expressed by others on Facebook 
influence our own emotions, constituting experimental evidence for 
massive-scale contagion via social networks.’

Of the millions of posts analysed, 4 million were found to be positive and 1.8million were determined to be negative.

The
 findings led the team to conclude that ‘in-person interaction and 
nonverbal cues are not strictly necessary for emotional contagion.’

This
 experiment was limited to users who viewed Facebook in English, but it 
is not known across which geographic boundaries.

'At the end of the day, the actual impact on people in the experiment was the minimal amount to statistically detect it - the result was that people produced an average of one fewer emotional word, per thousand words, over the following week,' continued Kramer.

'I can understand why some people have concerns about it, and my coauthors and I are very sorry for the way the paper described the research and any anxiety it caused.

'In hindsight, the research benefits of the paper may not have justified all of this anxiety.'

Commenting on the reports, Brett Dixon, director of the digital marketing agency DPOM, said:  'Despite Facebook's insistence this was merely an academic experiment, it sails perilously close to the illegal world of subliminal advertising.

'There's a reason this insidious form of manipulation is banned - it is an abuse of people's freedom to choose.

'But let's keep some perspective. This was a research project, not the birth of some social media thought police.'

@highlight

In 2012, Facebook manipulated the feeds of 689,003 users during one week

@highlight

It edited feeds to make either negative or positive posts more prominent

@highlight

The results were published over the weekend in the journal PNAS

@highlight

Facebook has apologised for the way the paper described the research and any anxiety that was caused

@highlight

During the test, negative posts received more positive responses

@highlight

A reduction in positive news feed items was met with negative posts