By
Victoria Woollaston

Despite the differences in the way we look and behave, humans share 99.9 per cent of genetic code with each other.

To quantify just how much data is contained within this code - both in the parts we all share, and how we differ - physics engineer Derek Muller applied information theory to the complex nature of DNA.

Using binary principles, Muller assigned two bits of information for each molecule in DNA, and converted these bits into bytes to discover the entire code equates to just 1.5GB. 

Scroll down for video

Human genetic code is contained in a sequence of four molecules, represented by letters A, T, G and C. Each can be encoded with two bits of binary information - 00, 10, 11, 01. When multiplied by 6 billion letters, and divided by eight bits per byte, a physician claims the entire code contains just 1.5GB of data

This means the entire code could be stored on a standard DVD, and is the equivalent to around 6,709 books, or 300 pages with 360,000 characters.

The smallest 
amount of information available is the answer to a ‘yes’ or ‘no’, or 
‘true’ or ‘false’ question.

In binary code, this can be simplified between a 1, for true, and a 0, for false.

Computers use 8
 bits as the unit of computation, and this is called it a byte.

A human’s genetic code is contained in a sequence of four molecules, represented by letters A, T, G and C.

Each
 can be encoded with two bits of binary information - 00, 10, 11, 01.

When
 multiplied by 6 billion letters that make up genetic code, and divided by eight bits per 
byte, Muller worked out the entire code contains just 1.5GB of 
data.

Expanding this 
further, Muller continued that each cell in the human body contains this
 1.5GB of data, and there are 40 trillion cells.

This works out at 60 zettabytes of information, or 60 followed by 21 zeros.

Furthermore,
 because 99.9 per cent of this information is shared with other humans, 
less than one part in a 1,000 is unique - and this small amount of data 
could easily fit on a floppy disk.

Alternatively, the code could hold 511 digital photos, if each image had an average 3MB size.

Muller posted the explanation on his Veritasium YouTube channel.

According to Muller, the smallest amount of information available is the answer to a ‘yes’ or ‘no’, or ‘true’ or ‘false’ question.

In binary code, this can be simplified between a 1, for true, and a 0, for false.

In 1963, the American Standard Code for Information Interchange (ASCII) stated there are seven bits of information to encode all of the letters in the alphabet, including upper and lower case letters and symbols.

Computers, which need a multiple of two to function due to the binary base of code, began using 8 bits as the unit of computation, and this was called it a byte.

A human’s genetic code is contained in a sequence of four molecules, represented by letters A, T, G and C.

Each can be encoded with two bits of information - 00, 10, 11, 01 - and when multiplied by the 6 billion letters, then divided by eight bits per byte, Muller worked out the entire code contains just 1.5GB of data.

Expanding this further, Muller continued that each cell in the human body contains this 1.5GB of data, and there are 40 trillion cells.

This works out at 60 zettabytes of information, or 60 followed by 21 zeros.

Furthermore, because 99.9 per cent of this information is shared with other humans, less than one part in a 1,000 is unique - and this small amount of data could easily fit on a floppy disk (stock image pictured)

At its simplest, the theory looks at how messages are made of information, how they’re different, and also how they’re the same.

For example, if someone wants to share an idea, they can email it, write it down, draw a picture, and speak it and so on.

All of these methods are different, but the outcome is the same; the information is shared.

With
 language, brains are able to take words, or thoughts, and break them 
down into chunks, which are then externalised.

Humans talk, 
birds sing and computers use electrical vibrations.

Information, no matter how its stored and shared, can be measured using a fundamental process known as entropy.

This
 is measured in bits, and the information - no matter how its shared - 
is made up of the same number of bits, just in different densities.

Furthermore, because 99.9 per cent of this information is shared with other humans, less than one part in a 1,000 is unique - and this small amount of data could easily fit on a floppy disk.

The work detailed in Veritasium’s video is based on the general concept of information theory.

At its simplest, the theory looks at how messages are made of information, how they’re different and also how they’re the same.

For example, if someone wants to share an idea, they can email it, write it down, draw a picture, speak it and so on.

All of these methods are different, but the outcome is the same; the information is shared.

With language, brains are able to take words, or thoughts, and break them down into chunks, which are then externalised using symbols or signals.

Humans talk, birds sing and computers use electrical vibrations.

Information, no matter how its stored and shared, can be measured using a fundamental process known as entropy.

This is measured in bits, and the information - no matter how its shared - is made up of the same number of bits, just in different densities.

 

@highlight

Video was produced by Australian-based physics engineer Derek Muller

@highlight

He began by breaking DNA into the four molecules A, T, G and C

@highlight

Each letter is encoded with two bits of information, using binary principles

@highlight

A human’s genetic code contains 6 billion letters and when this divided by eight bits per byte, it equates to 1.5GB per human

@highlight

But, when multiplied by 40 trillion cells, this is 60 zettabytes of information