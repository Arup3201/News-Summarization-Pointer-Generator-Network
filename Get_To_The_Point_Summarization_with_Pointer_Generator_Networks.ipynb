{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOmKgg30qVhaRlSKZnPKR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the wraping the outputs of colab"
      ],
      "metadata": {
        "id": "aWWumM-N6C31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "J2_Q63jmdSp3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps the save the stories of cnn and dailymail in `/data` folder\n",
        "\n",
        "**Step 1: Download gitpython**\n",
        "\n",
        "`\n",
        "!pip install gitpython\n",
        "`\n",
        "\n",
        "**Step 2: Import libraries**\n",
        "\n",
        "```python\n",
        "import os\n",
        "from git import Repo\n",
        "```\n",
        "\n",
        "**Step 3: Clone repository**\n",
        "\n",
        "```python\n",
        "username = \"Arup3201\"\n",
        "repository = \"Summarization-Project-using-Pointer-Gen\"\n",
        "repository_url = f\"https://github.com/{username}/{repository}.git\"\n",
        "repo_dir = \"Summarization-Project-using-Pointer-Gen\"  # Local directory to clone the repository\n",
        "Repo.clone_from(repository_url, repo_dir)\n",
        "```\n",
        "\n",
        "**Step 4: Download the datasets using `tf.keras.utils.get_file`**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download the CNN stories from the url into cnn_stories_tgz file\n",
        "cnn_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "# Download the Dailymail stories from the url into dailymail_stories_tgz file\n",
        "dailymail_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        "    extract=True\n",
        ")\n",
        "```\n",
        "\n",
        "NOTE: **Directory structure of the extracted cnn and dailymail folders👇**\n",
        "```\n",
        "cnn\n",
        "  stories\n",
        "    438411e10e1ef79b47cc48cd95296d85798c1e38.story\n",
        "    e453e379e8a70af2d3dff1c75c41b0a35edbe9cc.story\n",
        "    2079f35aca44978a7985afe0ddacdf02bedf98f2.story\n",
        "    4702f28c198223157bb8f69665b039d560eebb0f.story\n",
        "    db3e2ea79323a98379228b17cd3b9dec17dbd2cb.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "dailymail\n",
        "  stories\n",
        "    f4ba18635997139c751311b9f2ad18f455dd7c98.story\n",
        "    4a3ef32cff589c85ad0d22724e2ed747c0dacf87.story\n",
        "    5375ed75939108c72001b043d3b4799c47f32be9.story\n",
        "    fe9e57c21e21fb4ec26e394f0e92824f38d18a95.story\n",
        "    6a544b5cdd2384be6cc657b265d7aa2de72a99e0.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "**Step 5: Copy the stories of CNN and Dailymail to `data/cnn` and `data/dailymail` dir**\n",
        "\n",
        "```python\n",
        "import shutil\n",
        "\n",
        "# Source directory (files in Colab environment)\n",
        "cnn_source_dir = '/root/.keras/datasets/cnn/stories'  # Replace with the path to your dataset in Colab\n",
        "dailymail_source_dir = '/root/.keras/datasets/dailymail/stories'\n",
        "\n",
        "# Destination directory (cloned repository directory)\n",
        "cnn_destination_dir = os.path.join(repo_dir, \"data/cnn\")\n",
        "dailymail_destination_dir = os.path.join(repo_dir, \"data/dailymail\")\n",
        "\n",
        "# Copy the dataset files to the cloned repository directory\n",
        "shutil.copytree(cnn_source_dir, cnn_destination_dir)\n",
        "shutil.copytree(dailymail_source_dir, dailymail_destination_dir)\n",
        "```\n",
        "\n",
        "**Step 6: Commit the Changes in GitHub**\n",
        "```python\n",
        "repo = Repo(repo_dir)\n",
        "index = repo.index\n",
        "index.add([path_to_added_file])\n",
        "author = Actor(\"<USERNAME>\", \"yourexample@gmail.com\")\n",
        "\n",
        "index.commit(\"Added cnn and dailymail stories in /data folder.\", author=author)\n",
        "\n",
        "# Change current working directory to repo directory\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# Change the remote Origin\n",
        "!git remote remove origin\n",
        "!git remote add origin https://<USERNAME>:<PERSONALACCESTOKEN>@github.com/<USERNAME>/<REPO_NAME>.git\n",
        "\n",
        "# Push the changes\n",
        "!git push origin main\n",
        "```\n"
      ],
      "metadata": {
        "id": "ONsFbvbTr_zn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "x6QucUCZ0q2V",
        "outputId": "ee84a03a-b953-4850-eb64-e1665dc3eb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pathlib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# For tokenizing and processing the examples for the model training\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Layers for the Encoder, Attention and Decoder\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.layers import RepeatVector, Concatenate, Activation, Dot\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# For model initialization\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# For training the model\n",
        "from tensorflow.keras.optimizers.experimental import Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CNN stories from the url into cnn_stories_tgz file\n",
        "cnn_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        ")\n",
        "\n",
        "# Download the Dailymail stories from the url into dailymail_stories_tgz file\n",
        "dailymail_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        ")"
      ],
      "metadata": {
        "id": "k_03wyoLlFhO",
        "outputId": "dc2cca27-7390-4bba-aec5-c7991cd78577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\n",
            "158577824/158577824 [==============================] - 1s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\n",
            "375893739/375893739 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_tgz, dailymail_stories_tgz"
      ],
      "metadata": {
        "id": "fyid1XyFl1sp",
        "outputId": "746df15a-4b62-42dc-b100-17610f6d5e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.keras/datasets/cnn_stories.tgz',\n",
              " '/root/.keras/datasets/dailymail_stories.tgz')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /root/.keras/datasets/cnn_stories.tgz\n",
        "!tar -xzf /root/.keras/datasets/dailymail_stories.tgz"
      ],
      "metadata": {
        "id": "EHJtYhZpls0i",
        "outputId": "58b1bb56-cae0-468d-85bf-e4303486a252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir = pathlib.Path('/root/data/cnn')\n",
        "dailymail_stories_dir = pathlib.Path('/content/data/dailymail')"
      ],
      "metadata": {
        "id": "ag5ubSt7l88l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9dbefe07-8238-40f4-8b39-d5ef3e096f9a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir, dailymail_stories_dir"
      ],
      "metadata": {
        "id": "UmMCM--Yhm3B",
        "outputId": "9973be2b-5b24-421e-8889-dcbe46c4da01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/content/data/cnn'), PosixPath('/content/data/dailymail'))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_filenames(dir_path, num_files=5):\n",
        "  '''Prints the name of the files that are present at `dir_path`.\n",
        "  Maximum `num_files` number of files are shown.\n",
        "\n",
        "  Arguments:\n",
        "    dir_path: PosixPath, pointing to the directory of which the user\n",
        "              wants to prints the file names.\n",
        "    num_files: int, number of files user wants to print.\n",
        "\n",
        "  returns:\n",
        "    nothing\n",
        "  '''\n",
        "\n",
        "  count = 0\n",
        "  for f in dir_path.glob('*.story'):\n",
        "    print(f.name)\n",
        "    count += 1\n",
        "\n",
        "    if count == num_files:\n",
        "      break\n",
        "  else:\n",
        "    print(f\"Less than {num_files} is present!\")"
      ],
      "metadata": {
        "id": "dZFFXG7Pg86H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "084dae32-dc3a-4752-ffef-c120e137e510"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(cnn_stories_dir)"
      ],
      "metadata": {
        "id": "nv3g7-M316yL",
        "outputId": "29a9bde9-5f82-4542-a768-5d0679f0ae4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a7ae6005229a13ecc4a34d786c58c4f6d0585a30.story\n",
            "329aa4bda673a83d8995645f0d5b92939dfa57b3.story\n",
            "7afa20326e5060228a1c0ea23d0670676554093f.story\n",
            "dc931c471aaf41d8299cb1f2ced2c9fec820de70.story\n",
            "91f8b3053a36936726c6b9b0560f1fdd34e589ad.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(dailymail_stories_dir)"
      ],
      "metadata": {
        "id": "WcyW1wJa2EaN",
        "outputId": "b8610940-d57f-4c4e-878f-d0633f4d3ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a898a59b33de5e5656eb1fed3a3fc3eeeeea978d.story\n",
            "48f26a6cf819bf87a1636d149371f24eab0d9a51.story\n",
            "f4299be8f4f9f180ddc797bfd2618cbc43961b43.story\n",
            "2ef6fae822de7fbf552f6beeaa9534aaeed24814.story\n",
            "a99c5e5bf7af4aee9dba117e84d6750a65b6553c.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
        "              dm_single_close_quote, dm_double_close_quote, \")\"]\n",
        "\n",
        "# Maximum stories to process from cnn and dailymail each\n",
        "MAX_STORIES = 50000\n",
        "\n",
        "# From the total data how to split into train, val and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VAL_SIZE = 0.1\n",
        "TEST_SIZE = 0.1\n",
        "\n",
        "# For tokenization\n",
        "VOCAB_SIZE = 20000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# For standardization\n",
        "PAD_TOKEN = '<PAD>'\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "# For the number of tokens to use in representing articles and summaries, hyperparameters\n",
        "MAX_ARTICLE_TOKENS = 400\n",
        "MAX_SUMMARY_TOKENS = 100\n",
        "\n",
        "# For dataset creation hyperparameters\n",
        "BUFFER_SIZE = 5000\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "## Model Archietecture hyperparameters\n",
        "# Embedding output dimension\n",
        "EMB_OUT = 32\n",
        "# Encoder hidden(also cell) state dimension\n",
        "ENCODER_STATE_DIM = 32\n",
        "# Decoder hidden(also cell) state dimension\n",
        "DECODER_STATE_DIM = 64\n",
        "# Attention first dense layer units(calculates partial energy)\n",
        "DENSE1_UNITS = 16\n",
        "# Attention secodn dense layer units(calculated final energy)\n",
        "DENSE2_UNITS = 1\n",
        "# Units of the Dense layers before output layer\n",
        "DENSE_UNITS = 64"
      ],
      "metadata": {
        "id": "cubLAm1Q3SuG",
        "outputId": "86d909f6-1ca6-439f-cbe9-29a8ec5458b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a sample .story file from cnn stories\n",
        "sample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\n",
        "sample_filedir = cnn_stories_dir\n",
        "\n",
        "sample_filepath = sample_filedir / sample_filename\n",
        "with open(sample_filepath, 'r') as f:\n",
        "  sample_story = f.read()\n",
        "\n",
        "print(f\"A sample story:\\n{sample_story}\")"
      ],
      "metadata": {
        "id": "Uw0kqchX3X7X",
        "outputId": "f87a5f4c-b31c-4579-cded-df03b95af4fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample story:\n",
            "New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\n",
            "\n",
            "The figure represents a 0.7% increase from last year, adding 2,250,129 people to the U.S. population since the start of 2011, and a 1.3% increase since Census Day, April 1, 2010.\n",
            "\n",
            "The agency estimates that beginning in January, one American will be born every eight seconds and one will die every 12 seconds.\n",
            "\n",
            "U.S.-bound immigrants are also expected to add one person every 46 seconds.\n",
            "\n",
            "That combination of births, deaths and migration is expected to add a single person to the U.S. population every 17 seconds, the Census Bureau said.\n",
            "\n",
            "Meanwhile, millions are set to ring in the new year.\n",
            "\n",
            "In New York, authorities are preparing for large crowds in Manhattan's Times Square, where Lady Gaga is expected to join Mayor Michael Bloomberg to push the button that drops the Waterford Crystal ball at 11:59 p.m. ET on New Year's Eve.\n",
            "\n",
            "\"And I'm so looking forward to performing on NYE+dropping the Ball with Mayor Bloomberg!\" the pop star posted on Twitter. \"What an honor as a New Yorker.\"\n",
            "\n",
            "Past guests have included Muhammad Ali, Rudy Giuliani, Colin Powell and Bill and Hillary Clinton.\n",
            "\n",
            "On Thursday, officials conducted New York's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above Times Square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square.\n",
            "\n",
            "The Big Apple this year edged out Las Vegas for the first time in seven years as the top travel U.S. destination for those celebrating the new year, according to a December travel booking website poll.\n",
            "\n",
            "Seven New York neighborhoods made the top 10 list, with two districts in Las Vegas and one in New Orleans making up the other three, according to the Priceline poll.\n",
            "\n",
            "\"It appears that New York City will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman Brian Ek.\n",
            "\n",
            "@highlight\n",
            "\n",
            "Census Bureau: U.S. population is expected to be 312.8 million on New Year's Day\n",
            "\n",
            "@highlight\n",
            "\n",
            "That figure represents a 0.7% increase from last year\n",
            "\n",
            "@highlight\n",
            "\n",
            "Lady Gaga, mayor to activate ball drop at Times Square on New Year's Eve\n",
            "\n",
            "@highlight\n",
            "\n",
            "NYC has supplanted Las Vegas as the top New Year's destination, Priceline poll says\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n",
        "\n",
        "These are the steps -\n",
        "1. Check if line contains `@highlight`, if True then just return the line.\n",
        "2. Check if line is empty, then return line as it is.\n",
        "3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n",
        "4. Only is none of the above conditions match then append `.` to the current line."
      ],
      "metadata": {
        "id": "iZPt8EVBzJ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_missing_period(line, end_tokens=END_TOKENS):\n",
        "  '''function to fix the missing periods for some story lines which do not end with\n",
        "  any of the end_tokens mentioned.\n",
        "\n",
        "  Argument:\n",
        "    line: string, line of the story to fix the missing the period of.\n",
        "    end_tokens: list of strings, all the tokens that are considered as line end.\n",
        "\n",
        "  Returns:\n",
        "    new line with fixed the ending part by adding an ending token if not present.\n",
        "  '''\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  elif line == \"\":\n",
        "    return line\n",
        "  elif line[-1] in end_tokens:\n",
        "    return line\n",
        "\n",
        "  return line + '.'"
      ],
      "metadata": {
        "id": "i-S-Hss12TPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9e80ab95-cdca-46a4-a61c-5432d3deb977"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"i have a bad habit of not giving full-stop after sentence\\nLike this setence\"\n",
        "print(f\"Fixing {fix_missing_period(sample_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "L1hiNysxnmAO",
        "outputId": "cca89685-2148-4fb3-e38c-cf2f82b886ae"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing i have a bad habit of not giving full-stop after sentence\n",
            "Like this setence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `split_article_summary` which will split the story into article and summary parts.\n",
        "\n",
        "The function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n",
        "\n",
        "The steps to follow are -\n",
        "1. Split the story by new line `\\n`. I will get a list of lines.\n",
        "2. Strip the lines by using list comprehension.\n",
        "3. Use list comprehension to make lower case each line by using `.lower()`.\n",
        "4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n",
        "5. Make 2 empty list for `article` and `summary`.\n",
        "6. Go through each line. In each line, I need to check 4 things,\n",
        "  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n",
        "  * line is `\"\"` empty or not, if True then ignore.\n",
        "  * `next_highlight` is True or not, if True then append the line to `summary`.\n",
        "  * If non of the ebove then append to `article`.\n",
        "7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method."
      ],
      "metadata": {
        "id": "5-tqWtoowXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_article_summary(story):\n",
        "  '''Splits the story into 2 parts, one for article and other for summary of that\n",
        "  article. Returns the article and summary.\n",
        "\n",
        "  Argument:\n",
        "    story: string file that contains both article and summary combiningly.\n",
        "\n",
        "  Returns:\n",
        "    article, summary seperately from the story.\n",
        "\n",
        "  '''\n",
        "  lines = story.split('\\n')\n",
        "  lines = [line.strip() for line in lines]\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  # Fix the ending period\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # List to contain the article and summary lines\n",
        "  article = []\n",
        "  summary = []\n",
        "\n",
        "  # Indicator of whether the next line is the summary or not\n",
        "  next_highlight = False\n",
        "\n",
        "  for line in lines:\n",
        "    if \"@highlight\" in line:\n",
        "      next_highlight = True\n",
        "    elif line==\"\":\n",
        "      continue\n",
        "    elif next_highlight:\n",
        "      summary.append(line)\n",
        "    else:\n",
        "      article.append(line)\n",
        "\n",
        "  article = ' '.join(article)\n",
        "  summary = ' '.join(summary)\n",
        "\n",
        "  return article, summary"
      ],
      "metadata": {
        "id": "-X4eMltQnf10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1c687502-7a1e-4a24-901e-c47cd267e6b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article, sample_summary = split_article_summary(sample_story)\n",
        "\n",
        "print(f\"Sample Article after spliting:\\n{sample_article}\")\n",
        "print(f\"Sample Summary after spliting:\\n{sample_summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "nuUjOaN9orGU",
        "outputId": "082d76cb-fa2d-47b0-eb3d-32711b538f5a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Article after spliting:\n",
            "new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year's eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan's times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year's eve. \"and i'm so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.\n",
            "Sample Summary after spliting:\n",
            "census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n",
        "\n",
        "This function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n",
        "\n",
        "The process is simple. We will follow this steps -\n",
        "1. Create 2 empty lists of `articles` and `summaries`.\n",
        "2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n",
        "3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n",
        "4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n",
        "5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n",
        "6. Return the 2 lists."
      ],
      "metadata": {
        "id": "oTEa0m3Huxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_summaries(stories_dir, max_stories):\n",
        "  '''stores the stories from stories_dir folder into a list and returns the list\n",
        "\n",
        "  Arguments:\n",
        "    stories_dir: Posix string, the directory where the stories are stored\n",
        "    max_stories: maximum number of stories to store\n",
        "\n",
        "  Returns:\n",
        "    list of stories.\n",
        "\n",
        "  '''\n",
        "  articles = []\n",
        "  summaries = []\n",
        "\n",
        "  count = 0\n",
        "  for f in stories_dir.glob(\"*.story\"):\n",
        "    count += 1\n",
        "    with open(f, 'r') as reader:\n",
        "      story = reader.read()\n",
        "\n",
        "      article, summary = split_article_summary(story)\n",
        "\n",
        "      articles.append(article)\n",
        "      summaries.append(summary)\n",
        "\n",
        "    if count == max_stories:\n",
        "      break\n",
        "\n",
        "  return articles, summaries"
      ],
      "metadata": {
        "id": "4VUmbYSpnjAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "216682b1-5cd4-4f53-f4fa-09c0d97510e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them."
      ],
      "metadata": {
        "id": "Uk-BuCM4rIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aa9ZDQntpHQZ",
        "outputId": "7b2ff68e-7c8f-45da-e7e5-87fedce25af5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "-q_i-69YqJnj",
        "outputId": "d2092598-e11f-41dc-ad6a-70d06d58c8ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: (cnn) -- leszek balcerowicz, poland's former finance minister, recently said his country is enjoying \"its best period in 300 years.\" cnn looks at how the country emerged from communism to become one of eastern europe's most stable and thriving democracies. solidarity leader lech walesa addresses striking workers in gdansk, poland in 1989. modern poland gained independence in 1918 only to be overrun by germany and the soviet union during the second world war. almost six million poles, including the majority of the country's large jewish population, died during the devastating six-year conflict. the shadow of stalin continued to loom large over poland after the war, when the communist-dominated government ensured that poland would become a soviet satellite state for the next 40 years. the following decades were punctuated by revolts against the repressive authoritarian regime in warsaw, but none had a greater impact on poland's political future than events in 1980 at a shipyard in western poland. with a struggling economy and rumors of corruption and mismanagement within the state causing widespread discontent, a series of strikes by workers paralyzed the country. eventually the government was forced to negotiate and on august 31, 1980, workers at the massive lenin shipyard in gdansk, led by an electrician named lech walesa, signed a deal giving workers the right to strike and form trade unions. this heralded the creation of the solidarity movement, which would ultimately be instrumental in bringing poland's communist era to an end. the presence in the vatican at the time of polish-born pope john-paul ii was also a significant influence on the movement throughout the 1980s, as the catholic church had remained a very potent force in polish life. the pope even made a visit to the country in 1979. despite soviet-endorsed attempts to slow the erosion of the regime's grip on power -- including the declaration of martial law by general wojciech jaruzelski in 1981 which outlawed solidarity -- poland's worsening economic situation, compounded by further nationwide strikes, meant that the government had no alternative but to negotiate a date for free elections with walesa and the solidarity movement. solidarity members won a stunning victory in the election of 1989, taking almost all the seats in the senate and all of the 169 seats they were allowed to contest in the sejm or parliament. this gave them substantial influence in the new government. activist and journalist tadeusz mazowiecki was appointed prime minister, while lech walesa was elected as president the following year. were you in poland in 1989? send us your memories. after years of economic mismanagement under the communists, poland embarked on a painful reform program under finance minister leszek balcerowicz -- especially in traditional heavy industries such as coal and steel -- which moved away from the inefficient state-controlled system of economic planning. despite growing unemployment and a dilapidated infrastructure, poland was slowly transformed into an investment-friendly, market economy. banking and lending policies were reformed, while newly reshaped ownership relations, independent enterprises and strengthened domestic competition all had a massive impact. over a relatively short period of time, poland had become one of the most dynamically developing economies in europe and by the mid-1990s, it became known as the \"tiger of europe.\" poland also liberalized its international trade during this period. the national currency -- the zloty -- became convertible to other currencies and internal convertibility was also established, providing another platform for dynamic economic growth. new markets in countries that had been treated not so long before as ideological as well as economic enemies were opened up to polish companies. the eu and u.s. were now the key markets for polish goods. this realignment of policy was emphasized by its accession into the european union in 2004. it had also joined nato in 1999. unfortunately the continuing problem of high unemployment and the promise of better salaries encouraged many poles to work in other eu countries after 2004. however this trend started to reverse in 2008 as the polish economy enjoyed a boom period. politically, poland has also successfully transformed itself into a fully democratic country. since 1991 the polish people have voted in parliamentary elections and four presidential elections -- all free and fair. incumbent governments have transferred power smoothly and constitutionally in every instance to their successors.\n",
            "\n",
            "\n",
            "The summary of this article: poland was ruled by soviet-backed regime after the second world war. solidarity movement became a key factor in the fall of communist regime. centrally-planned economic system replaced by free market economy. poland joined the european union in 2004.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n",
        "                                                                 MAX_STORIES)"
      ],
      "metadata": {
        "id": "KT4Hrp6nqeKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3a2b77e2-a579-4372-a415-4c4ed2ecebdf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of dailymail stories captured are {len(dailymail_articles)}\\n\\n\")\n",
        "print(f\"One of the Dailymail articles: {dailymail_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "id": "nkzwSP9kh4VK",
        "outputId": "339e1f83-6ce4-4e2d-ced8-1d0c74bcdcb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of dailymail stories captured are 50000\n",
            "\n",
            "\n",
            "One of the Dailymail articles: mark wahlberg is seeking a pardon from the state of massachusetts for a vicious racially-charged attack he committed as a teenager that left a man blind in one eye. court documents reveal just how horrifying the 1988 attack was - including startling racial slurs he shouted at the victim and others - but the actor insists he is now 'a better person and citizen'. wahlberg, who served 45 days in jail after the assault, says his record continues to impact him and could prevent businesses, such as restaurants, that work with him from getting licenses. he insists he has come a long way since april 1988, when he attacked thanh lam with a five-foot stick while yelling slurs at him in order to steal the two cases of beer he was carrying. scroll down for video. troubled past: mark wahlberg served 45 days in prison after he blinded a man in a 1988 assault. documents included in his pardon request show he called the vietnamese immigrant a 'vietnam f***ing s***' before striking him so hard that the stick broke in two and knocked lam unconscious, according to the documents shared by the smoking gun. he then fled the scene and approached a second man, hoa trinh, put his arm around his shoulder and asked him to help him hide. but once police passed, wahlberg hit him, leaving him blind in one eye. when boston cops tracked wahlberg down and took him back back to the scene of the assault outside a convenience store in dorchester, he told officers: 'you don't have to let him identify me, i'll tell you now that's the motherf***** whose head i split open', the documents show. authorities wrote that he also made 'numerous unsolicited racial statements about 'gooks' and \"slant-eyed gooks\".' in his plea, wahlberg said that he was under the influence of alcohol and drugs at the time. 'from later accounts of the incident, it is my understanding that i may have caused serious injuries'  to the men, he wrote. other documents show that, in an earlier attack in dorchester, he and two friends saw a group of black siblings and chased them. changed man: now, wahlberg is hoping to get a pardon for his past actions. scene of the crime: the liquor store where wahlberg attacked a man with a stick and then ran away. 'we don't like black n****** in the area so get the f*** away from the area,' the boys shouted at the group, according to the documents. they then chased the boys, shouting: 'kill the n*****, kill the n*****'. the following day, the group encountered one of the boys again and threw rocks at him. one of the rocks thrown by wahlberg and one of his friends hit a nearby girl. the trio caused 'a high level of anxiety, fear and intimidation', the documents said. though he was then only 16 at the time of the 1988 attack, he was charged as an adult and convicted of assault. according to necn the initial charge was attempted murder. after emerging from prison, wahlberg went on to find success in music with marky mark and the funky bunch and as a calvin klein model. family guy: wahlberg (above with wife rhea durham and three of his children and kevin hart) is now committed to multiple charities and attends church every day. but it was his appearance in the 1996 film boogie nights that really changed things, catapulting wahlberg into the critically-acclaimed box office juggernaut that he is today. 'i am deeply sorry for the actions that i took on the night of april 8, 1988, as well as for any lasting damage that i may have caused the victims,' wahlberg wrote in the pardon application. 'since that time, i have dedicated myself to becoming a better person and citizen so that i can be a role model to my children and others.' he devotes much of his time and a large amount of his money to various charities, including the mark wahlberg youth foundation and the dorchester boys and girls club. he is also a family man now, married to former model rhea durham and the father of four children, and attends church daily. the board of pardons will investigate the petition to decide if it warrants a hearing before it is recommended to the governor who by that time will be the recently elected charlie baker. pardon? it will ultimately be up to newly elected massachusetts governor charlie baker (above) to decide whether or not wahlberg should be pardoned. and while erasing the past is important, wahlberg points out that he still talks about what he did to show how much a person can change. but he said that, as a convicted criminal, there are still some things he cannot do  and for that he needs a pardon, including working with law enforcement to help at-risk youth. in the end, however, it is about showing his growth. 'the more complex answer is that receiving a pardon would be a formal recognition that i am not the same person that i was on the night of april 8, 1988,' wahlberg explains. 'it would be formal recognition that someone like me can receive official public redemption if he devotes himself to personal improvement and a life of good works.'\n",
            "\n",
            "\n",
            "The summary of this article: mark wahlberg is seeking a pardon from the state of massachusetts for an assault he committed in 1988 that left a man blind in one eye. wahlberg, now 43, served 45 days in prison and was tried as an adult even though he was just 16 years old at the time. documents show he called one of his victims a 'vietnam f***ing s***' and that he later chased a group of black children, shouting: 'kill the n*****!' the actor, who has always been vocal about his troubled past, is now a married father who attends church every day.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating another function -\n",
        "`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples."
      ],
      "metadata": {
        "id": "8ea-PhS3iIJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_size, val_size, test_size):\n",
        "  first_split = train_size\n",
        "  second_split = train_size+val_size\n",
        "  third_split = train_size+val_size+test_size\n",
        "  return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]"
      ],
      "metadata": {
        "id": "v-iZDbUCqkdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "45214701-580b-4b2d-f8ab-6485babeafd7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a function `make_datasets`, that will be make training, validation and testing datasets. This function will -\n",
        "1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\n",
        "Objective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n",
        "```python\n",
        "[1, 2] + [3, 4] = [1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n",
        "```python\n",
        "  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n",
        "```\n",
        "4. Shuffle the dataset using `random.sample` method.\n",
        "```python\n",
        "random.seed(seed_value) # To make sure that everytime it gives the same shuffle\n",
        "random.sample(list_to_shuffle, len(list_to_shuffle))\n",
        "```\n",
        "5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`."
      ],
      "metadata": {
        "id": "FTB5eNzJrqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n",
        "  '''Create 3 datasets each for training, validation and testing respectively.\n",
        "  This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n",
        "  them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n",
        "  summaries using random.sample method (although we have a helper function for it). Finally we do the\n",
        "  splitting of the whole dataset. Remember here the returned values become tensors.\n",
        "\n",
        "  Arguments:\n",
        "    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n",
        "    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n",
        "    train_size: float, specifying how much fraction of the original dataset to take for training.\n",
        "    val_size: float, specifying how much fraction of the original dataset to take for validation.\n",
        "    test_size: float, specifying how much fraction of the original dataset to take for testing.\n",
        "\n",
        "  Returns:\n",
        "    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n",
        "    with the specified amount of data in it.\n",
        "    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n",
        "  '''\n",
        "  articles = cnn_stories[0] + dailymail_stories[0]\n",
        "  summaries = cnn_stories[1] + dailymail_stories[1]\n",
        "\n",
        "  articles = np.array(articles, dtype=object)\n",
        "  summaries = np.array(summaries, dtype=object)\n",
        "\n",
        "  dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n",
        "\n",
        "  random.seed(seed_value)\n",
        "  shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n",
        "\n",
        "  dataset = dataset[shuffled_indices, :]\n",
        "\n",
        "  train_size = int(train_fraction * dataset.shape[0])\n",
        "  val_size = int(val_fraction * dataset.shape[0])\n",
        "  test_size = dataset.shape[0] - (train_size + val_size)\n",
        "\n",
        "  training_samples, validation_samples, testing_samples = split_dataset(dataset,\n",
        "                                                                        train_size,\n",
        "                                                                        val_size,\n",
        "                                                                        test_size)\n",
        "\n",
        "  return (training_samples, validation_samples, testing_samples)"
      ],
      "metadata": {
        "id": "QDB0_32RrnHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d7eda692-48e7-486b-aac2-5c1a05904834"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ],
      "metadata": {
        "id": "GTnXBwd6Sa-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0d9e3eb1-747f-4302-a2bf-4a29aa40fb74"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n",
        "\n",
        "print(f\"Training dataset shape: {train_dataset.shape}\")\n",
        "print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
        "print(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n",
        "\n",
        "print(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")"
      ],
      "metadata": {
        "id": "AyRVodwIhkuQ",
        "outputId": "50f54996-5544-4222-bc2d-e123dc25dae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the datasets: <class 'numpy.ndarray'>\n",
            "\n",
            "Training dataset shape: (80000, 2)\n",
            "Validation dataset shape: (10000, 2)\n",
            "Testing dataset shape: (10000, 2)\n",
            "\n",
            "First example in the training dataset looks like: \n",
            " [\"teenagers who use cannabis regularly risk damaging their memory, scientists say – in turn leading to poor academic performance. they believe the brain abnormalities last for ‘at least a few years’ after users have stopped taking the drug. the researchers also said there was fresh evidence the habit may cause mental health problems in youngsters predisposed to schizophrenia. researchers found that the steroid hormone pregnenolone reduces the brain's sensitivity to thc, which is the chief high-inducing compound in cannabis. marijuana is the most commonly used. illicit drug among adolescents in the uk, with more than four in ten. admitting having taken it. almost 100 teenagers took part in the us research examining the effects of cannabis deep in the brain. it. found teenagers who smoked it daily for about three years had abnormal. changes in the brain structures related to remembering and processing. information and they performed poorly on memory tasks. the. brain abnormalities and memory problems were found on mri scans when. study participants were in their early twenties – two years after they. had stopped smoking the drug. memory-related. structures in their brains appeared to shrink and collapse inward and. the researchers said such damage was linked to poor academic performance. and everyday functioning. the study also shows the marijuana-related brain abnormalities look similar\\xa0 to those seen in schizophrenia patients. lead. author matthew smith, a professor in psychiatry and behavioural. sciences at northwestern university feinberg school of medicine,. chicago, said: ‘the study links the chronic use of marijuana to these. concerning brain abnormalities that appear to last for at least a few. years after people stop using it. ‘with the movement to decriminalise marijuana, we need more research to understand its effect on the brain.’ a naturally occurring hormone called pregnenolone  can be used to stop the 'high' produced by cannabis and the discovery could lead to new approaches to treating cannabis intoxication and dependence, according to new research. he said chronic cannabis use may lead to changes in brain structure associated with having schizophrenia. of. the 15 marijuana smokers who had schizophrenia in the study, 90 per. cent had started using the drug heavily before they developed the mental. disorder. a recent study found teenagers who use cannabis regularly risk damaging their memory as structures in their brains appeared to shrink and collapse inward, possibly reflecting a decrease in neurons. professor smith. added: ‘if someone has a family history of schizophrenia, they are. increasing their risk of developing schizophrenia if they abuse. marijuana. ‘if you have. schizophrenia and you frequently smoke marijuana, you may be at an. increased risk for poor working memory, which predicts your everyday. functioning.’ the study,. published in the journal schizophrenia bulletin, is the first to target. key brain regions in the deep grey matter of chronic marijuana users. with mri scanning. it is. also the first time abnormalities in these regions have been linked with. an impaired working memory – the ability to remember and process. information in the moment and then transfer it to long-term memory. participants. started using marijuana daily aged 16-17 for about three years – and. had been free of the drug for around two years at the time of the study. the 97 teenagers who took part included healthy people, those with a marijuana use disorder and schizophrenia patients. the. younger the individuals were when they started chronically using. marijuana, the more abnormally their brain regions were shaped,. suggesting parts related to memory are more susceptible to the effects. of the drug if abuse starts at an earlier age. under. the labour government cannabis was downgraded from class b to class c. in 2004, which critics argued gave the ‘green light’ to use by. youngsters. the decision was reversed in 2008 when ministers decided to overturn official scientific advice and return it to class b.\"\n",
            " 'the younger the users, the more abnormally their. brains were shaped. however, the abnormalities could have existed before they used cannabis. marijuana is most commonly used illicit drug among adolescents in uk.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n",
        "\n",
        "1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n",
        "2. Replace the single and double opening and closing quotes like `‘ → \\u2018`, `’ → \\u2019`, `“ → \\u201c` and `” → \\u201d` by `'` and `\"` respectively.\n",
        "3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\n",
        "In this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n",
        "4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n",
        "\n",
        "`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n",
        "1. Remove the `START_TOKEN` and `END_TOKEN` from the text. So that tokenizer does not standardize them.\n",
        "2. Standardize the text with `standardizer`.\n",
        "3. Add back the `START_TOKEN` and `END_TOKEN` because you want your tokenizer to learn them.\n",
        "4. Remove unwanted spaces in between words.\n",
        "5. Split the text into words which are seperated by ' '.\n",
        "6. Strip each of the words in the sentence. Finally, return it."
      ],
      "metadata": {
        "id": "TcZCpJ9hCyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the text data\n",
        "def standardizer(text):\n",
        "  '''Standardize the text provided to the function\n",
        "  The text is lower cased. Then, the opening and closing quotes are removed. I add spaces before the\n",
        "  punctuations like `don't` becomes `don ' t`, ignoring the numerical values so that `1.78` does not become\n",
        "  `1 . 78`. Finally, it strips the text and removes any type of unwanted spaces in it.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text to standardize\n",
        "\n",
        "  Returns:\n",
        "    returns the standadized text\n",
        "  '''\n",
        "\n",
        "  # Lower case the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Replace the special single and double opening and closing quotes\n",
        "  text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n",
        "  text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n",
        "\n",
        "  # Add space before punctuations and ignore floating point numbers.\n",
        "  text = re.sub(r'(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n",
        "                  r' \\1 ', text)  # It used to also remove commas after numbers like '27,' will be removed\n",
        "\n",
        "  # Remove spaces after sentence end and other unwanted spaces from text\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# custom analyzer for the Tokenizer class\n",
        "def custom_analyzer(text):\n",
        "  '''Custom analyzer to provide to the `Tokenizer` class when creating the tokenizer.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text that will be tokenized\n",
        "\n",
        "  Returns:\n",
        "    returns the splitted sentence\n",
        "  '''\n",
        "  # Remove START and END before standardizing\n",
        "  if START_TOKEN in text:\n",
        "    text = re.sub(f'{START_TOKEN} ', '', text)\n",
        "  if END_TOKEN in text:\n",
        "    text = re.sub(f'{END_TOKEN} ', '', text)\n",
        "\n",
        "  # Standardize the text first\n",
        "  text = standardizer(text)\n",
        "\n",
        "  # Add back the START and END tokens\n",
        "  text = ' '.join([START_TOKEN, text, END_TOKEN])\n",
        "\n",
        "  # Split the sentence into words to tokenize\n",
        "  words = text.split(' ')\n",
        "  words = [word.strip() for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "FD2h0OiAxJP_",
        "outputId": "0762d4df-9b82-4ce2-9b68-4db03a0500d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n",
        "                \"U.S won the world cup and bagged 1.78 million dollars.\",\n",
        "                \"India had M.S. Dhoni won made it this far.\",\n",
        "                \"My email address is arupjana7365@gmail.com.\",\n",
        "                \"It can take care of dailymail single opening quote’ also.\",\n",
        "                \"I have 10,000 Rs in my bank\",\n",
        "                \"This sentence has , after a number 12,\",\n",
        "                \"This sentence contains <START> token and <END> token.\"]\n",
        "\n",
        "print(f\"After Standardizing the sample texts:\\n{[standardizer(text) for text in sample_texts]}\\n\")\n",
        "print(f\"After applying custom analyzer on sample texts:\\n{[custom_analyzer(text) for text in sample_texts]}\")"
      ],
      "metadata": {
        "id": "AVi3kZhcLXS7",
        "outputId": "0dff8843-fba9-455a-c9aa-86544e12b8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Standardizing the sample texts:\n",
            "['i have been working on , but never did it in this way .', 'u . s won the world cup and bagged 1.78 million dollars .', 'india had m . s . dhoni won made it this far .', 'my email address is arupjana7365@gmail . com .', \"it can take care of dailymail single opening quote ' also .\", 'i have 10,000 rs in my bank', 'this sentence has , after a number 12,', 'this sentence contains < start > token and < end > token .']\n",
            "\n",
            "After applying custom analyzer on sample texts:\n",
            "[['<START>', 'i', 'have', 'been', 'working', 'on', ',', 'but', 'never', 'did', 'it', 'in', 'this', 'way', '.', '<END>'], ['<START>', 'u', '.', 's', 'won', 'the', 'world', 'cup', 'and', 'bagged', '1.78', 'million', 'dollars', '.', '<END>'], ['<START>', 'india', 'had', 'm', '.', 's', '.', 'dhoni', 'won', 'made', 'it', 'this', 'far', '.', '<END>'], ['<START>', 'my', 'email', 'address', 'is', 'arupjana7365@gmail', '.', 'com', '.', '<END>'], ['<START>', 'it', 'can', 'take', 'care', 'of', 'dailymail', 'single', 'opening', 'quote', \"'\", 'also', '.', '<END>'], ['<START>', 'i', 'have', '10,000', 'rs', 'in', 'my', 'bank', '<END>'], ['<START>', 'this', 'sentence', 'has', ',', 'after', 'a', 'number', '12,', '<END>'], ['<START>', 'this', 'sentence', 'contains', 'token', 'and', 'token', '.', '<END>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n",
        "\n",
        "When fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab."
      ],
      "metadata": {
        "id": "IiiWRFR64jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(texts, num_words, oov_token=None, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n",
        "  '''This will create the tokenizer needed for the task in hand.\n",
        "  The tokenizer will be trained on the `texts`. Tokenizer will have vocabulary length `num_words`.\n",
        "  The `oov_token` will be used as the token represent the out-of-vocabulary words. The `filters` are\n",
        "  the ones which the tokenizer will remove when tokenizing any sentence given to it. The returned\n",
        "  tokenizer is using a custom analyzer that can standardize the sentence before tokenizing using the\n",
        "  `standardizer` function and then splits the sentence into words. After that it tokenizes the sentence.\n",
        "  As for the vocabulary, the returned tokenizer's vocabulary does not contain any number, as I have removed\n",
        "  them before feeding them into `Tokenizer.fit_on_texts` method.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the tokenizer will be trained on this strings\n",
        "    num_words: int, number of vocabulary words the tokenizer will consider\n",
        "    oov_token: str, token to represent out-of-vocabulary words\n",
        "    filters: str, all the characters that the tokenizer will remove before tokenizing\n",
        "\n",
        "  Returns:\n",
        "    tokenzier of the `Tokenizer` class after learning vocabulary from `texts`\n",
        "  '''\n",
        "\n",
        "  # Create the tokenizer usinf Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=num_words,\n",
        "                        filters=filters,\n",
        "                        oov_token=oov_token,\n",
        "                        analyzer=custom_analyzer)\n",
        "\n",
        "  # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n",
        "  texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n",
        "\n",
        "  # Fit the data with fit_on_texts method\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "tR1FD3SESd0G",
        "outputId": "018b794f-f1a6-4670-a7f7-29efeec2e95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")"
      ],
      "metadata": {
        "id": "UsmErFoxqimM",
        "outputId": "ea4a54ae-b375-4647-8a49-7dcf6734d249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length the articles dataset: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `tokenizer` using the articles from training dataset by using `train_dataset[:, 0]`, with a vocabulary size of `VOCAB_SIZE` and use `OOV_TOKEN` token to represent out-of-vocabulary words."
      ],
      "metadata": {
        "id": "6KBXcUV2pYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)"
      ],
      "metadata": {
        "id": "151rEpqo7Pof",
        "outputId": "e83f1381-2617-4756-d01e-f09dbd179f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"{OOV_TOKEN} word has index: {tokenizer.word_index[OOV_TOKEN]}\")\n",
        "print(f\"{START_TOKEN} word has index: {tokenizer.word_index[START_TOKEN]}\")\n",
        "print(f\"{END_TOKEN} word has index: {tokenizer.word_index[END_TOKEN]}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\\n\")\n",
        "\n",
        "print(f\"Text:\\n{train_dataset[0, 0]}\\n\\n\")\n",
        "sample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\n",
        "print(f\"Text to Sequence of the first article:\\n{sample_sequence}\\n\")\n",
        "print(f\"Sequence to Text of the first acrticle:\\n{tokenizer.sequences_to_texts(sample_sequence)}\")"
      ],
      "metadata": {
        "id": "YUNreIjr8Kng",
        "outputId": "16d0417a-0990-4144-ae92-660039ebf008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary for the tokenizer has a length 252438\n",
            "\n",
            "\n",
            "<OOV> word has index: 1\n",
            "<START> word has index: 77\n",
            "<END> word has index: 78\n",
            "\n",
            "\n",
            "'teacher' word has index: 1575\n",
            "\n",
            "Text:\n",
            "teenagers who use cannabis regularly risk damaging their memory, scientists say – in turn leading to poor academic performance. they believe the brain abnormalities last for ‘at least a few years’ after users have stopped taking the drug. the researchers also said there was fresh evidence the habit may cause mental health problems in youngsters predisposed to schizophrenia. researchers found that the steroid hormone pregnenolone reduces the brain's sensitivity to thc, which is the chief high-inducing compound in cannabis. marijuana is the most commonly used. illicit drug among adolescents in the uk, with more than four in ten. admitting having taken it. almost 100 teenagers took part in the us research examining the effects of cannabis deep in the brain. it. found teenagers who smoked it daily for about three years had abnormal. changes in the brain structures related to remembering and processing. information and they performed poorly on memory tasks. the. brain abnormalities and memory problems were found on mri scans when. study participants were in their early twenties – two years after they. had stopped smoking the drug. memory-related. structures in their brains appeared to shrink and collapse inward and. the researchers said such damage was linked to poor academic performance. and everyday functioning. the study also shows the marijuana-related brain abnormalities look similar  to those seen in schizophrenia patients. lead. author matthew smith, a professor in psychiatry and behavioural. sciences at northwestern university feinberg school of medicine,. chicago, said: ‘the study links the chronic use of marijuana to these. concerning brain abnormalities that appear to last for at least a few. years after people stop using it. ‘with the movement to decriminalise marijuana, we need more research to understand its effect on the brain.’ a naturally occurring hormone called pregnenolone  can be used to stop the 'high' produced by cannabis and the discovery could lead to new approaches to treating cannabis intoxication and dependence, according to new research. he said chronic cannabis use may lead to changes in brain structure associated with having schizophrenia. of. the 15 marijuana smokers who had schizophrenia in the study, 90 per. cent had started using the drug heavily before they developed the mental. disorder. a recent study found teenagers who use cannabis regularly risk damaging their memory as structures in their brains appeared to shrink and collapse inward, possibly reflecting a decrease in neurons. professor smith. added: ‘if someone has a family history of schizophrenia, they are. increasing their risk of developing schizophrenia if they abuse. marijuana. ‘if you have. schizophrenia and you frequently smoke marijuana, you may be at an. increased risk for poor working memory, which predicts your everyday. functioning.’ the study,. published in the journal schizophrenia bulletin, is the first to target. key brain regions in the deep grey matter of chronic marijuana users. with mri scanning. it is. also the first time abnormalities in these regions have been linked with. an impaired working memory – the ability to remember and process. information in the moment and then transfer it to long-term memory. participants. started using marijuana daily aged 16-17 for about three years – and. had been free of the drug for around two years at the time of the study. the 97 teenagers who took part included healthy people, those with a marijuana use disorder and schizophrenia patients. the. younger the individuals were when they started chronically using. marijuana, the more abnormally their brain regions were shaped,. suggesting parts related to memory are more susceptible to the effects. of the drug if abuse starts at an earlier age. under. the labour government cannabis was downgraded from class b to class c. in 2004, which critics argued gave the ‘green light’ to use by. youngsters. the decision was reversed in 2008 when ministers decided to overturn official scientific advice and return it to class b.\n",
            "\n",
            "\n",
            "Text to Sequence of the first article:\n",
            "[[77, 3584, 41, 249, 4587, 2301, 634, 4708, 44, 2411, 4, 1336, 158, 245, 10, 797, 830, 6, 1238, 4727, 1139, 2, 35, 355, 3, 1203, 17323, 81, 15, 5, 26, 365, 7, 292, 88, 5, 47, 1003, 28, 1211, 352, 3, 622, 2, 3, 1394, 67, 21, 62, 16, 1918, 477, 3, 6568, 131, 778, 1552, 239, 696, 10, 5048, 1, 6, 13267, 2, 1394, 134, 14, 3, 15589, 8180, 1, 11753, 3, 1203, 5, 13, 9216, 6, 1, 4, 57, 18, 3, 464, 189, 11, 15922, 3838, 10, 4587, 2, 2080, 18, 3, 116, 5258, 195, 2, 9622, 622, 353, 17185, 10, 3, 416, 4, 22, 56, 75, 175, 10, 1253, 2, 4784, 291, 252, 19, 2, 458, 1, 3584, 207, 184, 10, 3, 172, 630, 6964, 3, 2062, 8, 4587, 1334, 10, 3, 1203, 2, 19, 2, 134, 3584, 41, 8425, 19, 550, 15, 55, 110, 88, 43, 11293, 2, 1182, 10, 3, 1203, 5355, 1165, 6, 9849, 9, 6526, 2, 383, 9, 35, 2536, 7040, 17, 2411, 7218, 2, 3, 2, 1203, 17323, 9, 2411, 696, 45, 134, 17, 11519, 7219, 54, 2, 708, 3785, 45, 10, 44, 349, 13595, 245, 69, 88, 47, 35, 2, 43, 1211, 2845, 3, 622, 2, 2411, 11, 1165, 2, 5355, 10, 44, 7448, 686, 6, 12467, 9, 3498, 1, 9, 2, 3, 1394, 21, 171, 1071, 16, 1681, 6, 1238, 4727, 1139, 2, 9, 4172, 8748, 2, 3, 708, 67, 463, 3, 2080, 11, 1165, 1203, 17323, 276, 766, 6, 129, 238, 10, 13267, 858, 2, 568, 2, 1838, 3093, 972, 4, 7, 1187, 10, 15798, 9, 16108, 2, 6965, 26, 8435, 335, 1, 190, 8, 2648, 4, 2, 1789, 4, 21, 23, 5, 3, 708, 3435, 3, 5517, 249, 8, 2080, 6, 156, 2, 6534, 1203, 17323, 14, 1163, 6, 81, 15, 26, 365, 7, 292, 2, 88, 47, 63, 471, 382, 19, 2, 5, 22, 3, 1318, 6, 1, 2080, 4, 42, 241, 56, 630, 6, 1001, 84, 1386, 17, 3, 1203, 2, 5, 7, 4680, 8706, 8180, 186, 1, 66, 31, 195, 6, 471, 3, 5, 189, 5, 1941, 29, 4587, 9, 3, 2441, 93, 568, 6, 68, 6595, 6, 4237, 4587, 16717, 9, 14273, 4, 128, 6, 68, 630, 2, 20, 21, 5517, 4587, 249, 131, 568, 6, 1182, 10, 1203, 3061, 1645, 22, 291, 13267, 2, 8, 2, 3, 1, 2080, 10081, 41, 43, 13267, 10, 3, 708, 4, 1, 228, 2, 336, 43, 379, 382, 3, 622, 2569, 96, 35, 1791, 3, 1552, 2, 3056, 2, 7, 424, 708, 134, 3584, 41, 249, 4587, 2301, 634, 4708, 44, 2411, 24, 5355, 10, 44, 7448, 686, 6, 12467, 9, 3498, 1, 4, 2435, 9194, 7, 8605, 10, 1, 2, 1187, 972, 2, 223, 23, 5, 73, 531, 32, 7, 118, 498, 8, 13267, 4, 35, 34, 2, 2369, 44, 634, 8, 2378, 13267, 73, 35, 916, 2, 2080, 2, 5, 73, 52, 28, 2, 13267, 9, 52, 3695, 2326, 2080, 4, 52, 131, 31, 26, 37, 2, 1637, 634, 15, 1238, 332, 2411, 4, 57, 9734, 148, 4172, 2, 8748, 2, 5, 3, 708, 4, 2, 361, 10, 3, 2336, 13267, 12723, 4, 18, 3, 86, 6, 1365, 2, 777, 1203, 3806, 10, 3, 1334, 3644, 838, 8, 5517, 2080, 1003, 2, 22, 11519, 11601, 2, 19, 18, 2, 67, 3, 86, 71, 17323, 10, 156, 3806, 28, 46, 1681, 22, 2, 37, 11116, 332, 2411, 245, 3, 1455, 6, 1275, 9, 668, 2, 383, 10, 3, 691, 9, 117, 2212, 19, 6, 180, 11, 730, 2411, 2, 3785, 2, 379, 382, 2080, 550, 1483, 1, 15, 55, 110, 88, 245, 9, 2, 43, 46, 406, 8, 3, 622, 15, 146, 69, 88, 26, 3, 71, 8, 3, 708, 2, 3, 1, 3584, 41, 207, 184, 923, 1646, 63, 4, 129, 22, 7, 2080, 249, 3056, 9, 13267, 858, 2, 3, 2, 1680, 3, 1816, 45, 54, 35, 379, 1, 382, 2, 2080, 4, 3, 56, 1, 44, 1203, 3806, 45, 4473, 4, 2, 3649, 1132, 1165, 6, 2411, 34, 56, 12113, 6, 3, 2062, 2, 8, 3, 622, 73, 916, 2807, 26, 37, 354, 517, 2, 181, 2, 3, 1117, 130, 4587, 16, 15748, 30, 942, 1696, 6, 942, 1183, 2, 10, 1, 57, 1882, 2137, 614, 3, 5, 991, 737, 5, 6, 249, 29, 2, 5048, 2, 3, 430, 16, 8945, 10, 1, 54, 2842, 752, 6, 10917, 384, 3433, 2014, 9, 567, 19, 6, 942, 1696, 2, 78]]\n",
            "\n",
            "Sequence to Text of the first acrticle:\n",
            "[\"<START> teenagers who use cannabis regularly risk damaging their memory , scientists say – in turn leading to poor academic performance . they believe the brain abnormalities last for ' at least a few years ' after users have stopped taking the drug . the researchers also said there was fresh evidence the habit may cause mental health problems in youngsters <OOV> to schizophrenia . researchers found that the steroid hormone <OOV> reduces the brain ' s sensitivity to <OOV> , which is the chief high - inducing compound in cannabis . marijuana is the most commonly used . illicit drug among adolescents in the uk , with more than four in ten . admitting having taken it . almost <OOV> teenagers took part in the us research examining the effects of cannabis deep in the brain . it . found teenagers who smoked it daily for about three years had abnormal . changes in the brain structures related to remembering and processing . information and they performed poorly on memory tasks . the . brain abnormalities and memory problems were found on mri scans when . study participants were in their early twenties – two years after they . had stopped smoking the drug . memory - related . structures in their brains appeared to shrink and collapse <OOV> and . the researchers said such damage was linked to poor academic performance . and everyday functioning . the study also shows the marijuana - related brain abnormalities look similar to those seen in schizophrenia patients . lead . author matthew smith , a professor in psychiatry and behavioural . sciences at northwestern university <OOV> school of medicine , . chicago , said : ' the study links the chronic use of marijuana to these . concerning brain abnormalities that appear to last for at least a few . years after people stop using it . ' with the movement to <OOV> marijuana , we need more research to understand its effect on the brain . ' a naturally occurring hormone called <OOV> can be used to stop the ' high ' produced by cannabis and the discovery could lead to new approaches to treating cannabis intoxication and dependence , according to new research . he said chronic cannabis use may lead to changes in brain structure associated with having schizophrenia . of . the <OOV> marijuana smokers who had schizophrenia in the study , <OOV> per . cent had started using the drug heavily before they developed the mental . disorder . a recent study found teenagers who use cannabis regularly risk damaging their memory as structures in their brains appeared to shrink and collapse <OOV> , possibly reflecting a decrease in <OOV> . professor smith . added : ' if someone has a family history of schizophrenia , they are . increasing their risk of developing schizophrenia if they abuse . marijuana . ' if you have . schizophrenia and you frequently smoke marijuana , you may be at an . increased risk for poor working memory , which predicts your everyday . functioning . ' the study , . published in the journal schizophrenia bulletin , is the first to target . key brain regions in the deep grey matter of chronic marijuana users . with mri scanning . it is . also the first time abnormalities in these regions have been linked with . an impaired working memory – the ability to remember and process . information in the moment and then transfer it to long - term memory . participants . started using marijuana daily aged <OOV> for about three years – and . had been free of the drug for around two years at the time of the study . the <OOV> teenagers who took part included healthy people , those with a marijuana use disorder and schizophrenia patients . the . younger the individuals were when they started <OOV> using . marijuana , the more <OOV> their brain regions were shaped , . suggesting parts related to memory are more susceptible to the effects . of the drug if abuse starts at an earlier age . under . the labour government cannabis was downgraded from class b to class c . in <OOV> which critics argued gave the ' green light ' to use by . youngsters . the decision was reversed in <OOV> when ministers decided to overturn official scientific advice and return it to class b . <END>\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\n",
        "NO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that."
      ],
      "metadata": {
        "id": "crn5t_zk6iBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.keys())[21000]"
      ],
      "metadata": {
        "id": "qhttL-heeP-P",
        "outputId": "fce42a9d-dc80-4c26-f92b-6c61f9d8398e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thirties'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov_word = list(tokenizer.word_index.keys())[21000]\n",
        "sample_text = f\"This example is to test the above fact with the word `{oov_word}`\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "S_Dnkcig7SIX",
        "outputId": "f3fd9f6a-412d-4e6b-a1bc-d7beaef74039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This example is to test the above fact with the word `thirties`\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> this example is to test the above fact with the word ` <OOV> ` <END>']\n",
            "Sequence: [[77, 38, 1089, 18, 6, 840, 3, 754, 545, 22, 3, 1294, 14876, 1, 14876, 78]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`."
      ],
      "metadata": {
        "id": "xvNeazXb-Yq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add a number 2.1 in this sentence!\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ng5MZ_DZ84kk",
        "outputId": "05ead593-1092-4856-df5d-666504ce9d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add a number 2.1 in this sentence!\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add a number <OOV> in this sentence ! <END>']\n",
            "Sequence: [[77, 70, 2064, 54, 27, 1949, 7, 257, 1, 10, 38, 1101, 294, 78]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add parenthesis (I am inside it!).\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ImlMHniH-Jnt",
        "outputId": "5e89f19b-9ec3-4813-9513-7ef40bd087d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add parenthesis (I am inside it!).\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add <OOV> ( i am inside it ! ) . <END>']\n",
            "Sequence: [[77, 70, 2064, 54, 27, 1949, 1, 50, 27, 329, 507, 19, 294, 49, 2, 78]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the `tokenizer` to tokenize the articles and summaries. We need to pad those sequences to fit the requirements.\n",
        "\n",
        "In the paper, the articles are limited to have 400 tokens and summary has, 100 tokens at training and 120 tokens for testing.\n",
        "\n",
        "I will be using `pad_sequences` method to pad or truncate the articles and summaries based on their length.\n",
        "\n",
        "NOTE: I am using same tokenizer for article and summary. But, later I might change that to 2 different tokenizers each having different `num_words`."
      ],
      "metadata": {
        "id": "jza9oQYKXB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pad(texts, tokenizer, padding, truncating, maxlen):\n",
        "  '''Tokenize the `texts` using the tokenizer. Then, pad the sequences or truncate the sequences\n",
        "  depending the length. If the length exceeds `maxlen` then it will be truncated and if not then it will be\n",
        "  padded. The padding and truncating can happend at the beginning or at the end of the sequence depending\n",
        "  on the value of `padding` and `truncating` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the sentences to tokenize and pad\n",
        "    tokenizer: Tokenizer class object, helps in tokenizing the `texts`\n",
        "    padding: str, can take 2 values `pre` or `post`. If `pre` then padding will happen at the beginning,\n",
        "    if `post` then padding will happen at the end.\n",
        "    truncating: str, can take 2 values `pre` or 'truncating`, works the same as `padding`\n",
        "    maxlen: int, maximum length after padding or truncating\n",
        "\n",
        "  Returns:\n",
        "    returns the tokenized and padded sentences\n",
        "  '''\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Alo70WYjW-tA",
        "outputId": "a5c87012-d7df-43ca-b465-8306830167ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "rttUwCiT4FzJ",
        "outputId": "ba9e0e2b-1040-4aba-824d-3202c1f6d1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have been working on, \\nbut \\tnever did it in this way.',\n",
              " 'U.S won the world cup and bagged 1.78 million dollars.',\n",
              " 'India had M.S. Dhoni won made it this far.',\n",
              " 'My email address is arupjana7365@gmail.com.',\n",
              " 'It can take care of dailymail single opening quote’ also.',\n",
              " 'I have 10,000 Rs in my bank',\n",
              " 'This sentence has , after a number 12,',\n",
              " 'This sentence contains <START> token and <END> token.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_pad(sample_texts, tokenizer, padding=\"post\", truncating=\"post\", maxlen=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "QNuBKpTq4lZW",
        "outputId": "0981b73e-8061-481b-c8cb-76d9de843eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   77,    27,    28,    46,   332,    17,     4,    33,   201,\n",
              "          149,    19,    10,    38,   143,     2,    78,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,   111,     2,    13,   301,     3,    97,   398,     9,\n",
              "        16849,     1,   164,  2126,     2,    78,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,  1017,    43,   138,     2,    13,     2,     1,   301,\n",
              "          121,    19,    38,   328,     2,    78,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,    87,  2664,  1255,    18,     1,     2,   607,     2,\n",
              "           78,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,    19,    66,   150,   360,     8, 18184,   845,  1092,\n",
              "         7945,     5,    67,     2,    78,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,    27,    28,     1, 17596,    10,    87,   859,    78,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,    38,  1101,    32,     4,    47,     7,   257,     1,\n",
              "           78,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   77,    38,  1101,  3958, 17020,     9, 17020,     2,    78,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Geneator function that generates the source and target for training the model."
      ],
      "metadata": {
        "id": "ZUtzIfXm6UnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example_v1(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n",
        "  '''Generates examples for the model. Processes the `inputs` and `targets` with their respective\n",
        "  tokenizers and tokenize them to `input_len` and `target_len` length.\n",
        "\n",
        "  Arguments:\n",
        "    inputs: list of input sentences\n",
        "    targets: list of target sentences\n",
        "    input_tokenizer: Tokenizer class object, tokenizer for inputs\n",
        "    target_tokenizer: Tokenizer class object, tokenizer for targets\n",
        "    input_len: int, the length of the tokenization for inputs\n",
        "    target_len: int, the length of the tokenization for targets\n",
        "\n",
        "  Returns:\n",
        "    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n",
        "    another numpy array target_tokens[1:]\n",
        "  '''\n",
        "\n",
        "  for inp, tar in zip(inputs, targets):\n",
        "    inp_tokens = tokenize_pad([inp],\n",
        "                              input_tokenizer,\n",
        "                              padding=\"post\",\n",
        "                              truncating=\"post\",\n",
        "                              maxlen=input_len)\n",
        "\n",
        "    tar_tokens = tokenize_pad([tar],\n",
        "                 target_tokenizer,\n",
        "                 padding=\"post\",\n",
        "                 truncating=\"post\",\n",
        "                 maxlen=target_len)\n",
        "\n",
        "    yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8647M-ey6n19",
        "outputId": "531c5d8d-e454-4e58-c72c-7b7427c76bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example generated by the generator :\")\n",
        "\n",
        "# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\n",
        "example_gen = generate_example_v1(list(train_dataset[:, 0]),\n",
        "                               list(train_dataset[:, 1]),\n",
        "                               input_tokenizer=tokenizer,\n",
        "                               target_tokenizer=tokenizer,\n",
        "                               input_len=MAX_ARTICLE_TOKENS,\n",
        "                               target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "inps, tar = next(example_gen)\n",
        "print(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Target:\\n{tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BKKyIeVa5B20",
        "outputId": "883df22b-7430-41d0-af77-2ecff7366874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example generated by the generator :\n",
            "Inputs:\n",
            "[   77  3584    41   249  4587  2301   634  4708    44  2411     4  1336\n",
            "   158   245    10   797   830     6  1238  4727  1139     2    35   355\n",
            "     3  1203 17323    81    15     5    26   365     7   292    88     5\n",
            "    47  1003    28  1211   352     3   622     2     3  1394    67    21\n",
            "    62    16  1918   477     3  6568   131   778  1552   239   696    10\n",
            "  5048     1     6 13267     2  1394   134    14     3 15589  8180     1\n",
            " 11753     3  1203     5    13  9216     6     1     4    57    18     3\n",
            "   464   189    11 15922  3838    10  4587     2  2080    18     3   116\n",
            "  5258   195     2  9622   622   353 17185    10     3   416     4    22\n",
            "    56    75   175    10  1253     2  4784   291   252    19     2   458\n",
            "     1  3584   207   184    10     3   172   630  6964     3  2062     8\n",
            "  4587  1334    10     3  1203     2    19     2   134  3584    41  8425\n",
            "    19   550    15    55   110    88    43 11293     2  1182    10     3\n",
            "  1203  5355  1165     6  9849     9  6526     2   383     9    35  2536\n",
            "  7040    17  2411  7218     2     3     2  1203 17323     9  2411   696\n",
            "    45   134    17 11519  7219    54     2   708  3785    45    10    44\n",
            "   349 13595   245    69    88    47    35     2    43  1211  2845     3\n",
            "   622     2  2411    11  1165     2  5355    10    44  7448   686     6\n",
            " 12467     9  3498     1     9     2     3  1394    21   171  1071    16\n",
            "  1681     6  1238  4727  1139     2     9  4172  8748     2     3   708\n",
            "    67   463     3  2080    11  1165  1203 17323   276   766     6   129\n",
            "   238    10 13267   858     2   568     2  1838  3093   972     4     7\n",
            "  1187    10 15798     9 16108     2  6965    26  8435   335     1   190\n",
            "     8  2648     4     2  1789     4    21    23     5     3   708  3435\n",
            "     3  5517   249     8  2080     6   156     2  6534  1203 17323    14\n",
            "  1163     6    81    15    26   365     7   292     2    88    47    63\n",
            "   471   382    19     2     5    22     3  1318     6     1  2080     4\n",
            "    42   241    56   630     6  1001    84  1386    17     3  1203     2\n",
            "     5     7  4680  8706  8180   186     1    66    31   195     6   471\n",
            "     3     5   189     5  1941    29  4587     9     3  2441    93   568\n",
            "     6    68  6595     6  4237  4587 16717     9 14273     4   128     6\n",
            "    68   630     2    20    21  5517  4587   249   131   568     6  1182\n",
            "    10  1203  3061  1645    22   291 13267     2     8     2     3     1\n",
            "  2080 10081    41    43]\n",
            "[   77     3  1680     3  1003     4     3    56     1    44     2  7448\n",
            "    45  4473     2   275     4     3 17323    93    28  7206    96    35\n",
            "   195  4587     2  2080    18   116  5258   195  9622   622   353 17185\n",
            "    10   416     2    78     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "\n",
            "Target:\n",
            "[    3  1680     3  1003     4     3    56     1    44     2  7448    45\n",
            "  4473     2   275     4     3 17323    93    28  7206    96    35   195\n",
            "  4587     2  2080    18   116  5258   195  9622   622   353 17185    10\n",
            "   416     2    78     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the Inputs:\\n{inps[0].shape}\\n{inps[1].shape}\\n\\n\")\n",
        "print(f\"Shape of the Target:\\n{tar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "E-w0hmecDcho",
        "outputId": "05610644-a459-43f6-a8d2-50f6bdd8b946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the Inputs:\n",
            "(400,)\n",
            "(99,)\n",
            "\n",
            "\n",
            "Shape of the Target:\n",
            "(99,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data Type of the Inputs data:\\n{inps[0].dtype}\\n{inps[1].dtype}\\n\\n\")\n",
        "print(f\"Data Type of the Target data:\\n{tar.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "suZmEEhfDvcu",
        "outputId": "cb7b73a2-54c4-4b88-a8b3-8005daa77c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Type of the Inputs data:\n",
            "int32\n",
            "int32\n",
            "\n",
            "\n",
            "Data Type of the Target data:\n",
            "int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inps, tar = next(example_gen)\n",
        "print(f\"Second Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Second Target:\\n{tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "82m4io2PSyt3",
        "outputId": "25a13b99-b0a5-48a8-d320-f7f7341e82a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second Inputs:\n",
            "[   77    29     2   550   599  1223     2   361    23     2     1   259\n",
            "     4     1   646     1   587     2   504    23     2     1   259     4\n",
            "     1   646     1   175    63    45   253    54     7     1  4933    76\n",
            "     7  7542  3513     7  2933  3660  2922     4    96     1   314     3\n",
            " 10746     9  6182    76    69 14042  1872     2    90    21     3   953\n",
            "     8     3  2645    16  2611     1     2    17     3  6888     1    10\n",
            "  6000   500   490    17   268   205    54    20   381     3  7542     9\n",
            "     2   402   459     8     3  1097     2     3  2645 19881    76     3\n",
            "     1  8394    96     1    76     7 10020    11  5310     9    37  5685\n",
            "     2  1179    23     3  2853   110    11  1097     1    16  4769    29\n",
            "     7  7542  3513     7  2933  3660  2922     2  3174    23   175    63\n",
            "    45   253    47     7  2142   381     7  7542     2     3  1179    16\n",
            "   780    29     7  7542  3513     3   593     2   212  1955     1   263\n",
            "   110   243     9    53   266   286    26     3   626     4    91     8\n",
            "  1362    45    40   980  1652 11476     2     3     2  2645     5    13\n",
            "   953     4     1    11   102 11452   963    30   269  1546     4    16\n",
            "     2   253     4   440    22  2089     1    11   102  2760     1    30\n",
            "     1     4     2  1738     2   963    16  1409    40   980     7  1652\n",
            "  4442     2    69     1     4  3660     4   898  2805     2    10     7\n",
            " 13193     1  5685    67   286    23     1    11   102  7467     2     1\n",
            "     9     1    11   102     1  3105     4    41    16  1409    40   980\n",
            "     7  1652  4442     2   173  2089    10     3  5685     4     1    11\n",
            "   102 14882  1200     4    16   955     2   212  1955     1    67   263\n",
            "     3   953     8     3  5310     4     1    11   102     1     1    30\n",
            "  5478     4 10766     4  3237 18847     2     3     1    16  2302   106\n",
            "  1773   175   763  1446  4392     4    33    16  8014   349   298     2\n",
            "  1990    23  7467     1     4     1    30     1     4  3660     4    16\n",
            "   253    10     3  3174  1179     2  5330    23 14882  1200     4     1\n",
            "    30     1     4  3660     4    16   955    10     3  1990   110    11\n",
            "  1097  4507     2    78     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[   77   110   243     9    53   266    45   253    54     7  2645   381\n",
            "     7  7542    17     7  3660  2922     9 19608    76    69    92  1872\n",
            "     2    53   153    16   955     2    78     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "\n",
            "Second Target:\n",
            "[  110   243     9    53   266    45   253    54     7  2645   381     7\n",
            "  7542    17     7  3660  2922     9 19608    76    69    92  1872     2\n",
            "    53   153    16   955     2    78     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, we need the model that we can train on this dataset. The model archietecture will be 3-\n",
        "1. Base-line model: Seq-Seq model with attention mechanism.\n",
        "2. Pointer Generetor model: With seq-seq attention model will be implementing the pointer generator that can either copy words from article or generate words from the pre-defined vocabulary.\n",
        "3. Coverage mechanism: Along with the pointer generator that will take case of the out-of-vocabulary words. Coverage mechanism will help prevent the repetition of the words in the summary."
      ],
      "metadata": {
        "id": "yM6KveyaUmfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base-Line Model: Seq-seq with Attention"
      ],
      "metadata": {
        "id": "UarmKLUIVkjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small demonstration of how Dot layer works"
      ],
      "metadata": {
        "id": "-2S_Jxhf25-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10).reshape(1, 2, 5)\n",
        "example_layer = tf.keras.layers.Dense(units=DENSE1_UNITS)\n",
        "\n",
        "print(f\"After applying dense layer on x of shape:{x.shape}, output has {example_layer(x).shape} shape\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qxBGPuOWKubc",
        "outputId": "b2bea899-26a8-492e-a7e6-607460ad6368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After applying dense layer on x of shape:(1, 2, 5), output has (1, 2, 16) shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.arange(10).reshape(1, 2, 5)\n",
        "x2 = np.arange(10, 22).reshape(1, 2, 6)\n",
        "print(f\"x1: {x1}\\nx2: {x2}\")\n",
        "\n",
        "Dot(axes=1)([x1, x2])"
      ],
      "metadata": {
        "id": "_WLTScTp1x-R",
        "outputId": "2bc05cb3-40c8-4495-9c77-ec2925baaee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: [[[0 1 2 3 4]\n",
            "  [5 6 7 8 9]]]\n",
            "x2: [[[10 11 12 13 14 15]\n",
            "  [16 17 18 19 20 21]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 5, 6), dtype=int64, numpy=\n",
              "array([[[ 80,  85,  90,  95, 100, 105],\n",
              "        [106, 113, 120, 127, 134, 141],\n",
              "        [132, 141, 150, 159, 168, 177],\n",
              "        [158, 169, 180, 191, 202, 213],\n",
              "        [184, 197, 210, 223, 236, 249]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_time_attention_v1(a, s_prev,\n",
        "                       repeater, concatenator, densor_1, densor_2, softmax_layer, dotter):\n",
        "  '''Calculates the attention score and returns the context for the current timestep in the decoder.\n",
        "  Attention mechanism uses encoder outputs `a` of shape `(batch, timesteps, features)` and decoder\n",
        "  previous hidden state `s_prev` of shape `(batch, features)`, then calculates alignment scores `alphas`\n",
        "  for each encoder timestep with the help of energies computed with 2 dense layers using `a` and `s_prev`.\n",
        "\n",
        "  Arguments:\n",
        "    a: tf.Tensor object, encoder output of shape `(batch, timesteps, features)` or `(batch, Tx, 2*n_a)`\n",
        "    s_prev: tf.Tensor object, decoder previous hidden state of shape `(batch, features)` or `(batch, n_s)`\n",
        "    repeater: RepeatVector layer, repeat the `s_prev` `Tx` times\n",
        "    concatenator: Concatenate layer, concatenates `a` and repeated `s_prev`, Concatenates along axis=-1\n",
        "    densor_1: Dense layer, calculates the pertial energies `e`, with `units=d1_units`\n",
        "    refer to `baseline_model` function for details about this variable\n",
        "    densor_2: Dense layer, calculated the energies `energies`, with `units=d2_units`\n",
        "    refer to `baseline_model` function for details about this variable\n",
        "    softmax_layer: Activation layer, computes softmax of the energies and calculates `alphas`, with\n",
        "    `units=article_vocab_size` refer to `baseline_model` function for details about this variable\n",
        "    dotter: Dot layer, Performs dot operation between `alphas` and `a` along axis=1\n",
        "\n",
        "  Returns:\n",
        "    returns the context of shape `(batch, features)`\n",
        "  '''\n",
        "\n",
        "  # Repeat the `s_prev` `Tx` times\n",
        "  s_prev = repeater(s_prev) # (batch, Tx, n_s)\n",
        "\n",
        "  # Concatenate `a` and `s_prev` along axis=-1\n",
        "  concat = concatenator([a, s_prev]) # (batch, Tx, n_a + n_s)\n",
        "\n",
        "  # Apply dense layer to get partial energies e\n",
        "  e = densor_1(concat) # (batch, Tx, d1_units)\n",
        "\n",
        "  # Apply dense layer again to get energies\n",
        "  energies = densor_2(e) # (batch, Tx, d2_units)\n",
        "\n",
        "  # Apply softmax over the energies\n",
        "  alphas = softmax_layer(energies) # (batch, Tx, d2_units)\n",
        "\n",
        "  # Dot the alphas and a along axes=1\n",
        "  context = dotter([alphas, a]) # (batch, d2_units, 2*n_a)\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B_rPlnWardTC",
        "outputId": "19ae4f6d-50e1-4506-9c95-e0f65921d753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model(Tx, Ty,\n",
        "                   emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                   article_vocab_size, summary_vocab_size):\n",
        "  '''This implements the bas-line model archietecture for summarization.\n",
        "  It is a seq-seq model with attention mechanism implemented in it. The encoder take an input\n",
        "  with `Tx` time-steps and summarizes with the help of decoder into Ty words. The encoder and decoder\n",
        "  hidden states are `n_a` and `n_s` dimension respectively. The words are taken from the vocabulary of\n",
        "  article and summary `article_vocab` and `summary_vocab` with size `article_vocab_size` and\n",
        "  `summary_vocab_size` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    Tx: int, length of the input article\n",
        "    Ty: int, length of the output summary\n",
        "    n_a: int, dimension of the encoder hidden states\n",
        "    n_s: int, dimension of the deocder hidden states\n",
        "    d1_units: int, units for the first dense layer in attention mechanism\n",
        "    d2_units: int, units for the second dense layer in attention mechanism\n",
        "    d_units: int, units for the dense layer before output layer\n",
        "    article_vocab_size: int, length of the article vocabulary\n",
        "    summary_vocab_size: int, length of the summary vocabulary\n",
        "\n",
        "  Returns:\n",
        "    returns the base line model\n",
        "  '''\n",
        "  # Defining the input for our model with shape (None, Tx) and (None, Ty) for encoder input and decoder input\n",
        "  X_inp = Input(shape=(Tx))\n",
        "  X_tar = Input(shape=(Ty))\n",
        "\n",
        "  # Initialize s0\n",
        "  s0 = Input(shape=(n_s, ), name=\"s0\")\n",
        "  # Initialize c0\n",
        "  c0 = Input(shape=(n_s, ), name=\"c0\")\n",
        "\n",
        "  # Initialize the a and s with a0 and s0\n",
        "  s = s0 # (batch, n_s)\n",
        "  c = c0 # (batch, n_s)\n",
        "\n",
        "  # Define the outputs as empty list\n",
        "  outputs = []\n",
        "\n",
        "  # First embedding layer for the article input\n",
        "  encoder_inp = Embedding(article_vocab_size, emb_dim)(X_inp) # (batch, Tx, emb_dim)\n",
        "\n",
        "  # Encoder: Bidirectional layer with LSTM cells\n",
        "  a = Bidirectional(LSTM(units=n_a, return_sequences=True))(encoder_inp) # (batch, Tx, n_a)\n",
        "\n",
        "  # Define the embedding for decoder\n",
        "  decoder_inp = Embedding(summary_vocab_size, emb_dim)(X_tar) # (batch, Ty, emb_dim)\n",
        "\n",
        "  # Define the layers for Attention so that we can use the same weights for all decoder timesteps\n",
        "  repeater = RepeatVector(Tx)\n",
        "  concatenator = Concatenate(axis=-1)\n",
        "  attn_densor1 = Dense(units=d1_units, activation='tanh')\n",
        "  attn_densor2 = Dense(units=d2_units, activation='linear', use_bias=False)\n",
        "  softmax_layer = Activation('softmax', name=\"attention_weights\")\n",
        "  dotter = Dot(axes=1)\n",
        "\n",
        "  # Define the Decoder unidirectional LSTM for shared weights\n",
        "  post_attention_lstm = LSTM(units=n_s, return_state=True)\n",
        "\n",
        "  # Define the last dense layer before output layer with linear activation\n",
        "  densor = Dense(units=d_units, activation='linear')\n",
        "\n",
        "  # Define the output layer so that it does not initalize again and again for shared weights\n",
        "  output_layer = Dense(units=summary_vocab_size, activation='softmax')\n",
        "\n",
        "  # Decoder: Appends outputs from the output layer in each timestep\n",
        "  for t in range(Ty):\n",
        "    # Get the decoder input for current timestep\n",
        "    curr_dec_in = decoder_inp[:, t:t+1, :] # (batch, 1, emb_dim)\n",
        "\n",
        "    # Get the context from the attention mechanism\n",
        "    context = one_time_attention_v1(a, s, # (batch, d2_units, 2*n_a)\n",
        "                                 repeater, concatenator, attn_densor1, attn_densor2, softmax_layer, dotter)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([curr_dec_in, context]) # (batch, d2_units, emb_dim+2*n_a); d2_units=1 otherwise error\n",
        "    _, s, c = post_attention_lstm(concat, initial_state=[s, c]) # _, (batch, n_s), (batch, n_s)\n",
        "\n",
        "    # Calculate the output after using 2 linear dense layers\n",
        "    out = densor(s) # (batch, d_units)\n",
        "    out = densor(out) # (batch, d_units)\n",
        "    # Use the output_layer to get the output\n",
        "    out  = output_layer(out) # (batch, summary_vocab_size)\n",
        "\n",
        "    # Append the final output to the outputs list\n",
        "    outputs.append(out)\n",
        "\n",
        "  # Stack the list of each timesteps output along axis=1\n",
        "  outputs = tf.stack(outputs, axis=1) # (batch, Ty, summary_vocab_size)\n",
        "\n",
        "  model = Model(inputs=[X_inp, X_tar, s0, c0], outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mudpzB78HDvZ",
        "outputId": "134c806b-e725-4577-962e-ac64f959f083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tx = MAX_ARTICLE_TOKENS\n",
        "Ty = MAX_SUMMARY_TOKENS - 1\n",
        "emb_dim = EMB_OUT\n",
        "n_a = ENCODER_STATE_DIM\n",
        "n_s= DECODER_STATE_DIM\n",
        "d1_units = DENSE1_UNITS\n",
        "d2_units = DENSE2_UNITS\n",
        "d_units = DENSE_UNITS\n",
        "article_vocab_size = VOCAB_SIZE\n",
        "summary_vocab_size = VOCAB_SIZE\n",
        "\n",
        "model = baseline_model(Tx, Ty,\n",
        "                       emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                       article_vocab_size, summary_vocab_size)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "V3ZAtG5I8mZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the generator to generate the examples, we can customize it for training, validation and testing set. For type of dataset, we will create different generator with the help of the `example_generator` method.\n",
        "\n",
        "We can use `tf.data` API to create the input data pipeline for our model. I will use the `tf.data.Dataset` class to get the the examples from the `train_example_generator` function which uses `generate_example`, we can save the generator inside `example_gen` which we can iterate over later to get the examples. We can yield the examples according to the need of the problem.\n",
        "\n",
        "Remember, along with input article tokens and input summary tokens, we need the initial states as an input to the model. So, as we process the examples we can create this zero-value tensors and yield them along with 2 original inputs.\n",
        "\n",
        "For more about datasets from generator, refer to [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)."
      ],
      "metadata": {
        "id": "usV8UchPA35e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_example_generetor_v1():\n",
        "  example_gen = generate_example_v1(list(train_dataset[:, 0]),\n",
        "                                list(train_dataset[:, 1]),\n",
        "                                input_tokenizer=tokenizer,\n",
        "                                target_tokenizer=tokenizer,\n",
        "                                input_len=MAX_ARTICLE_TOKENS,\n",
        "                                target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "  for example in example_gen:\n",
        "    s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "    c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "\n",
        "    (input_0, input_1), target = example\n",
        "    yield (input_0, input_1, s0, c0), target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_0r7ajRR8wk",
        "outputId": "8cb54c78-f1fe-4559-8b85-a789e7d399d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "tf_train_dataset = tf.data.Dataset.from_generator(generator=train_example_generetor_v1,\n",
        "                                                  output_signature=output_signature)\n",
        "tf_train_dataset = tf_train_dataset.shuffle(BUFFER_SIZE)\n",
        "tf_train_dataset = tf_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "tf_train_dataset = tf_train_dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "Zj-H_fEh5Zu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42279d75-da75-4349-c70c-cda3fc23962a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (art_inp, sum_inp, s0, c0), sum_tar in tf_train_dataset.take(1):\n",
        "  print(f\"Input tokenized article shape: {art_inp.shape}\")\n",
        "  print(f\"Input tokenized summary shape: {sum_inp.shape}\\n\")\n",
        "\n",
        "  print(f\"Target tokenized summary shape: {sum_tar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no4oMl22Fm6r",
        "outputId": "a116d0d2-8ef4-46c8-ab62-dfe62e49ddfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokenized article shape: (16, 400)\n",
            "Input tokenized summary shape: (16, 99)\n",
            "\n",
            "Target tokenized summary shape: (16, 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A look into how model will output on the above input."
      ],
      "metadata": {
        "id": "sN_nIO_-LdDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model_out = model((art_inp, sum_inp, s0, c0))\n",
        "\n",
        "print(f\"Model output has a type: {type(sample_model_out)}\")\n",
        "print(f\"Model Output list for the Inputs above are of length: {len(sample_model_out)}\")\n",
        "print(f\"Model Output list has each output of shape: {sample_model_out[0].shape}\")"
      ],
      "metadata": {
        "id": "xzqmZfKoLcfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model on whether it is compatible with training set or the model is performing as indended or the model architecture should be changed. To do that I will use a simple predefined loss function and optimizer without taking, whether those are right choice for us, into account. So that we can quickly find out any potential problem in our model."
      ],
      "metadata": {
        "id": "Phi6-CEN36_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.fit(tf_train_dataset, epochs=1, steps_per_epochs=10)"
      ],
      "metadata": {
        "id": "wpCcnLEC36IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_v1(y_true, y_pred):\n",
        "  '''Calculates the loss for the baseline model. The loss is calculated by taking the negative\n",
        "  log-likelihood of the target word(w*_t) in the current timestep. Then the overall loss\n",
        "  is the summation over all timesteps divided by T (not Ty because it would include paddings also).\n",
        "\n",
        "  Arguments:\n",
        "    y_true: tf.Tensor object, true values for the target\n",
        "    y_pred: list of tf.Tensor objects, predicted probablities of the summary words\n",
        "\n",
        "  Returns:\n",
        "    returns the loss on the predicted values for the model\n",
        "  '''\n",
        "  # Calculate the loss for each item in the batch.\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "  # Remove the paddings from calculation of loss\n",
        "  mask = tf.cast(y_true != 0, loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  # Divide the total loss after masking out paddings divided by total words which are not paddings\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def custom_accuracy_v1(y_true, y_pred):\n",
        "  '''Calculates accuracy of the baseline model. The accuracy is calculated by matching how many correct\n",
        "  words were predicted excluding the paddings. Then, just add those which are correct and you will get the\n",
        "  the accuracy and then just divide it by total words not including padding.\n",
        "\n",
        "  Arguments:\n",
        "    y_true: tf.Tensor object, expected target values\n",
        "    y_pred: list of tf.Tensor object, predicted target values by model\n",
        "\n",
        "  Returns:\n",
        "    returns the total accuracy over the batch of data\n",
        "  '''\n",
        "  # Find the word index with maximum probablity\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  # Count the words that matches with true values\n",
        "  match = tf.cast(y_pred == y_true, tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  # Mask out the paddings\n",
        "  match *= mask\n",
        "\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "dw-z3KccJdpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sample y true values: {sum_tar}\")\n",
        "print(f\"Sample y pred values(first 10 values of first 2 timestep): {sample_model_out[:2]}\")\n",
        "\n",
        "sample_loss = custom_loss_v1(sum_tar, sample_model_out)\n",
        "print(f\"Loss of the sample y_true and y_pred: {sample_loss}\")\n",
        "\n",
        "sample_acc = custom_accuracy_v1(sum_tar, sample_model_out)\n",
        "print(f\"Accuracy of the sample y_true and y_pred: {sample_acc}\")"
      ],
      "metadata": {
        "id": "SR25pZHrACmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model again so that we can start from scratch\n",
        "Tx = MAX_ARTICLE_TOKENS\n",
        "Ty = MAX_SUMMARY_TOKENS - 1\n",
        "emb_dim = EMB_OUT\n",
        "n_a = ENCODER_STATE_DIM\n",
        "n_s= DECODER_STATE_DIM\n",
        "d1_units = DENSE1_UNITS\n",
        "d2_units = DENSE2_UNITS\n",
        "d_units = DENSE_UNITS\n",
        "article_vocab_size = VOCAB_SIZE\n",
        "summary_vocab_size = VOCAB_SIZE\n",
        "\n",
        "model = baseline_model(Tx, Ty,\n",
        "                       emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                       article_vocab_size, summary_vocab_size)"
      ],
      "metadata": {
        "id": "bOTfC4BNnFgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adagrad(learning_rate=0.015,\n",
        "              initial_accumulator_value=0.1,\n",
        "              clipnorm=2)\n",
        "\n",
        "model.compile(loss=custom_loss_v1, optimizer=opt, metrics=[custom_loss_v1, custom_accuracy_v1])"
      ],
      "metadata": {
        "id": "APnZYH9BEVKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_example_generetor_v1():\n",
        "  example_gen = generate_example_v1(list(val_dataset[:, 0]),\n",
        "                                list(val_dataset[:, 1]),\n",
        "                                input_tokenizer=tokenizer,\n",
        "                                target_tokenizer=tokenizer,\n",
        "                                input_len=MAX_ARTICLE_TOKENS,\n",
        "                                target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "  for example in example_gen:\n",
        "    s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "    c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "\n",
        "    (input_0, input_1), target = example\n",
        "    yield (input_0, input_1, s0, c0), target\n",
        "\n",
        "\n",
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "tf_val_dataset = tf.data.Dataset.from_generator(generator=val_example_generetor_v1,\n",
        "                                                  output_signature=output_signature)\n",
        "tf_val_dataset = tf_val_dataset.shuffle(BUFFER_SIZE)\n",
        "tf_val_dataset = tf_val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "-2qJcIbjRZXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(tf_train_dataset.repeat(),\n",
        "                    epochs=10,\n",
        "                    validation_data=tf_val_dataset,\n",
        "                    steps_per_epoch=5000)"
      ],
      "metadata": {
        "id": "OQE9gr_Bd9cJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}