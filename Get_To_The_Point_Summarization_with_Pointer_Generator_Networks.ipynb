{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMieXruBae9NnhyMF8q0now",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the wraping the outputs of colab"
      ],
      "metadata": {
        "id": "aWWumM-N6C31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "J2_Q63jmdSp3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x6QucUCZ0q2V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c108bcbc-9767-4e83-ec5b-a9355113e540"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# For tokenizing and processing the examples for the model training\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Layers for the Encoder, Attention and Decoder\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.layers import RepeatVector, Concatenate, Activation, Dot\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# For model initialization\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# For training the model\n",
        "from tensorflow.keras.optimizers.experimental import Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CNN stories from the url into cnn_stories_tgz file\n",
        "cnn_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        ")\n",
        "\n",
        "# Download the Dailymail stories from the url into dailymail_stories_tgz file\n",
        "dailymail_stories_tgz = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "k_03wyoLlFhO",
        "outputId": "e667a7cc-07a8-4018-d8b7-be899f63a289"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\n",
            "158577824/158577824 [==============================] - 2s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\n",
            "375893739/375893739 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_tgz, dailymail_stories_tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fyid1XyFl1sp",
        "outputId": "95588ba6-fbc5-46b0-a631-45d2fb88517b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.keras/datasets/cnn_stories.tgz',\n",
              " '/root/.keras/datasets/dailymail_stories.tgz')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /root/.keras/datasets/cnn_stories.tgz\n",
        "!tar -xzf /root/.keras/datasets/dailymail_stories.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EHJtYhZpls0i",
        "outputId": "73918440-5867-4d72-bc7b-a6b931c34301"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir = pathlib.Path('/content/cnn/stories')\n",
        "dailymail_stories_dir = pathlib.Path('/content/cnn/stories')"
      ],
      "metadata": {
        "id": "ag5ubSt7l88l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e42ba1a0-abf2-453c-fd4e-99e855fff277"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir, dailymail_stories_dir"
      ],
      "metadata": {
        "id": "UmMCM--Yhm3B",
        "outputId": "b7b05a09-af6b-4805-fd47-48923ea8c0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/content/cnn/stories'), PosixPath('/content/cnn/stories'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_filenames(dir_path, num_files=5):\n",
        "  '''Prints the name of the files that are present at `dir_path`.\n",
        "  Maximum `num_files` number of files are shown.\n",
        "\n",
        "  Arguments:\n",
        "    dir_path: PosixPath, pointing to the directory of which the user\n",
        "              wants to prints the file names.\n",
        "    num_files: int, number of files user wants to print.\n",
        "\n",
        "  returns:\n",
        "    nothing\n",
        "  '''\n",
        "\n",
        "  count = 0\n",
        "  for f in dir_path.glob('*.story'):\n",
        "    print(f.name)\n",
        "    count += 1\n",
        "\n",
        "    if count == num_files:\n",
        "      break\n",
        "  else:\n",
        "    print(f\"Less than {num_files} is present!\")"
      ],
      "metadata": {
        "id": "dZFFXG7Pg86H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b39689c2-5a39-4c95-d5a7-8eec0587c582"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(cnn_stories_dir)"
      ],
      "metadata": {
        "id": "nv3g7-M316yL",
        "outputId": "e9249d85-9712-45a7-c0d6-39818018cf09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4747b7385747f47a410ad654454cd3bdff09fed2.story\n",
            "52473c68ee49d1212ad99ba1b1fa23eaa0c086be.story\n",
            "abe953a8d687d49d653424990842769c10779f13.story\n",
            "9278263cef5f1e26ffec49e08e20219f284a913c.story\n",
            "e699eeca9bdd1c490f1ee675d66560b686c5934f.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(dailymail_stories_dir)"
      ],
      "metadata": {
        "id": "WcyW1wJa2EaN",
        "outputId": "add97f10-9095-468c-9b81-8e259a1c11ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4747b7385747f47a410ad654454cd3bdff09fed2.story\n",
            "52473c68ee49d1212ad99ba1b1fa23eaa0c086be.story\n",
            "abe953a8d687d49d653424990842769c10779f13.story\n",
            "9278263cef5f1e26ffec49e08e20219f284a913c.story\n",
            "e699eeca9bdd1c490f1ee675d66560b686c5934f.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "dm_single_close_quote = u'\\u2019' # unicode for closing single quote\n",
        "dm_double_close_quote = u'\\u201d' # unicode for closing double quote\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
        "              dm_single_close_quote, dm_double_close_quote, \")\"]\n",
        "\n",
        "# Maximum stories to process from cnn and dailymail each\n",
        "MAX_STORIES = 50000\n",
        "\n",
        "# From the total data how to split into train, val and test\n",
        "TRAIN_SIZE = 0.8 # Fraction of the total dataset to use for training\n",
        "VAL_SIZE = 0.1 # Fraction of the total dataset to use for validation\n",
        "TEST_SIZE = 0.1 # Fraction of the total dataset to use for testing\n",
        "\n",
        "# For tokenization\n",
        "VOCAB_SIZE = 20000 # Vocabulary size or no of unique words\n",
        "OOV_TOKEN = \"<OOV>\" # Word token to represent the out-of-vocabulary words\n",
        "\n",
        "# For standardization\n",
        "START_TOKEN = '<START>' # Starting word of each sentence\n",
        "END_TOKEN = '<END>' # Ending word of each sentence\n",
        "\n",
        "# For the number of tokens to use in representing articles and summaries, hyperparameters\n",
        "MAX_ARTICLE_TOKENS = 400 # Maximum no of tokens to consider in article when processing them for model\n",
        "MAX_SUMMARY_TOKENS = 100 # Maximum no of tokens to consider in summary when processing them for model\n",
        "\n",
        "# For dataset creation hyperparameters\n",
        "BUFFER_SIZE = 5000 # Buffer size when using shuffle\n",
        "BATCH_SIZE = 16 # No of examples in each batch\n",
        "\n",
        "# Model Archietecture hyperparameters\n",
        "EMB_OUT = 32 # Embedding output dimension\n",
        "ENCODER_STATE_DIM = 32 # Encoder hidden(also cell) state dimension\n",
        "DECODER_STATE_DIM = 64 # Decoder hidden(also cell) state dimension\n",
        "DENSE1_UNITS = 16 # Attention first dense layer units(calculates partial energy)\n",
        "DENSE2_UNITS = 1 # Attention secodn dense layer units(calculated final energy)\n",
        "DENSE_UNITS = 64 # Units of the Dense layers before output layer\n",
        "\n",
        "# Model Optimizer hyperparameters\n",
        "LEARNING_RATE=0.15 # Learning rate\n",
        "INIT_ACC_VAL=0.1 # Initial accumulator value\n",
        "MAX_GRAD_NORM=2 # Gardient norm\n",
        "\n",
        "# Model Checkpoint hyperparameters\n",
        "BASELINE_MODEL_CHECKPOINT = \"baseline-model/cp-{epoch:04d}.ckpt\"\n",
        "PATIENCE = 5\n",
        "\n",
        "# Model Fit\n",
        "EPOCHS = 35\n",
        "STEPS_PER_EPOCHS = 5000\n",
        "\n",
        "# Coverage mechanism\n",
        "LAMBDA_VAL = 1"
      ],
      "metadata": {
        "id": "cubLAm1Q3SuG",
        "outputId": "51ac0280-8f2f-4fff-9248-8bde0748e2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a sample .story file from cnn stories\n",
        "sample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\n",
        "sample_filedir = cnn_stories_dir\n",
        "\n",
        "sample_filepath = sample_filedir / sample_filename\n",
        "with open(sample_filepath, 'r') as f:\n",
        "  sample_story = f.read()\n",
        "\n",
        "print(f\"A sample story:\\n{sample_story}\")"
      ],
      "metadata": {
        "id": "Uw0kqchX3X7X",
        "outputId": "8391d402-85fd-48c8-e148-2d4e8a70792d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample story:\n",
            "New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\n",
            "\n",
            "The figure represents a 0.7% increase from last year, adding 2,250,129 people to the U.S. population since the start of 2011, and a 1.3% increase since Census Day, April 1, 2010.\n",
            "\n",
            "The agency estimates that beginning in January, one American will be born every eight seconds and one will die every 12 seconds.\n",
            "\n",
            "U.S.-bound immigrants are also expected to add one person every 46 seconds.\n",
            "\n",
            "That combination of births, deaths and migration is expected to add a single person to the U.S. population every 17 seconds, the Census Bureau said.\n",
            "\n",
            "Meanwhile, millions are set to ring in the new year.\n",
            "\n",
            "In New York, authorities are preparing for large crowds in Manhattan's Times Square, where Lady Gaga is expected to join Mayor Michael Bloomberg to push the button that drops the Waterford Crystal ball at 11:59 p.m. ET on New Year's Eve.\n",
            "\n",
            "\"And I'm so looking forward to performing on NYE+dropping the Ball with Mayor Bloomberg!\" the pop star posted on Twitter. \"What an honor as a New Yorker.\"\n",
            "\n",
            "Past guests have included Muhammad Ali, Rudy Giuliani, Colin Powell and Bill and Hillary Clinton.\n",
            "\n",
            "On Thursday, officials conducted New York's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above Times Square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square.\n",
            "\n",
            "The Big Apple this year edged out Las Vegas for the first time in seven years as the top travel U.S. destination for those celebrating the new year, according to a December travel booking website poll.\n",
            "\n",
            "Seven New York neighborhoods made the top 10 list, with two districts in Las Vegas and one in New Orleans making up the other three, according to the Priceline poll.\n",
            "\n",
            "\"It appears that New York City will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman Brian Ek.\n",
            "\n",
            "@highlight\n",
            "\n",
            "Census Bureau: U.S. population is expected to be 312.8 million on New Year's Day\n",
            "\n",
            "@highlight\n",
            "\n",
            "That figure represents a 0.7% increase from last year\n",
            "\n",
            "@highlight\n",
            "\n",
            "Lady Gaga, mayor to activate ball drop at Times Square on New Year's Eve\n",
            "\n",
            "@highlight\n",
            "\n",
            "NYC has supplanted Las Vegas as the top New Year's destination, Priceline poll says\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n",
        "\n",
        "These are the steps -\n",
        "1. Check if line contains `@highlight`, if True then just return the line.\n",
        "2. Check if line is empty, then return line as it is.\n",
        "3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n",
        "4. Only is none of the above conditions match then append `.` to the current line."
      ],
      "metadata": {
        "id": "iZPt8EVBzJ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_missing_period(line, end_tokens=END_TOKENS):\n",
        "  '''function to fix the missing periods for some story lines which do not end with\n",
        "  any of the end_tokens mentioned.\n",
        "\n",
        "  Argument:\n",
        "    line: string, line of the story to fix the missing the period of.\n",
        "    end_tokens: list of strings, all the tokens that are considered as line end.\n",
        "\n",
        "  Returns:\n",
        "    new line with fixed the ending part by adding an ending token if not present.\n",
        "  '''\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  elif line == \"\":\n",
        "    return line\n",
        "  elif line[-1] in end_tokens:\n",
        "    return line\n",
        "\n",
        "  return line + '.'"
      ],
      "metadata": {
        "id": "i-S-Hss12TPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "dd96ab09-2e3a-46d3-d21b-7508fa4f5899"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"i have a bad habit of not giving full-stop after sentence\\nLike this setence\"\n",
        "print(f\"Fixing {fix_missing_period(sample_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "L1hiNysxnmAO",
        "outputId": "7056e2c0-0d04-4bf3-8909-56f1b8246e9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing i have a bad habit of not giving full-stop after sentence\n",
            "Like this setence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `split_article_summary` which will split the story into article and summary parts.\n",
        "\n",
        "The function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n",
        "\n",
        "The steps to follow are -\n",
        "1. Split the story by new line `\\n`. I will get a list of lines.\n",
        "2. Strip the lines by using list comprehension.\n",
        "3. Use list comprehension to make lower case each line by using `.lower()`.\n",
        "4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n",
        "5. Make 2 empty list for `article` and `summary`.\n",
        "6. Go through each line. In each line, I need to check 4 things,\n",
        "  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n",
        "  * line is `\"\"` empty or not, if True then ignore.\n",
        "  * `next_highlight` is True or not, if True then append the line to `summary`.\n",
        "  * If non of the ebove then append to `article`.\n",
        "7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method."
      ],
      "metadata": {
        "id": "5-tqWtoowXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_article_summary(story):\n",
        "  '''Splits the story into 2 parts, one for article and other for summary of that\n",
        "  article. Returns the article and summary.\n",
        "\n",
        "  Argument:\n",
        "    story: string file that contains both article and summary combiningly.\n",
        "\n",
        "  Returns:\n",
        "    article, summary seperately from the story.\n",
        "\n",
        "  '''\n",
        "  lines = story.split('\\n')\n",
        "  lines = [line.strip() for line in lines]\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  # Fix the ending period\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # List to contain the article and summary lines\n",
        "  article = []\n",
        "  summary = []\n",
        "\n",
        "  # Indicator of whether the next line is the summary or not\n",
        "  next_highlight = False\n",
        "\n",
        "  for line in lines:\n",
        "    if \"@highlight\" in line:\n",
        "      next_highlight = True\n",
        "    elif line==\"\":\n",
        "      continue\n",
        "    elif next_highlight:\n",
        "      summary.append(line)\n",
        "    else:\n",
        "      article.append(line)\n",
        "\n",
        "  article = ' '.join(article)\n",
        "  summary = ' '.join(summary)\n",
        "\n",
        "  return article, summary"
      ],
      "metadata": {
        "id": "-X4eMltQnf10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "383a238a-0d49-44f0-81d6-653337330910"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article, sample_summary = split_article_summary(sample_story)\n",
        "\n",
        "print(f\"Sample Article after spliting:\\n{sample_article}\")\n",
        "print(f\"Sample Summary after spliting:\\n{sample_summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "nuUjOaN9orGU",
        "outputId": "43a8d610-7e25-4f47-f5eb-2040e63c44a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Article after spliting:\n",
            "new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year's eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan's times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year's eve. \"and i'm so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.\n",
            "Sample Summary after spliting:\n",
            "census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n",
        "\n",
        "This function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n",
        "\n",
        "The process is simple. We will follow this steps -\n",
        "1. Create 2 empty lists of `articles` and `summaries`.\n",
        "2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n",
        "3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n",
        "4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n",
        "5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n",
        "6. Return the 2 lists."
      ],
      "metadata": {
        "id": "oTEa0m3Huxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_summaries(stories_dir, max_stories):\n",
        "  '''stores the stories from stories_dir folder into a list and returns the list\n",
        "\n",
        "  Arguments:\n",
        "    stories_dir: Posix string, the directory where the stories are stored\n",
        "    max_stories: maximum number of stories to store\n",
        "\n",
        "  Returns:\n",
        "    list of stories.\n",
        "\n",
        "  '''\n",
        "  articles = []\n",
        "  summaries = []\n",
        "\n",
        "  count = 0\n",
        "  for f in stories_dir.glob(\"*.story\"):\n",
        "    count += 1\n",
        "    with open(f, 'r') as reader:\n",
        "      story = reader.read()\n",
        "\n",
        "      article, summary = split_article_summary(story)\n",
        "\n",
        "      articles.append(article)\n",
        "      summaries.append(summary)\n",
        "\n",
        "    if count == max_stories:\n",
        "      break\n",
        "\n",
        "  return articles, summaries"
      ],
      "metadata": {
        "id": "4VUmbYSpnjAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e20aacb4-d47e-4174-fdf6-5b4692d4b77d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them."
      ],
      "metadata": {
        "id": "Uk-BuCM4rIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aa9ZDQntpHQZ",
        "outputId": "d1ba603c-e0b4-4a3f-8bbf-242b761e5e03"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "-q_i-69YqJnj",
        "outputId": "e3a1956d-bad0-4e0a-bc14-4cdedcd15ccf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: (cnn) -- british innovator james dyson, who has built a multi-billion dollar empire around his distinctive vacuum cleaners, has described patent laws across europe as \"absolute madness\", saying they are unhelpful for inventors and small businesses. dyson told cnn he wants the patent system in europe radically overhauled. over the last four decades, dyson said, he has been affected enormously by people copying his ideas. government leaders are continuously telling businesses that innovation drives the economy. but dyson points to the red tape surrounding the patenting process as being a massive hurdle for businesses wanting to develop ideas. \"the problem with inventing: as soon as you file a patent they see what you are doing and they can see ways to get around it,\" said dyson, who made his fortune inventing a bagless vacuum. the 64-year-old is an outspoken critic of chinese counterfeiters, calling on governments to do more to protect intellectual property rights. problems can arise because of the wording of the patents, dyson said. \"there are no diagrams or drawings and often something hinges on the particular phrasing of the patent.\" according to dyson, if it is obvious someone has copied another person's ideas they should be dealt with without the parties going through a protracted legal battle. \"there shouldn't be this endless rigmarole of 'could this have been devised by one skilled in the art?'\" because the current system involves many expenses due to varying jurisdictions throughout the continent, a europe-wide patent is the answer, according to dyson. \"you have to file in each country, you have to translate in each country, sue in each country, renews in each country -- it's seen as a profit center for each country.\" issues surrounding plagiarism are not limited to businesses, with consumers also feeling the impact of the high cost of producing and securing new inventions, dyson said. energy-saving and cost effective products and technology won't be created because of the enormous upfront investment it takes to develop them, he said \"it's anticompetitive to make copying easy.\" cnn's emily smith contributed to this story.\n",
            "\n",
            "\n",
            "The summary of this article: british innovator james dyson has described patent laws across europe as \"absolute madness\" dyson says the current system involves many expenses due to varying jurisdictions throughout the continent. he says a europe-wide patent is the answer.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n",
        "                                                                 MAX_STORIES)"
      ],
      "metadata": {
        "id": "KT4Hrp6nqeKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fad65ff4-3439-47f1-eb61-84174f2b47b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of dailymail stories captured are {len(dailymail_articles)}\\n\\n\")\n",
        "print(f\"One of the Dailymail articles: {dailymail_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "id": "nkzwSP9kh4VK",
        "outputId": "36707058-8d66-4c9c-b03d-28491124cd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of dailymail stories captured are 50000\n",
            "\n",
            "\n",
            "One of the Dailymail articles: (cnn) -- british innovator james dyson, who has built a multi-billion dollar empire around his distinctive vacuum cleaners, has described patent laws across europe as \"absolute madness\", saying they are unhelpful for inventors and small businesses. dyson told cnn he wants the patent system in europe radically overhauled. over the last four decades, dyson said, he has been affected enormously by people copying his ideas. government leaders are continuously telling businesses that innovation drives the economy. but dyson points to the red tape surrounding the patenting process as being a massive hurdle for businesses wanting to develop ideas. \"the problem with inventing: as soon as you file a patent they see what you are doing and they can see ways to get around it,\" said dyson, who made his fortune inventing a bagless vacuum. the 64-year-old is an outspoken critic of chinese counterfeiters, calling on governments to do more to protect intellectual property rights. problems can arise because of the wording of the patents, dyson said. \"there are no diagrams or drawings and often something hinges on the particular phrasing of the patent.\" according to dyson, if it is obvious someone has copied another person's ideas they should be dealt with without the parties going through a protracted legal battle. \"there shouldn't be this endless rigmarole of 'could this have been devised by one skilled in the art?'\" because the current system involves many expenses due to varying jurisdictions throughout the continent, a europe-wide patent is the answer, according to dyson. \"you have to file in each country, you have to translate in each country, sue in each country, renews in each country -- it's seen as a profit center for each country.\" issues surrounding plagiarism are not limited to businesses, with consumers also feeling the impact of the high cost of producing and securing new inventions, dyson said. energy-saving and cost effective products and technology won't be created because of the enormous upfront investment it takes to develop them, he said \"it's anticompetitive to make copying easy.\" cnn's emily smith contributed to this story.\n",
            "\n",
            "\n",
            "The summary of this article: british innovator james dyson has described patent laws across europe as \"absolute madness\" dyson says the current system involves many expenses due to varying jurisdictions throughout the continent. he says a europe-wide patent is the answer.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating another function -\n",
        "`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples."
      ],
      "metadata": {
        "id": "8ea-PhS3iIJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_size, val_size, test_size):\n",
        "  first_split = train_size\n",
        "  second_split = train_size+val_size\n",
        "  third_split = train_size+val_size+test_size\n",
        "  return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]"
      ],
      "metadata": {
        "id": "v-iZDbUCqkdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0c5bcdf9-b6ed-4df5-adfb-46fa90572a60"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a function `make_datasets`, that will be make training, validation and testing datasets. This function will -\n",
        "1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\n",
        "Objective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n",
        "```python\n",
        "[1, 2] + [3, 4] = [1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n",
        "```python\n",
        "  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n",
        "```\n",
        "4. Shuffle the dataset using `random.sample` method.\n",
        "```python\n",
        "random.seed(seed_value) # To make sure that everytime it gives the same shuffle\n",
        "random.sample(list_to_shuffle, len(list_to_shuffle))\n",
        "```\n",
        "5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`."
      ],
      "metadata": {
        "id": "FTB5eNzJrqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n",
        "  '''Create 3 datasets each for training, validation and testing respectively.\n",
        "  This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n",
        "  them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n",
        "  summaries using random.sample method (although we have a helper function for it). Finally we do the\n",
        "  splitting of the whole dataset. Remember here the returned values become tensors.\n",
        "\n",
        "  Arguments:\n",
        "    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n",
        "    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n",
        "    train_size: float, specifying how much fraction of the original dataset to take for training.\n",
        "    val_size: float, specifying how much fraction of the original dataset to take for validation.\n",
        "    test_size: float, specifying how much fraction of the original dataset to take for testing.\n",
        "\n",
        "  Returns:\n",
        "    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n",
        "    with the specified amount of data in it.\n",
        "    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n",
        "  '''\n",
        "  articles = cnn_stories[0] + dailymail_stories[0]\n",
        "  summaries = cnn_stories[1] + dailymail_stories[1]\n",
        "\n",
        "  articles = np.array(articles, dtype=object)\n",
        "  summaries = np.array(summaries, dtype=object)\n",
        "\n",
        "  dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n",
        "\n",
        "  random.seed(seed_value)\n",
        "  shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n",
        "\n",
        "  dataset = dataset[shuffled_indices, :]\n",
        "\n",
        "  train_size = int(train_fraction * dataset.shape[0])\n",
        "  val_size = int(val_fraction * dataset.shape[0])\n",
        "  test_size = dataset.shape[0] - (train_size + val_size)\n",
        "\n",
        "  training_samples, validation_samples, testing_samples = split_dataset(dataset,\n",
        "                                                                        train_size,\n",
        "                                                                        val_size,\n",
        "                                                                        test_size)\n",
        "\n",
        "  return (training_samples, validation_samples, testing_samples)"
      ],
      "metadata": {
        "id": "QDB0_32RrnHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "52ce2281-0d0c-4c32-c628-52f992ab4a42"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ],
      "metadata": {
        "id": "GTnXBwd6Sa-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8a112af0-b344-4332-896f-df4ee50ff4e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n",
        "\n",
        "print(f\"Training dataset shape: {train_dataset.shape}\")\n",
        "print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
        "print(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n",
        "\n",
        "print(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")"
      ],
      "metadata": {
        "id": "AyRVodwIhkuQ",
        "outputId": "c83763cc-7c89-4742-c487-0ed3121e5db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the datasets: <class 'numpy.ndarray'>\n",
            "\n",
            "Training dataset shape: (80000, 2)\n",
            "Validation dataset shape: (10000, 2)\n",
            "Testing dataset shape: (10000, 2)\n",
            "\n",
            "First example in the training dataset looks like: \n",
            " ['(cnn) -- barack obama touched hearts during an impassioned speech to a 200,000-strong crowd in berlin, german newspapers agreed, but suspicions remain about the white house contender\\'s motives for courting a european audience. barack obama addressed a huge and friendly crowd in berlin on thursday. berlin\\'s der tagesspiegel wondered whether so many young people had ever gathered for a political event in germany and said that obama\\'s address -- which echoed speeches by former u.s. presidents john f. kennedy and ronald reagan when the divided city was in the cold war frontline -- could only have been made in the german capital. \"barack obama\\'s address might not have been statesmanlike and it definitely wasn\\'t worldly-wise. but with its symbolism and the message of this 46-year-old, it certainly was the signal of a new era for a new generation on both sides of the atlantic,\" the newspaper said. \"what better thing could have happened to us than the potential next president of the u.s. sending this message to the world from here?\" obama moved on from germany to france on friday where he was due to hold talks with president nicolas sarkozy. he is also slated to meet british prime minister gordon brown and opposition leader david cameron in london later friday. german tabloid bild said obama had articulated his version of the american dream -- the idea that politics can change the world: \"unlike george w. bush, he wants to do this in cooperation with others, especially europe. that\\'s his message from berlin: let\\'s try this together!\" but bild warned that an obama presidency would place fresh demands on traditional allies such as germany, which fell out with washington over the war in iraq and has refused to contribute combat troops to nato operations in afghanistan. ireport.com: view photos from the scene. \"he didn\\'t say what he expects, but it\\'s not hard to figure it out. he\\'ll call for more german participation during international crises; he\\'ll call for more german soldiers,\" said bild. but it concluded: \"no matter how you might feel about this: a president obama would be germany\\'s friend -- and a fan of berlin!\" munich\\'s sueddeutsche zeitung sounded a more cautious note, warning that an obama presidency would be \"expensive for germany.\" \"he explicitly called for german soldiers for afghanistan -- he did not say \\'more soldiers\\' but that was what he meant,\" said commentator reymer kluever. \"and obama also indicated this: he will want support as president to wind up the iraq adventure.\" the paper noted too that the speech had been intended less for the crowds gathered in berlin\\'s tiergarten park than for \"hesitant white voters in ohio and pennsylvania, colorado and virginia\" and said that obama had proved himself to be a clever tactician, capable of shifting positions to suit his audience. \"he wants to convince (voters) that the world will also listen to a black president... and there\\'s one other thing one shouldn\\'t forget when talking about obama: he easily reworks even positions that have been written in stone and adjusts to new requirements.\" but nobody in the german press appears to have been quite as smitten by their visitor as bild reporter judith bonesky, who had the opportunity to work out with obama in the gym of the ritz carlton hotel earlier in the day. \"obama (with toned arms and a strong back) puts on his headphones for his ipod to listen to pop music. he hums quietly. then he jumps on a fitness bike. he pushes three times on the pedals -- but then can\\'t be bothered with it. \"he picks up a pair of 16 kilo weights and starts curling them with his left and right arms, 30 repetitions on each side. then, amazingly, he picks up the 32 kilo weights! very slowly he lifts them, first 10 curls with his right, then 10 with his left. quickly i ask: \"mr. obama, could i take a photo?\". \"of course!\" he answers, before asking my name and coming over to stand next to me. \"my name\\'s judith\" i reply. \"i\\'m barack obama, nice to meet you!\" he says, and puts his arm across my shoulder. i put my arm around his hip -- wow, he didn\\'t even sweat! what a man!\"'\n",
            " 'german papers agree that barack obama won hearts with berlin speech. der tagesspiegel: obama\\'s speech a \"signal of a new era for a new generation\" but question marks hang over white house contender\\'s motives. papers say obama will demand more german troops for afghanistan.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n",
        "\n",
        "1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n",
        "2. Replace the single and double opening and closing quotes like `  \\u2018`, `  \\u2019`, `  \\u201c` and `  \\u201d` by `'` and `\"` respectively.\n",
        "3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\n",
        "In this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n",
        "4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n",
        "\n",
        "`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n",
        "1. Remove the `START_TOKEN` and `END_TOKEN` from the text. So that tokenizer does not standardize them.\n",
        "2. Standardize the text with `standardizer`.\n",
        "3. Add back the `START_TOKEN` and `END_TOKEN` because you want your tokenizer to learn them.\n",
        "4. Remove unwanted spaces in between words.\n",
        "5. Split the text into words which are seperated by ' '.\n",
        "6. Strip each of the words in the sentence. Finally, return it."
      ],
      "metadata": {
        "id": "TcZCpJ9hCyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the text data\n",
        "def standardizer(text):\n",
        "  '''Standardize the text provided to the function\n",
        "  The text is lower cased. Then, the opening and closing quotes are removed. I add spaces before the\n",
        "  punctuations like `don't` becomes `don ' t`, ignoring the numerical values so that `1.78` does not become\n",
        "  `1 . 78`. Finally, it strips the text and removes any type of unwanted spaces in it.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text to standardize\n",
        "\n",
        "  Returns:\n",
        "    returns the standadized text\n",
        "  '''\n",
        "\n",
        "  # Lower case the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Replace the special single and double opening and closing quotes\n",
        "  text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n",
        "  text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n",
        "\n",
        "  # Add space before punctuations and ignore floating point numbers.\n",
        "  text = re.sub(r'(?<!\\d)\\s*([!\"#$%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n",
        "                  r' \\1 ', text)  # It used to also remove commas after numbers like '27,' will be removed\n",
        "\n",
        "  # Remove spaces after sentence end and other unwanted spaces from text\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# custom analyzer for the Tokenizer class\n",
        "def custom_analyzer(text):\n",
        "  '''Custom analyzer to provide to the `Tokenizer` class when creating the tokenizer.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text that will be tokenized\n",
        "\n",
        "  Returns:\n",
        "    returns the splitted sentence\n",
        "  '''\n",
        "  # Remove START and END before standardizing\n",
        "  if START_TOKEN in text:\n",
        "    text = re.sub(f'{START_TOKEN} ', '', text)\n",
        "  if END_TOKEN in text:\n",
        "    text = re.sub(f'{END_TOKEN} ', '', text)\n",
        "\n",
        "  # Standardize the text first\n",
        "  text = standardizer(text)\n",
        "\n",
        "  # Add back the START and END tokens\n",
        "  text = ' '.join([START_TOKEN, text, END_TOKEN])\n",
        "\n",
        "  # Split the sentence into words to tokenize\n",
        "  words = text.split(' ')\n",
        "  words = [word.strip() for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "FD2h0OiAxJP_",
        "outputId": "28e9ed3f-96ef-4cd1-8902-dc43827b1204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n",
        "                \"U.S won the world cup and bagged 1.78 million dollars.\",\n",
        "                \"India had M.S. Dhoni won made it this far.\",\n",
        "                \"My email address is arupjana7365@gmail.com.\",\n",
        "                \"It can take care of dailymail single opening quote also.\",\n",
        "                \"I have 10,000 Rs in my bank\",\n",
        "                \"This sentence has , after a number 12,\",\n",
        "                \"This sentence contains <START> token and <END> token.\"]\n",
        "\n",
        "print(f\"After Standardizing the sample texts:\\n{[standardizer(text) for text in sample_texts]}\\n\")\n",
        "print(f\"After applying custom analyzer on sample texts:\\n{[custom_analyzer(text) for text in sample_texts]}\")"
      ],
      "metadata": {
        "id": "AVi3kZhcLXS7",
        "outputId": "c6a0744f-f18b-456c-feea-e70328b1b5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Standardizing the sample texts:\n",
            "['i have been working on , but never did it in this way .', 'u . s won the world cup and bagged 1.78 million dollars .', 'india had m . s . dhoni won made it this far .', 'my email address is arupjana7365@gmail . com .', \"it can take care of dailymail single opening quote ' also .\", 'i have 10,000 rs in my bank', 'this sentence has , after a number 12,', 'this sentence contains < start > token and < end > token .']\n",
            "\n",
            "After applying custom analyzer on sample texts:\n",
            "[['<START>', 'i', 'have', 'been', 'working', 'on', ',', 'but', 'never', 'did', 'it', 'in', 'this', 'way', '.', '<END>'], ['<START>', 'u', '.', 's', 'won', 'the', 'world', 'cup', 'and', 'bagged', '1.78', 'million', 'dollars', '.', '<END>'], ['<START>', 'india', 'had', 'm', '.', 's', '.', 'dhoni', 'won', 'made', 'it', 'this', 'far', '.', '<END>'], ['<START>', 'my', 'email', 'address', 'is', 'arupjana7365@gmail', '.', 'com', '.', '<END>'], ['<START>', 'it', 'can', 'take', 'care', 'of', 'dailymail', 'single', 'opening', 'quote', \"'\", 'also', '.', '<END>'], ['<START>', 'i', 'have', '10,000', 'rs', 'in', 'my', 'bank', '<END>'], ['<START>', 'this', 'sentence', 'has', ',', 'after', 'a', 'number', '12,', '<END>'], ['<START>', 'this', 'sentence', 'contains', 'token', 'and', 'token', '.', '<END>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n",
        "\n",
        "When fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab."
      ],
      "metadata": {
        "id": "IiiWRFR64jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(texts, num_words, oov_token=None, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n",
        "  '''This will create the tokenizer needed for the task in hand.\n",
        "  The tokenizer will be trained on the `texts`. Tokenizer will have vocabulary length `num_words`.\n",
        "  The `oov_token` will be used as the token represent the out-of-vocabulary words. The `filters` are\n",
        "  the ones which the tokenizer will remove when tokenizing any sentence given to it. The returned\n",
        "  tokenizer is using a custom analyzer that can standardize the sentence before tokenizing using the\n",
        "  `standardizer` function and then splits the sentence into words. After that it tokenizes the sentence.\n",
        "  As for the vocabulary, the returned tokenizer's vocabulary does not contain any number, as I have removed\n",
        "  them before feeding them into `Tokenizer.fit_on_texts` method.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the tokenizer will be trained on this strings\n",
        "    num_words: int, number of vocabulary words the tokenizer will consider\n",
        "    oov_token: str, token to represent out-of-vocabulary words\n",
        "    filters: str, all the characters that the tokenizer will remove before tokenizing\n",
        "\n",
        "  Returns:\n",
        "    tokenzier of the `Tokenizer` class after learning vocabulary from `texts`\n",
        "  '''\n",
        "\n",
        "  # Create the tokenizer usinf Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=num_words,\n",
        "                        filters=filters,\n",
        "                        oov_token=oov_token,\n",
        "                        analyzer=custom_analyzer)\n",
        "\n",
        "  # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n",
        "  texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n",
        "\n",
        "  # Fit the data with fit_on_texts method\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "tR1FD3SESd0G",
        "outputId": "4bdc2e56-94b1-411f-8c2b-ca0256bccbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")"
      ],
      "metadata": {
        "id": "UsmErFoxqimM",
        "outputId": "f7238ef8-b05e-4229-e4eb-c69e73ab77b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length the articles dataset: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `tokenizer` using the articles from training dataset by using `train_dataset[:, 0]`, with a vocabulary size of `VOCAB_SIZE` and use `OOV_TOKEN` token to represent out-of-vocabulary words."
      ],
      "metadata": {
        "id": "6KBXcUV2pYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)"
      ],
      "metadata": {
        "id": "151rEpqo7Pof",
        "outputId": "f0606020-e7f6-4705-98ef-cb35db0a7ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"{OOV_TOKEN} word has index: {tokenizer.word_index[OOV_TOKEN]}\")\n",
        "print(f\"{START_TOKEN} word has index: {tokenizer.word_index[START_TOKEN]}\")\n",
        "print(f\"{END_TOKEN} word has index: {tokenizer.word_index[END_TOKEN]}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\\n\")\n",
        "\n",
        "print(f\"Text:\\n{train_dataset[0, 0]}\\n\\n\")\n",
        "sample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\n",
        "print(f\"Text to Sequence of the first article:\\n{sample_sequence}\\n\")\n",
        "print(f\"Sequence to Text of the first acrticle:\\n{tokenizer.sequences_to_texts(sample_sequence)}\")"
      ],
      "metadata": {
        "id": "YUNreIjr8Kng",
        "outputId": "13a524b6-fca8-4678-84ae-ecf29d34bd17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary for the tokenizer has a length 171586\n",
            "\n",
            "\n",
            "<OOV> word has index: 1\n",
            "<START> word has index: 80\n",
            "<END> word has index: 81\n",
            "\n",
            "\n",
            "'teacher' word has index: 2087\n",
            "\n",
            "Text:\n",
            "(cnn) -- barack obama touched hearts during an impassioned speech to a 200,000-strong crowd in berlin, german newspapers agreed, but suspicions remain about the white house contender's motives for courting a european audience. barack obama addressed a huge and friendly crowd in berlin on thursday. berlin's der tagesspiegel wondered whether so many young people had ever gathered for a political event in germany and said that obama's address -- which echoed speeches by former u.s. presidents john f. kennedy and ronald reagan when the divided city was in the cold war frontline -- could only have been made in the german capital. \"barack obama's address might not have been statesmanlike and it definitely wasn't worldly-wise. but with its symbolism and the message of this 46-year-old, it certainly was the signal of a new era for a new generation on both sides of the atlantic,\" the newspaper said. \"what better thing could have happened to us than the potential next president of the u.s. sending this message to the world from here?\" obama moved on from germany to france on friday where he was due to hold talks with president nicolas sarkozy. he is also slated to meet british prime minister gordon brown and opposition leader david cameron in london later friday. german tabloid bild said obama had articulated his version of the american dream -- the idea that politics can change the world: \"unlike george w. bush, he wants to do this in cooperation with others, especially europe. that's his message from berlin: let's try this together!\" but bild warned that an obama presidency would place fresh demands on traditional allies such as germany, which fell out with washington over the war in iraq and has refused to contribute combat troops to nato operations in afghanistan. ireport.com: view photos from the scene. \"he didn't say what he expects, but it's not hard to figure it out. he'll call for more german participation during international crises; he'll call for more german soldiers,\" said bild. but it concluded: \"no matter how you might feel about this: a president obama would be germany's friend -- and a fan of berlin!\" munich's sueddeutsche zeitung sounded a more cautious note, warning that an obama presidency would be \"expensive for germany.\" \"he explicitly called for german soldiers for afghanistan -- he did not say 'more soldiers' but that was what he meant,\" said commentator reymer kluever. \"and obama also indicated this: he will want support as president to wind up the iraq adventure.\" the paper noted too that the speech had been intended less for the crowds gathered in berlin's tiergarten park than for \"hesitant white voters in ohio and pennsylvania, colorado and virginia\" and said that obama had proved himself to be a clever tactician, capable of shifting positions to suit his audience. \"he wants to convince (voters) that the world will also listen to a black president... and there's one other thing one shouldn't forget when talking about obama: he easily reworks even positions that have been written in stone and adjusts to new requirements.\" but nobody in the german press appears to have been quite as smitten by their visitor as bild reporter judith bonesky, who had the opportunity to work out with obama in the gym of the ritz carlton hotel earlier in the day. \"obama (with toned arms and a strong back) puts on his headphones for his ipod to listen to pop music. he hums quietly. then he jumps on a fitness bike. he pushes three times on the pedals -- but then can't be bothered with it. \"he picks up a pair of 16 kilo weights and starts curling them with his left and right arms, 30 repetitions on each side. then, amazingly, he picks up the 32 kilo weights! very slowly he lifts them, first 10 curls with his right, then 10 with his left. quickly i ask: \"mr. obama, could i take a photo?\". \"of course!\" he answers, before asking my name and coming over to stand next to me. \"my name's judith\" i reply. \"i'm barack obama, nice to meet you!\" he says, and puts his arm across my shoulder. i put my arm around his hip -- wow, he didn't even sweat! what a man!\"\n",
            "\n",
            "\n",
            "Text to Sequence of the first article:\n",
            "[[80, 44, 42, 43, 12, 12, 831, 127, 3951, 3915, 152, 36, 18545, 821, 5, 9, 1, 1296, 10, 3733, 4, 1330, 4243, 1151, 4, 31, 9579, 812, 50, 2, 258, 173, 7545, 11, 13, 9225, 15, 17603, 9, 587, 1606, 3, 831, 127, 3064, 9, 978, 8, 2466, 1296, 10, 3733, 19, 233, 3, 3733, 11, 13, 4038, 1, 6059, 267, 73, 108, 292, 57, 52, 396, 2268, 15, 9, 227, 591, 10, 1087, 8, 17, 14, 127, 11, 13, 1029, 12, 12, 66, 5408, 5142, 32, 201, 79, 3, 13, 3, 3087, 377, 1582, 3, 1501, 8, 4316, 2867, 59, 2, 3546, 150, 20, 10, 2, 1556, 237, 18546, 12, 12, 99, 112, 27, 55, 135, 10, 2, 1330, 635, 3, 6, 831, 127, 11, 13, 1029, 280, 38, 27, 55, 1, 8, 18, 2239, 567, 11, 41, 1, 12, 5157, 3, 31, 22, 71, 13873, 8, 2, 740, 7, 33, 1, 12, 151, 4, 18, 1214, 20, 2, 3859, 7, 9, 68, 1828, 15, 9, 68, 1513, 19, 193, 1388, 7, 2, 2687, 4, 6, 2, 1104, 17, 3, 6, 61, 314, 399, 99, 27, 550, 5, 163, 76, 2, 794, 212, 100, 7, 2, 79, 3, 13, 3, 2153, 33, 740, 5, 2, 94, 29, 207, 77, 6, 127, 902, 19, 29, 1087, 5, 741, 19, 230, 111, 21, 20, 841, 5, 887, 1155, 22, 100, 5382, 5729, 3, 21, 16, 69, 7619, 5, 891, 506, 586, 347, 4459, 784, 8, 710, 438, 702, 2908, 10, 464, 236, 230, 3, 1330, 6944, 1, 17, 127, 52, 1, 26, 1377, 7, 2, 169, 1569, 12, 12, 2, 735, 14, 1068, 63, 310, 2, 94, 47, 6, 2521, 941, 1990, 3, 768, 4, 21, 822, 5, 93, 33, 10, 2731, 22, 295, 4, 698, 746, 3, 14, 11, 13, 26, 740, 29, 3733, 47, 474, 11, 13, 538, 33, 469, 302, 6, 31, 1, 1525, 14, 36, 127, 2677, 65, 223, 2082, 2809, 19, 1532, 2089, 161, 23, 1087, 4, 66, 1487, 64, 22, 279, 90, 2, 237, 10, 370, 8, 34, 1649, 5, 4475, 1894, 618, 5, 1405, 1093, 10, 532, 3, 2182, 3, 411, 47, 1003, 971, 29, 2, 906, 3, 6, 21, 238, 11, 41, 141, 61, 21, 4059, 4, 31, 18, 11, 13, 38, 393, 5, 1311, 18, 64, 3, 21, 11, 372, 379, 15, 54, 1330, 4656, 152, 199, 7498, 165, 21, 11, 372, 379, 15, 54, 1330, 865, 4, 6, 17, 1, 3, 31, 18, 2702, 47, 6, 78, 709, 106, 40, 280, 421, 50, 33, 47, 9, 100, 127, 65, 30, 1087, 11, 13, 636, 12, 12, 8, 9, 1949, 7, 3733, 302, 6, 4403, 11, 13, 1, 1, 5937, 9, 54, 7010, 1601, 4, 1591, 14, 36, 127, 2677, 65, 30, 6, 2395, 15, 1087, 3, 6, 6, 21, 10076, 187, 15, 1330, 865, 15, 532, 12, 12, 21, 142, 38, 141, 11, 54, 865, 11, 31, 14, 20, 61, 21, 1698, 4, 6, 17, 8516, 1, 1, 3, 6, 8, 127, 69, 2760, 33, 47, 21, 45, 176, 250, 23, 100, 5, 2048, 67, 2, 370, 4481, 3, 6, 2, 1740, 1314, 243, 14, 2, 821, 52, 55, 2230, 386, 15, 2, 3340, 2268, 10, 3733, 11, 13, 1, 670, 76, 15, 6, 14271, 258, 892, 10, 1800, 8, 2005, 4, 1704, 8, 1242, 6, 8, 17, 14, 127, 52, 2793, 569, 5, 30, 9, 7981, 1, 4, 3191, 7, 6945, 2575, 5, 2034, 26, 1606, 3, 6, 21, 822, 5, 4862, 44, 892, 43, 14, 2, 94, 45, 69, 2720, 5, 9, 388, 100, 3, 3, 3, 8, 58, 11, 13, 53, 83, 399, 53, 2523, 11, 41, 2409, 59, 980, 50, 127, 47, 21, 2244, 1, 124, 2575, 14, 27, 55, 1144, 10, 2207, 8, 1, 5, 68, 4347, 3, 6, 31, 2281, 10, 2, 1330, 712, 1298, 5, 27, 55, 1094, 23, 1, 32, 49, 6861, 23, 1, 2437, 15925, 1, 4, 39, 52, 2, 930, 5, 167, 64, 22, 127, 10, 2, 4813, 7, 2, 13095, 11402, 748, 378, 10, 2, 118, 3, 6, 127, 44, 22, 19329, 1768, 8, 9, 661, 126, 43, 3167, 19, 26, 15926, 15, 26, 8794, 5, 2720, 5, 1932, 592, 3, 21, 1, 5775, 3, 131, 21, 11892, 19, 9, 4709, 4022, 3, 21, 10688, 123, 277, 19, 2, 1, 12, 12, 31, 131, 63, 11, 41, 30, 10547, 22, 18, 3, 6, 21, 7047, 67, 9, 2054, 7, 1, 1, 15550, 8, 3020, 13710, 89, 22, 26, 248, 8, 197, 1768, 4, 1, 1, 19, 284, 375, 3, 131, 4, 14045, 4, 21, 7047, 67, 2, 1, 1, 15550, 302, 132, 3373, 21, 12517, 89, 4, 87, 1, 1, 22, 26, 197, 4, 131, 1, 22, 26, 248, 3, 938, 24, 890, 47, 6, 1412, 3, 127, 4, 99, 24, 158, 9, 1456, 77, 6, 3, 6, 7, 533, 302, 6, 21, 2705, 4, 113, 1273, 91, 424, 8, 537, 90, 5, 852, 212, 5, 130, 3, 6, 91, 424, 11, 13, 15925, 6, 24, 9305, 3, 6, 24, 11, 133, 831, 127, 4, 2237, 5, 891, 40, 302, 6, 21, 98, 4, 8, 3167, 26, 2497, 322, 91, 4063, 3, 24, 264, 91, 2497, 178, 26, 3742, 12, 12, 6418, 4, 21, 238, 11, 41, 124, 7945, 302, 61, 9, 198, 302, 6, 81]]\n",
            "\n",
            "Sequence to Text of the first acrticle:\n",
            "['<START> ( cnn ) - - barack obama touched hearts during an impassioned speech to a <OOV> crowd in berlin , german newspapers agreed , but suspicions remain about the white house contender \\' s motives for courting a european audience . barack obama addressed a huge and friendly crowd in berlin on thursday . berlin \\' s der <OOV> wondered whether so many young people had ever gathered for a political event in germany and said that obama \\' s address - - which echoed speeches by former u . s . presidents john f . kennedy and ronald reagan when the divided city was in the cold war frontline - - could only have been made in the german capital . \" barack obama \\' s address might not have been <OOV> and it definitely wasn \\' t <OOV> - wise . but with its symbolism and the message of this <OOV> - old , it certainly was the signal of a new era for a new generation on both sides of the atlantic , \" the newspaper said . \" what better thing could have happened to us than the potential next president of the u . s . sending this message to the world from here ? \" obama moved on from germany to france on friday where he was due to hold talks with president nicolas sarkozy . he is also slated to meet british prime minister gordon brown and opposition leader david cameron in london later friday . german tabloid <OOV> said obama had <OOV> his version of the american dream - - the idea that politics can change the world : \" unlike george w . bush , he wants to do this in cooperation with others , especially europe . that \\' s his message from berlin : let \\' s try this together ! \" but <OOV> warned that an obama presidency would place fresh demands on traditional allies such as germany , which fell out with washington over the war in iraq and has refused to contribute combat troops to nato operations in afghanistan . ireport . com : view photos from the scene . \" he didn \\' t say what he expects , but it \\' s not hard to figure it out . he \\' ll call for more german participation during international crises ; he \\' ll call for more german soldiers , \" said <OOV> . but it concluded : \" no matter how you might feel about this : a president obama would be germany \\' s friend - - and a fan of berlin ! \" munich \\' s <OOV> <OOV> sounded a more cautious note , warning that an obama presidency would be \" expensive for germany . \" \" he explicitly called for german soldiers for afghanistan - - he did not say \\' more soldiers \\' but that was what he meant , \" said commentator <OOV> <OOV> . \" and obama also indicated this : he will want support as president to wind up the iraq adventure . \" the paper noted too that the speech had been intended less for the crowds gathered in berlin \\' s <OOV> park than for \" hesitant white voters in ohio and pennsylvania , colorado and virginia \" and said that obama had proved himself to be a clever <OOV> , capable of shifting positions to suit his audience . \" he wants to convince ( voters ) that the world will also listen to a black president . . . and there \\' s one other thing one shouldn \\' t forget when talking about obama : he easily <OOV> even positions that have been written in stone and <OOV> to new requirements . \" but nobody in the german press appears to have been quite as <OOV> by their visitor as <OOV> reporter judith <OOV> , who had the opportunity to work out with obama in the gym of the ritz carlton hotel earlier in the day . \" obama ( with toned arms and a strong back ) puts on his headphones for his ipod to listen to pop music . he <OOV> quietly . then he jumps on a fitness bike . he pushes three times on the <OOV> - - but then can \\' t be bothered with it . \" he picks up a pair of <OOV> <OOV> weights and starts curling them with his left and right arms , <OOV> <OOV> on each side . then , amazingly , he picks up the <OOV> <OOV> weights ! very slowly he lifts them , first <OOV> <OOV> with his right , then <OOV> with his left . quickly i ask : \" mr . obama , could i take a photo ? \" . \" of course ! \" he answers , before asking my name and coming over to stand next to me . \" my name \\' s judith \" i reply . \" i \\' m barack obama , nice to meet you ! \" he says , and puts his arm across my shoulder . i put my arm around his hip - - wow , he didn \\' t even sweat ! what a man ! \" <END>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\n",
        "NO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that."
      ],
      "metadata": {
        "id": "crn5t_zk6iBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.keys())[21000]"
      ],
      "metadata": {
        "id": "qhttL-heeP-P",
        "outputId": "c7f6cbf2-5f45-4049-cdb7-4ea4ec6285da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'academically'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov_word = list(tokenizer.word_index.keys())[21000]\n",
        "sample_text = f\"This example is to test the above fact with the word `{oov_word}`\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "S_Dnkcig7SIX",
        "outputId": "a34248a0-f0f7-4eac-81e2-983f6b1bc2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This example is to test the above fact with the word `academically`\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> this example is to test the above fact with the word <OOV> <OOV> <OOV> <END>']\n",
            "Sequence: [[80, 33, 917, 16, 5, 935, 2, 1236, 470, 22, 2, 1062, 1, 1, 1, 81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`."
      ],
      "metadata": {
        "id": "xvNeazXb-Yq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add a number 2.1 in this sentence!\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ng5MZ_DZ84kk",
        "outputId": "05a58b57-982f-4060-8e32-d2c7fdeb18fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add a number 2.1 in this sentence!\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add a number <OOV> in this sentence ! <END>']\n",
            "Sequence: [[80, 61, 1684, 59, 24, 1930, 9, 281, 1, 10, 33, 1293, 302, 81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add parenthesis (I am inside it!).\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ImlMHniH-Jnt",
        "outputId": "60782c7e-b993-4399-ffcc-83fd0f2ea032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add parenthesis (I am inside it!).\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add <OOV> ( i am inside it ! ) . <END>']\n",
            "Sequence: [[80, 61, 1684, 59, 24, 1930, 1, 44, 24, 480, 559, 18, 302, 43, 3, 81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the `tokenizer` to tokenize the articles and summaries. We need to pad those sequences to fit the requirements.\n",
        "\n",
        "In the paper, the articles are limited to have 400 tokens and summary has, 100 tokens at training and 120 tokens for testing.\n",
        "\n",
        "I will be using `pad_sequences` method to pad or truncate the articles and summaries based on their length.\n",
        "\n",
        "NOTE: I am using same tokenizer for article and summary. But, later I might change that to 2 different tokenizers each having different `num_words`."
      ],
      "metadata": {
        "id": "jza9oQYKXB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pad(texts, tokenizer, padding, truncating, maxlen):\n",
        "  '''Tokenize the `texts` using the tokenizer. Then, pad the sequences or truncate the sequences\n",
        "  depending the length. If the length exceeds `maxlen` then it will be truncated and if not then it will be\n",
        "  padded. The padding and truncating can happend at the beginning or at the end of the sequence depending\n",
        "  on the value of `padding` and `truncating` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the sentences to tokenize and pad\n",
        "    tokenizer: Tokenizer class object, helps in tokenizing the `texts`\n",
        "    padding: str, can take 2 values `pre` or `post`. If `pre` then padding will happen at the beginning,\n",
        "    if `post` then padding will happen at the end.\n",
        "    truncating: str, can take 2 values `pre` or 'truncating`, works the same as `padding`\n",
        "    maxlen: int, maximum length after padding or truncating\n",
        "\n",
        "  Returns:\n",
        "    returns the tokenized and padded sentences\n",
        "  '''\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Alo70WYjW-tA",
        "outputId": "673dbaf2-9898-41f5-d536-152c80e8784f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "rttUwCiT4FzJ",
        "outputId": "215d2d2f-a87a-4933-f047-74cadc86ad55"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have been working on, \\nbut \\tnever did it in this way.',\n",
              " 'U.S won the world cup and bagged 1.78 million dollars.',\n",
              " 'India had M.S. Dhoni won made it this far.',\n",
              " 'My email address is arupjana7365@gmail.com.',\n",
              " 'It can take care of dailymail single opening quote also.',\n",
              " 'I have 10,000 Rs in my bank',\n",
              " 'This sentence has , after a number 12,',\n",
              " 'This sentence contains <START> token and <END> token.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_pad(sample_texts, tokenizer, padding=\"post\", truncating=\"post\", maxlen=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "QNuBKpTq4lZW",
        "outputId": "1277d19c-8edb-41a1-c028-07deb10a66bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   80,    24,    27,    55,   321,    19,     4,    31,   211,\n",
              "          142,    18,    10,    33,   139,     3,    81,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    79,     3,    13,   265,     2,    94,   637,     8,\n",
              "            1,     1,   172,  1610,     3,    81,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,   838,    52,   133,     3,    13,     3,     1,   265,\n",
              "          135,    18,    33,   312,     3,    81,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    91,  4620,  1029,    16,     1,     3,   411,     3,\n",
              "           81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    18,    63,   158,   316,     7,     1,   863,  1073,\n",
              "         6894,    11,    69,     3,    81,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    24,    27,     1,     1,    10,    91,  1028,    81,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    33,  1293,    34,     4,    60,     9,   281,     1,\n",
              "           81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    33,  1293,  4799, 18473,     8, 18473,     3,    81,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, we need the model that we can train on this dataset. The model archietecture will be 3-\n",
        "1. Base-line model: Seq-Seq model with attention mechanism.\n",
        "2. Pointer Generetor model: With seq-seq attention model will be implementing the pointer generator that can either copy words from article or generate words from the pre-defined vocabulary.\n",
        "3. Coverage mechanism: Along with the pointer generator that will take case of the out-of-vocabulary words. Coverage mechanism will help prevent the repetition of the words in the summary."
      ],
      "metadata": {
        "id": "yM6KveyaUmfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base-Line Model: Seq-seq with Attention"
      ],
      "metadata": {
        "id": "UarmKLUIVkjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating `tf_train_dataset`, `tf_val_dataset` using tf.data API\n",
        "\n",
        "Now, I need write the `generate_example` function that can help me generate model inputs for training, validation and testing set. For different type of dataset, we will create different generator with the help of the `example_generator` method to create `tf.data.Dataset` object for our model.\n",
        "\n",
        "We can use `tf.data` API to create the input data pipeline for our model. I will use the `tf.data.Dataset` class to get the the examples from the `train_example_generator` function which uses `generate_example`, we can save the generator inside `example_gen` which we can iterate over later to get the examples. We can yield the examples according to the need of the problem.\n",
        "\n",
        "Remember, along with input article tokens and input summary tokens, we need the initial states as an input to the model. So, as we process the examples we can create this zero-value tensors and yield them along with 2 original inputs.\n",
        "\n",
        "For more about datasets from generator, refer to [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)."
      ],
      "metadata": {
        "id": "usV8UchPA35e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Geneator function that generates the source and target for training the model."
      ],
      "metadata": {
        "id": "ZUtzIfXm6UnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example_v1(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n",
        "  '''Generates examples for the model. Processes the `inputs` and `targets` with their respective\n",
        "  tokenizers and tokenize them to `input_len` and `target_len` length.\n",
        "\n",
        "  Arguments:\n",
        "    inputs: list of input sentences\n",
        "    targets: list of target sentences\n",
        "    input_tokenizer: Tokenizer class object, tokenizer for inputs\n",
        "    target_tokenizer: Tokenizer class object, tokenizer for targets\n",
        "    input_len: int, the length of the tokenization for inputs\n",
        "    target_len: int, the length of the tokenization for targets\n",
        "\n",
        "  Returns:\n",
        "    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n",
        "    another numpy array target_tokens[1:]\n",
        "  '''\n",
        "\n",
        "  for inp, tar in zip(inputs, targets):\n",
        "    # Tokenizing article words\n",
        "    inp_tokens = tokenize_pad([inp],\n",
        "                              input_tokenizer,\n",
        "                              padding=\"post\",\n",
        "                              truncating=\"post\",\n",
        "                              maxlen=input_len)\n",
        "\n",
        "    # Tokenizing summary words\n",
        "    tar_tokens = tokenize_pad([tar],\n",
        "                 target_tokenizer,\n",
        "                 padding=\"post\",\n",
        "                 truncating=\"post\",\n",
        "                 maxlen=target_len)\n",
        "\n",
        "    yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8647M-ey6n19",
        "outputId": "0783fcd2-3e99-45dc-cff0-102a8f9572f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example generated by the generator :\")\n",
        "\n",
        "# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\n",
        "example_gen = generate_example_v1(list(train_dataset[:, 0]),\n",
        "                               list(train_dataset[:, 1]),\n",
        "                               input_tokenizer=tokenizer,\n",
        "                               target_tokenizer=tokenizer,\n",
        "                               input_len=MAX_ARTICLE_TOKENS,\n",
        "                               target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "inps, tar = next(example_gen)\n",
        "print(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Target:\\n{tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "BKKyIeVa5B20",
        "outputId": "f424eca8-5f29-4dd0-915b-7f5e362399ec"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example generated by the generator :\n",
            "Inputs:\n",
            "[   80    44    42    43    12    12   831   127  3951  3915   152    36\n",
            " 18545   821     5     9     1  1296    10  3733     4  1330  4243  1151\n",
            "     4    31  9579   812    50     2   258   173  7545    11    13  9225\n",
            "    15 17603     9   587  1606     3   831   127  3064     9   978     8\n",
            "  2466  1296    10  3733    19   233     3  3733    11    13  4038     1\n",
            "  6059   267    73   108   292    57    52   396  2268    15     9   227\n",
            "   591    10  1087     8    17    14   127    11    13  1029    12    12\n",
            "    66  5408  5142    32   201    79     3    13     3  3087   377  1582\n",
            "     3  1501     8  4316  2867    59     2  3546   150    20    10     2\n",
            "  1556   237 18546    12    12    99   112    27    55   135    10     2\n",
            "  1330   635     3     6   831   127    11    13  1029   280    38    27\n",
            "    55     1     8    18  2239   567    11    41     1    12  5157     3\n",
            "    31    22    71 13873     8     2   740     7    33     1    12   151\n",
            "     4    18  1214    20     2  3859     7     9    68  1828    15     9\n",
            "    68  1513    19   193  1388     7     2  2687     4     6     2  1104\n",
            "    17     3     6    61   314   399    99    27   550     5   163    76\n",
            "     2   794   212   100     7     2    79     3    13     3  2153    33\n",
            "   740     5     2    94    29   207    77     6   127   902    19    29\n",
            "  1087     5   741    19   230   111    21    20   841     5   887  1155\n",
            "    22   100  5382  5729     3    21    16    69  7619     5   891   506\n",
            "   586   347  4459   784     8   710   438   702  2908    10   464   236\n",
            "   230     3  1330  6944     1    17   127    52     1    26  1377     7\n",
            "     2   169  1569    12    12     2   735    14  1068    63   310     2\n",
            "    94    47     6  2521   941  1990     3   768     4    21   822     5\n",
            "    93    33    10  2731    22   295     4   698   746     3    14    11\n",
            "    13    26   740    29  3733    47   474    11    13   538    33   469\n",
            "   302     6    31     1  1525    14    36   127  2677    65   223  2082\n",
            "  2809    19  1532  2089   161    23  1087     4    66  1487    64    22\n",
            "   279    90     2   237    10   370     8    34  1649     5  4475  1894\n",
            "   618     5  1405  1093    10   532     3  2182     3   411    47  1003\n",
            "   971    29     2   906     3     6    21   238    11    41   141    61\n",
            "    21  4059     4    31    18    11    13    38   393     5  1311    18\n",
            "    64     3    21    11   372   379    15    54  1330  4656   152   199\n",
            "  7498   165    21    11]\n",
            "[  80 1330 3696 1835   14  831  127  265 3915   22 3733  821    3 4038\n",
            "    1   47  127   11   13  821    9    6 3859    7    9   68 1828   15\n",
            "    9   68 1513    6   31  603 3153 4711   90  258  173 7545   11   13\n",
            " 9225    3 3696  141  127   45 1628   54 1330  618   15  532    3   81\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n",
            "Target:\n",
            "[1330 3696 1835   14  831  127  265 3915   22 3733  821    3 4038    1\n",
            "   47  127   11   13  821    9    6 3859    7    9   68 1828   15    9\n",
            "   68 1513    6   31  603 3153 4711   90  258  173 7545   11   13 9225\n",
            "    3 3696  141  127   45 1628   54 1330  618   15  532    3   81    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the Inputs:\\n{inps[0].shape}\\n{inps[1].shape}\\n\\n\")\n",
        "print(f\"Shape of the Target:\\n{tar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "E-w0hmecDcho",
        "outputId": "dc67edf1-88c5-4f30-935e-e3ea04f8b0f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the Inputs:\n",
            "(400,)\n",
            "(99,)\n",
            "\n",
            "\n",
            "Shape of the Target:\n",
            "(99,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data Type of the Inputs data:\\n{inps[0].dtype}\\n{inps[1].dtype}\\n\\n\")\n",
        "print(f\"Data Type of the Target data:\\n{tar.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "suZmEEhfDvcu",
        "outputId": "0e2da90a-79b0-4b89-a5da-5b28d899609c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Type of the Inputs data:\n",
            "int32\n",
            "int32\n",
            "\n",
            "\n",
            "Data Type of the Target data:\n",
            "int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inps, tar = next(example_gen)\n",
        "print(f\"Second Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Second Target:\\n{tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "82m4io2PSyt3",
        "outputId": "d95bc6aa-4364-4c1f-c3de-8b0a11ce5f25"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second Inputs:\n",
            "[   80    44    42    43    12    12   222  6837     1     1     8     1\n",
            "    10   454   695    19   230  1234    60  7293    10  8812  1046  1015\n",
            "    49  5048   378    10     2   118     3  1450     1     8  3349    29\n",
            "     2   695   254     7  1881    48   145   321    10     2   308     4\n",
            "  9036  1031 12160     1    17     3     6  2072  1280     5  1046     8\n",
            "  1351  1274    72    25    62   476     4     6    46    17     3     6\n",
            "   344    15  1491     8  1450     3     6  8812  1046    52  9698   647\n",
            "     5  7643   549  6953     7  5879     1  7318     7     9     1  4173\n",
            "     7     2  5879    48  1231    10   193  7266    29  1675  8390     5\n",
            "  2537     1     4     1    17     3    58    48   282  4843     4   185\n",
            "     9   105   620    39    20   442    32     9   419   117   359     5\n",
            "   114     5   174  1479    19  5879     1     2   620    20  4267   233\n",
            "  1439    22  2781  1373     4    42  1260     1   234     3  1239  1450\n",
            "    69  1015     2  5048     7    79     3    13     3  2807     1   612\n",
            "     7  1675  8390     5     2    68   683   107   501     3     2   171\n",
            "   990   336  4705 10582    15   230   270     8   305    10  1675  8390\n",
            "     4  2537     1     8     1     4    22   426  6523    32   305   270\n",
            "     3    81     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[   80   998     7     1     1     8     1    48  2158   102     7  1450\n",
            "     8  7293     3  1450     1     8  3349  1154  3089     3  6246  5327\n",
            " 10582   144   305     4   131   426  6523     3    81     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "\n",
            "Second Target:\n",
            "[  998     7     1     1     8     1    48  2158   102     7  1450     8\n",
            "  7293     3  1450     1     8  3349  1154  3089     3  6246  5327 10582\n",
            "   144   305     4   131   426  6523     3    81     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_example_generetor_v1():\n",
        "  example_gen = generate_example_v1(list(train_dataset[:, 0]),\n",
        "                                list(train_dataset[:, 1]),\n",
        "                                input_tokenizer=tokenizer,\n",
        "                                target_tokenizer=tokenizer,\n",
        "                                input_len=MAX_ARTICLE_TOKENS,\n",
        "                                target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "  for example in example_gen:\n",
        "    s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "    c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "\n",
        "    (input_0, input_1), target = example\n",
        "    yield (input_0, input_1, s0, c0), target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "J_0r7ajRR8wk",
        "outputId": "367e274d-63e4-4e55-ae53-a7a96011f899"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "tf_train_dataset = tf.data.Dataset.from_generator(generator=train_example_generetor_v1,\n",
        "                                                  output_signature=output_signature)\n",
        "tf_train_dataset = tf_train_dataset.shuffle(BUFFER_SIZE)\n",
        "tf_train_dataset = tf_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "tf_train_dataset = tf_train_dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "Zj-H_fEh5Zu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "524e02fb-448c-4b19-b09f-4d81b72f57a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val_example_generetor_v1():\n",
        "  example_gen = generate_example_v1(list(val_dataset[:, 0]),\n",
        "                                list(val_dataset[:, 1]),\n",
        "                                input_tokenizer=tokenizer,\n",
        "                                target_tokenizer=tokenizer,\n",
        "                                input_len=MAX_ARTICLE_TOKENS,\n",
        "                                target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "  for example in example_gen:\n",
        "    s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "    c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n",
        "\n",
        "    (input_0, input_1), target = example\n",
        "    yield (input_0, input_1, s0, c0), target\n",
        "\n",
        "\n",
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "tf_val_dataset = tf.data.Dataset.from_generator(generator=val_example_generetor_v1,\n",
        "                                                  output_signature=output_signature)\n",
        "tf_val_dataset = tf_val_dataset.shuffle(BUFFER_SIZE)\n",
        "tf_val_dataset = tf_val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "-2qJcIbjRZXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "89e83c15-798b-4136-818d-bac52016ad60"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (art_inp, sum_inp, s0, c0), sum_tar in tf_train_dataset.take(1):\n",
        "  print(f\"Input tokenized article shape: {art_inp.shape}\")\n",
        "  print(f\"Input tokenized summary shape: {sum_inp.shape}\\n\")\n",
        "\n",
        "  print(f\"Target tokenized summary shape: {sum_tar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "no4oMl22Fm6r",
        "outputId": "d3c68da1-cb08-4c1b-8e09-b769ccfcbb97"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokenized article shape: (16, 400)\n",
            "Input tokenized summary shape: (16, 99)\n",
            "\n",
            "Target tokenized summary shape: (16, 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small demonstration of how Dot layer works"
      ],
      "metadata": {
        "id": "-2S_Jxhf25-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10).reshape(1, 2, 5)\n",
        "example_layer = tf.keras.layers.Dense(units=DENSE1_UNITS)\n",
        "\n",
        "print(f\"After applying dense layer on x of shape:{x.shape}, output has {example_layer(x).shape} shape\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qxBGPuOWKubc",
        "outputId": "ac9e69f8-52f5-490c-81e2-80607a134c36"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After applying dense layer on x of shape:(1, 2, 5), output has (1, 2, 16) shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.arange(10).reshape(1, 2, 5)\n",
        "x2 = np.arange(10, 22).reshape(1, 2, 6)\n",
        "print(f\"x1: {x1}\\nx2: {x2}\")\n",
        "\n",
        "Dot(axes=1)([x1, x2])"
      ],
      "metadata": {
        "id": "_WLTScTp1x-R",
        "outputId": "82c0a1d8-f90a-4d8c-8520-f25a9c06b0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: [[[0 1 2 3 4]\n",
            "  [5 6 7 8 9]]]\n",
            "x2: [[[10 11 12 13 14 15]\n",
            "  [16 17 18 19 20 21]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 5, 6), dtype=int64, numpy=\n",
              "array([[[ 80,  85,  90,  95, 100, 105],\n",
              "        [106, 113, 120, 127, 134, 141],\n",
              "        [132, 141, 150, 159, 168, 177],\n",
              "        [158, 169, 180, 191, 202, 213],\n",
              "        [184, 197, 210, 223, 236, 249]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Mechanism"
      ],
      "metadata": {
        "id": "BVsZ_DeVmaie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_time_attention_v1(a, s_prev,\n",
        "                       repeater, concatenator, densor_1, densor_2, softmax_layer, dotter):\n",
        "  '''Calculates the attention score and returns the context for the current timestep in the decoder.\n",
        "  Attention mechanism uses encoder outputs `a` of shape `(batch, timesteps, features)` and decoder\n",
        "  previous hidden state `s_prev` of shape `(batch, features)`, then calculates alignment scores `alphas`\n",
        "  for each encoder timestep with the help of energies computed with 2 dense layers using `a` and `s_prev`.\n",
        "\n",
        "  Arguments:\n",
        "    a: tf.Tensor object, encoder output of shape `(batch, timesteps, features)` or `(batch, Tx, 2*n_a)`\n",
        "    s_prev: tf.Tensor object, decoder previous hidden state of shape `(batch, features)` or `(batch, n_s)`\n",
        "    repeater: RepeatVector layer, repeat the `s_prev` `Tx` times\n",
        "    concatenator: Concatenate layer, concatenates `a` and repeated `s_prev`, Concatenates along axis=-1\n",
        "    densor_1: Dense layer, calculates the pertial energies `e`, with `units=d1_units`\n",
        "    refer to `baseline_model` function for details about this variable\n",
        "    densor_2: Dense layer, calculated the energies `energies`, with `units=d2_units`\n",
        "    refer to `baseline_model` function for details about this variable\n",
        "    softmax_layer: Activation layer, computes softmax of the energies and calculates `alphas`, with\n",
        "    `units=article_vocab_size` refer to `baseline_model` function for details about this variable\n",
        "    dotter: Dot layer, Performs dot operation between `alphas` and `a` along axis=1\n",
        "\n",
        "  Returns:\n",
        "    returns the context of shape `(batch, features)`\n",
        "  '''\n",
        "\n",
        "  # Repeat the `s_prev` `Tx` times\n",
        "  s_prev = repeater(s_prev) # (batch, Tx, n_s)\n",
        "\n",
        "  # Concatenate `a` and `s_prev` along axis=-1\n",
        "  concat = concatenator([a, s_prev]) # (batch, Tx, n_a + n_s)\n",
        "\n",
        "  # Apply dense layer to get partial energies e\n",
        "  e = densor_1(concat) # (batch, Tx, d1_units)\n",
        "\n",
        "  # Apply dense layer again to get energies\n",
        "  energies = densor_2(e) # (batch, Tx, d2_units)\n",
        "\n",
        "  # Apply softmax over the energies\n",
        "  alphas = softmax_layer(energies) # (batch, Tx, d2_units)\n",
        "\n",
        "  # Dot the alphas and a along axes=1\n",
        "  context = dotter([alphas, a]) # (batch, d2_units, 2*n_a)\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B_rPlnWardTC",
        "outputId": "2b856755-06a5-47b7-90fb-30f2001b4690"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder-decoder Model using Attention mechanism"
      ],
      "metadata": {
        "id": "QxkE_-Ifn2k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model(Tx, Ty,\n",
        "                   emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                   article_vocab_size, summary_vocab_size):\n",
        "  '''This implements the bas-line model archietecture for summarization.\n",
        "  It is a seq-seq model with attention mechanism implemented in it. The encoder take an input\n",
        "  with `Tx` time-steps and summarizes with the help of decoder into Ty words. The encoder and decoder\n",
        "  hidden states are `n_a` and `n_s` dimension respectively. The words are taken from the vocabulary of\n",
        "  article and summary `article_vocab` and `summary_vocab` with size `article_vocab_size` and\n",
        "  `summary_vocab_size` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    Tx: int, length of the input article\n",
        "    Ty: int, length of the output summary\n",
        "    n_a: int, dimension of the encoder hidden states\n",
        "    n_s: int, dimension of the deocder hidden states\n",
        "    d1_units: int, units for the first dense layer in attention mechanism\n",
        "    d2_units: int, units for the second dense layer in attention mechanism\n",
        "    d_units: int, units for the dense layer before output layer\n",
        "    article_vocab_size: int, length of the article vocabulary\n",
        "    summary_vocab_size: int, length of the summary vocabulary\n",
        "\n",
        "  Returns:\n",
        "    returns the base line model\n",
        "  '''\n",
        "  # Defining the input for our model with shape (None, Tx) and (None, Ty) for encoder input and decoder input\n",
        "  X_inp = Input(shape=(Tx))\n",
        "  X_tar = Input(shape=(Ty))\n",
        "\n",
        "  # Initialize s0\n",
        "  s0 = Input(shape=(n_s, ), name=\"s0\")\n",
        "  # Initialize c0\n",
        "  c0 = Input(shape=(n_s, ), name=\"c0\")\n",
        "\n",
        "  # Initialize the a and s with a0 and s0\n",
        "  s = s0 # (batch, n_s)\n",
        "  c = c0 # (batch, n_s)\n",
        "\n",
        "  # Define the outputs as empty list\n",
        "  outputs = []\n",
        "\n",
        "  # First embedding layer for the article input\n",
        "  encoder_inp = Embedding(article_vocab_size, emb_dim)(X_inp) # (batch, Tx, emb_dim)\n",
        "\n",
        "  # Encoder: Bidirectional layer with LSTM cells\n",
        "  a = Bidirectional(LSTM(units=n_a, return_sequences=True))(encoder_inp) # (batch, Tx, n_a)\n",
        "\n",
        "  # Define the embedding for decoder\n",
        "  decoder_inp = Embedding(summary_vocab_size, emb_dim)(X_tar) # (batch, Ty, emb_dim)\n",
        "\n",
        "  # Define the layers for Attention so that we can use the same weights for all decoder timesteps\n",
        "  repeater = RepeatVector(Tx)\n",
        "  concatenator = Concatenate(axis=-1)\n",
        "  attn_densor1 = Dense(units=d1_units, activation='tanh')\n",
        "  attn_densor2 = Dense(units=d2_units, activation='linear', use_bias=False)\n",
        "  softmax_layer = Activation('softmax', name=\"attention_weights\")\n",
        "  dotter = Dot(axes=1)\n",
        "\n",
        "  # Define the Decoder unidirectional LSTM for shared weights\n",
        "  post_attention_lstm = LSTM(units=n_s, return_state=True)\n",
        "\n",
        "  # Define the last dense layer before output layer with linear activation\n",
        "  densor = Dense(units=d_units, activation='linear')\n",
        "\n",
        "  # Define the output layer so that it does not initalize again and again for shared weights\n",
        "  output_layer = Dense(units=summary_vocab_size, activation='softmax')\n",
        "\n",
        "  # Decoder: Appends outputs from the output layer in each timestep\n",
        "  for t in range(Ty):\n",
        "    # Get the decoder input for current timestep\n",
        "    curr_dec_in = decoder_inp[:, t:t+1, :] # (batch, 1, emb_dim)\n",
        "\n",
        "    # Get the context from the attention mechanism\n",
        "    context = one_time_attention_v1(a, s, # (batch, d2_units, 2*n_a)\n",
        "                                 repeater, concatenator, attn_densor1, attn_densor2, softmax_layer, dotter)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([curr_dec_in, context]) # (batch, d2_units, emb_dim+2*n_a); d2_units=1 otherwise error\n",
        "    _, s, c = post_attention_lstm(concat, initial_state=[s, c]) # _, (batch, n_s), (batch, n_s)\n",
        "\n",
        "    # Calculate the output after using 2 linear dense layers\n",
        "    out = densor(s) # (batch, d_units)\n",
        "    out = densor(out) # (batch, d_units)\n",
        "    # Use the output_layer to get the output\n",
        "    out  = output_layer(out) # (batch, summary_vocab_size)\n",
        "\n",
        "    # Append the final output to the outputs list\n",
        "    outputs.append(out)\n",
        "\n",
        "  # Stack the list of each timesteps output along axis=1\n",
        "  outputs = tf.stack(outputs, axis=1) # (batch, Ty, summary_vocab_size)\n",
        "\n",
        "  model = Model(inputs=[X_inp, X_tar, s0, c0], outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mudpzB78HDvZ",
        "outputId": "2fb0d213-077a-45c1-ada0-1648edd12e2d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset states generated by Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "APJAr_NwcRRj",
        "outputId": "d41ea3e1-367f-462a-f038-33ffdb99bc6b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tx = MAX_ARTICLE_TOKENS\n",
        "Ty = MAX_SUMMARY_TOKENS - 1\n",
        "emb_dim = EMB_OUT\n",
        "n_a = ENCODER_STATE_DIM\n",
        "n_s= DECODER_STATE_DIM\n",
        "d1_units = DENSE1_UNITS\n",
        "d2_units = DENSE2_UNITS\n",
        "d_units = DENSE_UNITS\n",
        "article_vocab_size = VOCAB_SIZE\n",
        "summary_vocab_size = VOCAB_SIZE\n",
        "\n",
        "model = baseline_model(Tx, Ty,\n",
        "                       emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                       article_vocab_size, summary_vocab_size)"
      ],
      "metadata": {
        "id": "V3ZAtG5I8mZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model has {model.count_params():,} parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2eMakRHeZzrF",
        "outputId": "08d0700c-d496-412d-fff3-456f4d3e5dd0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 2,644,096 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A look into how model will output on the above input."
      ],
      "metadata": {
        "id": "sN_nIO_-LdDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model_out = model((art_inp, sum_inp, s0, c0))\n",
        "\n",
        "print(f\"Model output has a type: {type(sample_model_out)}\")\n",
        "print(f\"Model Output list for the Inputs above are of length: {len(sample_model_out)}\")\n",
        "print(f\"Model Output list has each output of shape: {sample_model_out[0].shape}\")"
      ],
      "metadata": {
        "id": "xzqmZfKoLcfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "32287225-4fc5-4b3a-dc15-8b2c005978ab"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output has a type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Model Output list for the Inputs above are of length: 16\n",
            "Model Output list has each output of shape: (99, 20000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_v1(y_true, y_pred):\n",
        "  '''Calculates the loss for the baseline model. The loss is calculated by taking the negative\n",
        "  log-likelihood of the target word(w*_t) in the current timestep. Then the overall loss\n",
        "  is the summation over all timesteps divided by T (not Ty because it would include paddings also).\n",
        "\n",
        "  Arguments:\n",
        "    y_true: tf.Tensor object, true values for the target\n",
        "    y_pred: list of tf.Tensor objects, predicted probablities of the summary words\n",
        "\n",
        "  Returns:\n",
        "    returns the loss on the predicted values for the model\n",
        "  '''\n",
        "  # Calculate the loss for each item in the batch.\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "  # Remove the paddings from calculation of loss\n",
        "  mask = tf.cast(y_true != 0, loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  # Divide the total loss after masking out paddings divided by total words which are not paddings\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def custom_accuracy_v1(y_true, y_pred):\n",
        "  '''Calculates accuracy of the baseline model. The accuracy is calculated by matching how many correct\n",
        "  words were predicted excluding the paddings. Then, just add those which are correct and you will get the\n",
        "  the accuracy and then just divide it by total words not including padding.\n",
        "\n",
        "  Arguments:\n",
        "    y_true: tf.Tensor object, expected target values\n",
        "    y_pred: list of tf.Tensor object, predicted target values by model\n",
        "\n",
        "  Returns:\n",
        "    returns the total accuracy over the batch of data\n",
        "  '''\n",
        "  # Find the word index with maximum probablity\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  # Count the words that matches with true values\n",
        "  match = tf.cast(y_pred == y_true, tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  # Mask out the paddings\n",
        "  match *= mask\n",
        "\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "dw-z3KccJdpj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "194cd324-9fd0-44e9-8aa1-e8a4f3b73e50"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sample y true values: {sum_tar}\")\n",
        "print(f\"Sample y pred values(first 10 values of first 2 timestep): {sample_model_out[:2]}\")\n",
        "\n",
        "sample_loss = custom_loss_v1(sum_tar, sample_model_out)\n",
        "print(f\"Loss of the sample y_true and y_pred: {sample_loss}\")\n",
        "\n",
        "sample_acc = custom_accuracy_v1(sum_tar, sample_model_out)\n",
        "print(f\"Accuracy of the sample y_true and y_pred: {sample_acc}\")"
      ],
      "metadata": {
        "id": "SR25pZHrACmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "ee0770bc-19c8-4c74-d6cd-be5a51114c64"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample y true values: [[   2 1594 1589 ...    0    0    0]\n",
            " [  36 2526 7765 ...    0    0    0]\n",
            " [1024  358 5136 ...    0    0    0]\n",
            " ...\n",
            " [ 446  178    2 ...    0    0    0]\n",
            " [  68   47 1745 ...    0    0    0]\n",
            " [ 241 1043    1 ...    0    0    0]]\n",
            "Sample y pred values(first 10 values of first 2 timestep): [[[4.9766826e-05 4.9918344e-05 4.9897033e-05 ... 4.9849703e-05\n",
            "   5.0315728e-05 5.0180632e-05]\n",
            "  [4.9540908e-05 4.9899852e-05 4.9744962e-05 ... 4.9732076e-05\n",
            "   5.0619914e-05 5.0087474e-05]\n",
            "  [4.9342078e-05 4.9915510e-05 4.9560989e-05 ... 4.9633010e-05\n",
            "   5.0835100e-05 4.9942733e-05]\n",
            "  ...\n",
            "  [4.8487480e-05 5.0055794e-05 4.8468002e-05 ... 4.9327318e-05\n",
            "   5.1671042e-05 4.9163184e-05]\n",
            "  [4.8487480e-05 5.0055794e-05 4.8468002e-05 ... 4.9327318e-05\n",
            "   5.1671042e-05 4.9163184e-05]\n",
            "  [4.8487480e-05 5.0055794e-05 4.8468009e-05 ... 4.9327322e-05\n",
            "   5.1671042e-05 4.9163184e-05]]\n",
            "\n",
            " [[4.9986847e-05 4.9694067e-05 5.0172279e-05 ... 5.0206174e-05\n",
            "   5.0213788e-05 5.0044073e-05]\n",
            "  [5.0031969e-05 4.9539321e-05 5.0381001e-05 ... 5.0354411e-05\n",
            "   5.0285667e-05 5.0100505e-05]\n",
            "  [5.0090632e-05 4.9461749e-05 5.0534971e-05 ... 5.0449049e-05\n",
            "   5.0280312e-05 5.0154122e-05]\n",
            "  ...\n",
            "  [5.0358569e-05 4.9316510e-05 5.0878778e-05 ... 5.0763316e-05\n",
            "   5.0129740e-05 5.0536150e-05]\n",
            "  [5.0358594e-05 4.9316495e-05 5.0878807e-05 ... 5.0763316e-05\n",
            "   5.0129740e-05 5.0536142e-05]\n",
            "  [5.0358627e-05 4.9316488e-05 5.0878847e-05 ... 5.0763323e-05\n",
            "   5.0129747e-05 5.0536146e-05]]]\n",
            "Loss of the sample y_true and y_pred: 9.907593727111816\n",
            "Accuracy of the sample y_true and y_pred: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LEARNING_RATE\n",
        "initial_accumulator_value = INIT_ACC_VAL\n",
        "clipnorm = MAX_GRAD_NORM\n",
        "\n",
        "opt = Adagrad(learning_rate=lr,\n",
        "              initial_accumulator_value=initial_accumulator_value,\n",
        "              clipnorm=clipnorm)\n",
        "\n",
        "model.compile(loss=custom_loss_v1, optimizer=opt, metrics=[custom_loss_v1, custom_accuracy_v1])"
      ],
      "metadata": {
        "id": "bOTfC4BNnFgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "48f21c46-7ca9-461a-c987-c53e1689dae8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mention the checkpoint path and it's directory where you will save the model\n",
        "checkpoint_path = BASELINE_MODEL_CHECKPOINT\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Calculate no of batches, I am taking floor because when creating the training data I used drop_remainder\n",
        "n_batches = int(train_dataset.shape[0] / BATCH_SIZE)\n",
        "\n",
        "# Create the checkpoint for model saving, monitoring val_custom_accuracy_v1 and save only weights of the model\n",
        "saving_chkpnt = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                monitor='val_custom_accuracy_v1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_freq=n_batches//2)\n",
        "\n",
        "# Create the checkpoint for stopping early after noticing that val_custom_accuracy_v1 is not increasing even after 5 consecutive epochs\n",
        "earlystop_chkpnt = tf.keras.callbacks.EarlyStopping(monitor='val_custom_accuracy_v1',\n",
        "                                                    patience=PATIENCE,\n",
        "                                                    mode='max',\n",
        "                                                    )\n",
        "\n",
        "# Store the checkpoints in a list\n",
        "checkpoints = [saving_chkpnt, earlystop_chkpnt]"
      ],
      "metadata": {
        "id": "_OFR6b5hD10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = EPOCHS\n",
        "steps_per_epoch = STEPS_PER_EPOCHS\n",
        "\n",
        "history = model.fit(tf_train_dataset.repeat(),\n",
        "                    epochs=epochs,\n",
        "                    validation_data=tf_val_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    checkpoint=checkpoints\n",
        "                    )"
      ],
      "metadata": {
        "id": "OQE9gr_Bd9cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pointer Generetor model: Adding Pointer Generator to avoid getting OOV tokens in the summary"
      ],
      "metadata": {
        "id": "cDBfIISTS0j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating `tf_train_dataset`, `tf_val_dataset` dataset using tf.data API\n",
        "\n",
        "I already have created to the `generate_example` function, I can use it again to generate examples for training and validation respectively.\n",
        "\n",
        "But this time I have to also consider the `<OOV>` with token value `1`. Instead of using `1`, I have to generate a new token value for each of the `<OOV>` words.\n",
        "\n",
        "How to do it?\n"
      ],
      "metadata": {
        "id": "0Y9oJUdnUM_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example_v2(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n",
        "  '''Generates examples for the Pointer Generator model. Processes the `inputs` and `targets`\n",
        "  with their respective tokenizers and tokenize them to `input_len` and `target_len` length.\n",
        "  After tokenizing the article words, it looks for the out-of-vocabulary words and creates unique tokens\n",
        "  for each of those OOV words. Then, instead of keeping the oov word tokens as 1, it replaces them\n",
        "  with their respective newly generated tokens. This tokens are temporary\n",
        "\n",
        "  Arguments:\n",
        "    inputs: list of input sentences\n",
        "    targets: list of target sentences\n",
        "    input_tokenizer: Tokenizer class object, tokenizer for inputs\n",
        "    target_tokenizer: Tokenizer class object, tokenizer for targets\n",
        "    input_len: int, the length of the tokenization for inputs\n",
        "    target_len: int, the length of the tokenization for targets\n",
        "\n",
        "  Returns:\n",
        "    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n",
        "    another numpy array target_tokens[1:]\n",
        "  '''\n",
        "\n",
        "  for inp, tar in zip(inputs, targets):\n",
        "    # Tokenizing article words\n",
        "    inp_tokens = tokenize_pad([inp],\n",
        "                              input_tokenizer,\n",
        "                              padding=\"post\",\n",
        "                              truncating=\"post\",\n",
        "                              maxlen=input_len)\n",
        "\n",
        "    # Tokenizing summary words\n",
        "    tar_tokens = tokenize_pad([tar],\n",
        "                 target_tokenizer,\n",
        "                 padding=\"post\",\n",
        "                 truncating=\"post\",\n",
        "                 maxlen=target_len)\n",
        "\n",
        "    yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]"
      ],
      "metadata": {
        "id": "bWzy1fmHTG_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pointer Generator\n",
        "\n",
        "The function `generate_pointer` will return the $p_{gen}$. $p_{gen}$ will be used to decide whether the next word needs to be copied from the article or we should generate a new word from the vocabulary. This decision will be taken using the equation- $$P(w) = p_{gen}*P_{vocab}(w) + (1-p_{gen})*\\sum_{i:w_i=w}a^t_i$$\n",
        "\n",
        "Where\n",
        "- $P$ is the extended vocabulary containing words from vocabulary and article both.\n",
        "- $a^t_i$ is the attention $t^{th}$ time step need to give to $i^{th}$ article word.\n",
        "- $P_{vocab}$ is the previous vocabulary distribution without considering words from article."
      ],
      "metadata": {
        "id": "j7wikoNOuKsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pointer():\n",
        "  pass"
      ],
      "metadata": {
        "id": "P7y1utw3mAZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}