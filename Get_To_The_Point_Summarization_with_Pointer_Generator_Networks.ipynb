{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"For the wraping the outputs of colab","metadata":{"id":"aWWumM-N6C31"}},{"cell_type":"code","source":"from IPython.display import HTML, display\n\ndef set_css():\n  display(HTML('''\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  '''))\nget_ipython().events.register('pre_run_cell', set_css)","metadata":{"id":"J2_Q63jmdSp3","execution":{"iopub.status.busy":"2023-08-18T09:04:34.884921Z","iopub.execute_input":"2023-08-18T09:04:34.885349Z","iopub.status.idle":"2023-08-18T09:04:34.926814Z","shell.execute_reply.started":"2023-08-18T09:04:34.885314Z","shell.execute_reply":"2023-08-18T09:04:34.925816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports Libraries","metadata":{"id":"A3eWiUNHDwjn"}},{"cell_type":"code","source":"import os\nimport pathlib\nimport re\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport pickle\n\n# For tokenizing and processing the examples for the model training\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Layers for the Encoder, Attention and Decoder\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\nfrom tensorflow.keras.layers import RepeatVector, Concatenate, Activation, Dot\nfrom tensorflow.keras.layers import Dense\n\n# For model initialization\nfrom tensorflow.keras import Model\n\n# For training the model\nfrom tensorflow.keras.optimizers.experimental import Adagrad","metadata":{"id":"x6QucUCZ0q2V","execution":{"iopub.status.busy":"2023-08-18T09:04:39.987819Z","iopub.execute_input":"2023-08-18T09:04:39.988243Z","iopub.status.idle":"2023-08-18T09:04:50.188957Z","shell.execute_reply.started":"2023-08-18T09:04:39.988209Z","shell.execute_reply":"2023-08-18T09:04:50.187781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download Data","metadata":{"id":"Z-SylnbUD2s6"}},{"cell_type":"code","source":"# Download the CNN stories from the url into cnn_stories_tgz file\ncnn_stories_tgz = tf.keras.utils.get_file(\n    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n)\n\n# Download the Dailymail stories from the url into dailymail_stories_tgz file\ndailymail_stories_tgz = tf.keras.utils.get_file(\n    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n)","metadata":{"id":"k_03wyoLlFhO","outputId":"ad1d3d12-a264-45b5-8773-b5c923cf5fd9","execution":{"iopub.status.busy":"2023-08-18T09:05:10.310942Z","iopub.execute_input":"2023-08-18T09:05:10.311784Z","iopub.status.idle":"2023-08-18T09:05:18.918373Z","shell.execute_reply.started":"2023-08-18T09:05:10.311745Z","shell.execute_reply":"2023-08-18T09:05:18.917196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_stories_tgz, dailymail_stories_tgz","metadata":{"id":"fyid1XyFl1sp","outputId":"fa9020ad-37f3-4249-f23b-c41499da48d5","execution":{"iopub.status.busy":"2023-08-18T09:05:28.229929Z","iopub.execute_input":"2023-08-18T09:05:28.230966Z","iopub.status.idle":"2023-08-18T09:05:28.241222Z","shell.execute_reply.started":"2023-08-18T09:05:28.230918Z","shell.execute_reply":"2023-08-18T09:05:28.239805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -xzf /root/.keras/datasets/cnn_stories.tgz\n!tar -xzf /root/.keras/datasets/dailymail_stories.tgz","metadata":{"id":"EHJtYhZpls0i","execution":{"iopub.status.busy":"2023-08-18T09:06:30.820833Z","iopub.execute_input":"2023-08-18T09:06:30.821259Z","iopub.status.idle":"2023-08-18T09:06:58.200158Z","shell.execute_reply.started":"2023-08-18T09:06:30.821226Z","shell.execute_reply":"2023-08-18T09:06:58.198678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_stories_dir = pathlib.Path('/kaggle/working/cnn/stories')\ndailymail_stories_dir = pathlib.Path('/kaggle/working/dailymail/stories')","metadata":{"id":"ag5ubSt7l88l","execution":{"iopub.status.busy":"2023-08-18T09:09:12.911895Z","iopub.execute_input":"2023-08-18T09:09:12.912353Z","iopub.status.idle":"2023-08-18T09:09:12.920187Z","shell.execute_reply.started":"2023-08-18T09:09:12.912315Z","shell.execute_reply":"2023-08-18T09:09:12.919002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_stories_dir, dailymail_stories_dir","metadata":{"id":"UmMCM--Yhm3B","outputId":"8cb6a093-72f6-40df-a083-2d5c4e35f2e1","execution":{"iopub.status.busy":"2023-08-18T09:09:17.489802Z","iopub.execute_input":"2023-08-18T09:09:17.490265Z","iopub.status.idle":"2023-08-18T09:09:17.499780Z","shell.execute_reply.started":"2023-08-18T09:09:17.490225Z","shell.execute_reply":"2023-08-18T09:09:17.498603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Print Stories","metadata":{"id":"WqScGmO6D5XI"}},{"cell_type":"code","source":"def print_filenames(dir_path, num_files=5):\n    '''Prints the name of the files that are present at `dir_path`.\n    Maximum `num_files` number of files are shown.\n\n    Arguments:\n    dir_path: PosixPath, pointing to the directory of which the user\n              wants to prints the file names.\n    num_files: int, number of files user wants to print.\n\n    returns:\n    nothing\n    '''\n\n    count = 0\n    for f in dir_path.glob('*.story'):\n        print(f.name)\n        count += 1\n\n        if count == num_files:\n            break\n    else:\n        print(f\"Less than {num_files} is present!\")","metadata":{"id":"dZFFXG7Pg86H","execution":{"iopub.status.busy":"2023-08-18T09:10:03.185487Z","iopub.execute_input":"2023-08-18T09:10:03.185941Z","iopub.status.idle":"2023-08-18T09:10:03.195424Z","shell.execute_reply.started":"2023-08-18T09:10:03.185905Z","shell.execute_reply":"2023-08-18T09:10:03.194025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_filenames(cnn_stories_dir)","metadata":{"id":"nv3g7-M316yL","outputId":"1cb2a81e-2da0-4875-ed50-778ef6351829","execution":{"iopub.status.busy":"2023-08-18T09:10:04.749738Z","iopub.execute_input":"2023-08-18T09:10:04.750174Z","iopub.status.idle":"2023-08-18T09:10:04.866320Z","shell.execute_reply.started":"2023-08-18T09:10:04.750137Z","shell.execute_reply":"2023-08-18T09:10:04.865080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_filenames(dailymail_stories_dir)","metadata":{"id":"WcyW1wJa2EaN","outputId":"a8c02393-6b34-4fd4-b013-edf41ebb38f9","execution":{"iopub.status.busy":"2023-08-18T09:10:07.134644Z","iopub.execute_input":"2023-08-18T09:10:07.135063Z","iopub.status.idle":"2023-08-18T09:10:07.401125Z","shell.execute_reply.started":"2023-08-18T09:10:07.135028Z","shell.execute_reply":"2023-08-18T09:10:07.400026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking a sample .story file from cnn stories\nsample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\nsample_filedir = cnn_stories_dir\n\nsample_filepath = sample_filedir / sample_filename\nwith open(sample_filepath, 'r') as f:\n    sample_story = f.read()\n\nprint(f\"A sample story:\\n{sample_story}\")","metadata":{"id":"Uw0kqchX3X7X","outputId":"b8a73eb0-be8e-4c46-d84c-6b67080ca83f","execution":{"iopub.status.busy":"2023-08-18T09:10:10.909621Z","iopub.execute_input":"2023-08-18T09:10:10.910039Z","iopub.status.idle":"2023-08-18T09:10:10.920991Z","shell.execute_reply.started":"2023-08-18T09:10:10.910006Z","shell.execute_reply":"2023-08-18T09:10:10.919512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Global Variables","metadata":{"id":"ihAsXBceD7ew"}},{"cell_type":"code","source":"# Define the global variables\ndm_single_close_quote = u'\\u2019' # unicode for closing single quote\ndm_double_close_quote = u'\\u201d' # unicode for closing double quote\nEND_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n              dm_single_close_quote, dm_double_close_quote, \")\"]\n\n# Maximum stories to process from cnn and dailymail each\nMAX_STORIES = 310000\n\n# From the total data how to split into train, val and test\nTRAIN_SIZE = 0.8 # Fraction of the total dataset to use for training\nVAL_SIZE = 0.1 # Fraction of the total dataset to use for validation\nTEST_SIZE = 0.1 # Fraction of the total dataset to use for testing\n\n# For tokenization\nVOCAB_SIZE = 50000 # Vocabulary size or no of unique words\nOOV_TOKEN = \"<OOV>\" # Word token to represent the out-of-vocabulary words\n\n# For standardization\nSTART_TOKEN = '<START>' # Starting word of each sentence\nEND_TOKEN = '<END>' # Ending word of each sentence\n\n# Total tokens to represent encoder input sentence and decoder input sentence(Values are taken from paper)\nMAX_ARTICLE_TOKENS = 400 # Maximum no of tokens to consider in article when processing them for model\nMAX_SUMMARY_TOKENS = 100 # Maximum no of tokens to consider in summary when processing them for model\n\n# For dataset creation hyperparameters\nBUFFER_SIZE = 5000 # Buffer size when using shuffle\nBATCH_SIZE = 16 # No of examples in each batch\n\n# Model Archietecture hyperparameters (Values are taken from the paper)\nEMB_OUT = 128 # Embedding output dimension\nENCODER_STATE_DIM = 256 # Encoder hidden(also cell) state dimension\nDECODER_STATE_DIM = 2*ENCODER_STATE_DIM # Decoder hidden(also cell) state dimension\nDENSE1_UNITS = 128 # Attention first dense layer units(calculates partial energy)\nDENSE2_UNITS = 1 # Attention secodn dense layer units(calculated final energy)\nDENSE_UNITS = 512 # Units of the Dense layers before output layer, make sure it is same as decoder state dimension\n\n# Model Optimizer hyperparameters (Values are taken from the paper)\nLEARNING_RATE=0.15 # Learning rate\nINIT_ACC_VAL=0.1 # Initial accumulator value\nMAX_GRAD_NORM=2 # Gardient norm\n\n# Model Checkpoint hyperparameters\nBASELINE_MODEL_CHECKPOINT = \"/kaggle/working/baseline-model/cp-{epoch:04d}.ckpt\"\nPOINTER_MODEL_CHECKPOINT = \"/kaggle/working/pointer-model/cp-{epoch:04d}.ckpt\"\nCOVERAGE_MODEL_CHECKPOINT = \"/kaggle/working/coverage-model/cp-{epoch:04d}.ckpt\"\nPATIENCE = 5\n\n## Model Fit (All values are choosen closer to the ones in the paper)\n# Base model\nBASE_EPOCHS = 33\nSTEPS_PER_EPOCHS_BASE = 18181\n# Pointer Model\nPOINTER_EPOCHS = 13\nPOINTER_STEPS_PER_EPOCHS = 18000\n# Coverage Model\nCOVERAGE_EPOCHS = 1\nCOVERAGE_STEPS_PER_EPOCHS = 3000\n\n# Coverage mechanism\nLAMBDA_VAL = 1","metadata":{"id":"cubLAm1Q3SuG","execution":{"iopub.status.busy":"2023-08-18T09:10:45.342763Z","iopub.execute_input":"2023-08-18T09:10:45.343186Z","iopub.status.idle":"2023-08-18T09:10:45.358008Z","shell.execute_reply.started":"2023-08-18T09:10:45.343154Z","shell.execute_reply":"2023-08-18T09:10:45.356854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fixing periods of the stories","metadata":{"id":"UXHUPXvlEDTo"}},{"cell_type":"markdown","source":"I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n\nThese are the steps -\n1. Check if line contains `@highlight`, if True then just return the line.\n2. Check if line is empty, then return line as it is.\n3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n4. Only is none of the above conditions match then append `.` to the current line.","metadata":{"id":"iZPt8EVBzJ8a"}},{"cell_type":"code","source":"def fix_missing_period(line, end_tokens=END_TOKENS):\n    '''function to fix the missing periods for some story lines which do not end with\n    any of the end_tokens mentioned.\n\n    Argument:\n    line: string, line of the story to fix the missing the period of.\n    end_tokens: list of strings, all the tokens that are considered as line end.\n\n    Returns:\n    new line with fixed the ending part by adding an ending token if not present.\n    '''\n    if \"@highlight\" in line:\n        return line\n    elif line == \"\":\n        return line\n    elif line[-1] in end_tokens:\n        return line\n\n    return line + '.'","metadata":{"id":"i-S-Hss12TPk","execution":{"iopub.status.busy":"2023-08-18T09:11:07.409566Z","iopub.execute_input":"2023-08-18T09:11:07.409975Z","iopub.status.idle":"2023-08-18T09:11:07.419617Z","shell.execute_reply.started":"2023-08-18T09:11:07.409942Z","shell.execute_reply":"2023-08-18T09:11:07.418360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = \"i have a bad habit of not giving full-stop after sentence\\nLike this setence\"\nprint(f\"Fixing {fix_missing_period(sample_text)}\")","metadata":{"id":"L1hiNysxnmAO","outputId":"1493cc4e-e13e-49d1-bb3d-907b44d3c67d","execution":{"iopub.status.busy":"2023-08-18T09:11:19.497232Z","iopub.execute_input":"2023-08-18T09:11:19.497683Z","iopub.status.idle":"2023-08-18T09:11:19.506186Z","shell.execute_reply.started":"2023-08-18T09:11:19.497647Z","shell.execute_reply":"2023-08-18T09:11:19.504933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the Stories into articles and summaries","metadata":{"id":"fEoN3HDSEHjI"}},{"cell_type":"markdown","source":"I am creating a function `split_article_summary` which will split the story into article and summary parts.\n\nThe function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n\nThe steps to follow are -\n1. Split the story by new line `\\n`. I will get a list of lines.\n2. Strip the lines by using list comprehension.\n3. Use list comprehension to make lower case each line by using `.lower()`.\n4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n5. Make 2 empty list for `article` and `summary`.\n6. Go through each line. In each line, I need to check 4 things,\n  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n  * line is `\"\"` empty or not, if True then ignore.\n  * `next_highlight` is True or not, if True then append the line to `summary`.\n  * If non of the ebove then append to `article`.\n7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method.","metadata":{"id":"5-tqWtoowXxl"}},{"cell_type":"code","source":"def split_article_summary(story):\n    '''Splits the story into 2 parts, one for article and other for summary of that\n    article. Returns the article and summary.\n\n    Argument:\n    story: string file that contains both article and summary combiningly.\n\n    Returns:\n    article, summary seperately from the story.\n\n    '''\n    lines = story.split('\\n')\n    lines = [line.strip() for line in lines]\n    lines = [line.lower() for line in lines]\n\n    # Fix the ending period\n    lines = [fix_missing_period(line) for line in lines]\n\n    # List to contain the article and summary lines\n    article = []\n    summary = []\n\n    # Indicator of whether the next line is the summary or not\n    next_highlight = False\n\n    for line in lines:\n        if \"@highlight\" in line:\n            next_highlight = True\n        elif line==\"\":\n            continue\n        elif next_highlight:\n            summary.append(line)\n        else:\n            article.append(line)\n\n    article = ' '.join(article)\n    summary = ' '.join(summary)\n\n    return article, summary","metadata":{"id":"-X4eMltQnf10","execution":{"iopub.status.busy":"2023-08-18T09:12:41.034505Z","iopub.execute_input":"2023-08-18T09:12:41.034964Z","iopub.status.idle":"2023-08-18T09:12:41.046419Z","shell.execute_reply.started":"2023-08-18T09:12:41.034933Z","shell.execute_reply":"2023-08-18T09:12:41.045179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_article, sample_summary = split_article_summary(sample_story)\n\nprint(f\"Sample Article after spliting:\\n{sample_article}\")\nprint(f\"Sample Summary after spliting:\\n{sample_summary}\")","metadata":{"id":"nuUjOaN9orGU","outputId":"e0133cca-84cd-4b01-e063-ec57f7244ae3","execution":{"iopub.status.busy":"2023-08-18T09:12:42.989742Z","iopub.execute_input":"2023-08-18T09:12:42.990415Z","iopub.status.idle":"2023-08-18T09:12:43.000128Z","shell.execute_reply.started":"2023-08-18T09:12:42.990377Z","shell.execute_reply":"2023-08-18T09:12:42.998704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n\nThis function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n\nThe process is simple. We will follow this steps -\n1. Create 2 empty lists of `articles` and `summaries`.\n2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n6. Return the 2 lists.","metadata":{"id":"oTEa0m3Huxz4"}},{"cell_type":"code","source":"def get_articles_summaries(stories_dir, max_stories):\n    '''stores the stories from stories_dir folder into a list and returns the list\n\n    Arguments:\n    stories_dir: Posix string, the directory where the stories are stored\n    max_stories: maximum number of stories to store\n\n    Returns:\n    list of stories.\n\n    '''\n    articles = []\n    summaries = []\n\n    count = 0\n    for f in stories_dir.glob(\"*.story\"):\n        count += 1\n        with open(f, 'r') as reader:\n            story = reader.read()\n\n            article, summary = split_article_summary(story)\n\n            articles.append(article)\n            summaries.append(summary)\n\n        if count == max_stories:\n            break\n\n    return articles, summaries","metadata":{"id":"4VUmbYSpnjAr","execution":{"iopub.status.busy":"2023-08-18T09:13:28.590371Z","iopub.execute_input":"2023-08-18T09:13:28.590797Z","iopub.status.idle":"2023-08-18T09:13:28.600351Z","shell.execute_reply.started":"2023-08-18T09:13:28.590766Z","shell.execute_reply":"2023-08-18T09:13:28.599094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them.","metadata":{"id":"Uk-BuCM4rIiO"}},{"cell_type":"code","source":"cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)","metadata":{"id":"aa9ZDQntpHQZ","execution":{"iopub.status.busy":"2023-08-18T09:13:32.094813Z","iopub.execute_input":"2023-08-18T09:13:32.095240Z","iopub.status.idle":"2023-08-18T09:13:43.652568Z","shell.execute_reply.started":"2023-08-18T09:13:32.095206Z","shell.execute_reply":"2023-08-18T09:13:43.651398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\nprint(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\nprint(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")","metadata":{"id":"-q_i-69YqJnj","outputId":"6078cf9e-816e-49fd-a540-7fda9f3b03c1","execution":{"iopub.status.busy":"2023-08-18T09:14:22.691220Z","iopub.execute_input":"2023-08-18T09:14:22.691724Z","iopub.status.idle":"2023-08-18T09:14:22.701042Z","shell.execute_reply.started":"2023-08-18T09:14:22.691686Z","shell.execute_reply":"2023-08-18T09:14:22.699952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n                                                                 MAX_STORIES)","metadata":{"id":"KT4Hrp6nqeKu","execution":{"iopub.status.busy":"2023-08-18T09:14:47.696034Z","iopub.execute_input":"2023-08-18T09:14:47.696738Z","iopub.status.idle":"2023-08-18T09:15:35.178254Z","shell.execute_reply.started":"2023-08-18T09:14:47.696670Z","shell.execute_reply":"2023-08-18T09:15:35.177080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total no of dailymail stories captured are {len(dailymail_articles)}\\n\\n\")\nprint(f\"One of the Dailymail articles: {dailymail_articles[0]}\\n\\n\")\nprint(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")","metadata":{"id":"nkzwSP9kh4VK","outputId":"0d3aaab8-5a41-46ae-95b9-d2949dc9e336","execution":{"iopub.status.busy":"2023-08-18T09:15:35.180381Z","iopub.execute_input":"2023-08-18T09:15:35.180778Z","iopub.status.idle":"2023-08-18T09:15:35.189921Z","shell.execute_reply.started":"2023-08-18T09:15:35.180744Z","shell.execute_reply":"2023-08-18T09:15:35.188831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data into training, validation and testing set","metadata":{"id":"nEaIcT3lEaez"}},{"cell_type":"markdown","source":"I am creating another function -\n`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples.","metadata":{"id":"8ea-PhS3iIJZ"}},{"cell_type":"code","source":"def split_dataset(dataset, train_size, val_size, test_size):\n    first_split = train_size\n    second_split = train_size+val_size\n    third_split = train_size+val_size+test_size\n    return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]","metadata":{"id":"v-iZDbUCqkdC","execution":{"iopub.status.busy":"2023-08-18T09:15:48.130751Z","iopub.execute_input":"2023-08-18T09:15:48.131181Z","iopub.status.idle":"2023-08-18T09:15:48.140749Z","shell.execute_reply.started":"2023-08-18T09:15:48.131147Z","shell.execute_reply":"2023-08-18T09:15:48.139340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us create a function `make_datasets`, that will be make training, validation and testing datasets. This function will -\n1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\nObjective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n```python\n[1, 2] + [3, 4] = [1, 2, 3, 4]\n```\n\n3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n```python\n  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n```\n4. Shuffle the dataset using `random.sample` method.\n```python\nrandom.seed(seed_value) # To make sure that everytime it gives the same shuffle\nrandom.sample(list_to_shuffle, len(list_to_shuffle))\n```\n5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`.","metadata":{"id":"FTB5eNzJrqyr"}},{"cell_type":"code","source":"def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n    '''Create 3 datasets each for training, validation and testing respectively.\n    This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n    them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n    summaries using random.sample method (although we have a helper function for it). Finally we do the\n    splitting of the whole dataset. Remember here the returned values become tensors.\n\n    Arguments:\n    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n    train_size: float, specifying how much fraction of the original dataset to take for training.\n    val_size: float, specifying how much fraction of the original dataset to take for validation.\n    test_size: float, specifying how much fraction of the original dataset to take for testing.\n\n    Returns:\n    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n    with the specified amount of data in it.\n    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n    '''\n    articles = cnn_stories[0] + dailymail_stories[0]\n    summaries = cnn_stories[1] + dailymail_stories[1]\n\n    articles = np.array(articles, dtype=object)\n    summaries = np.array(summaries, dtype=object)\n\n    dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n\n    random.seed(seed_value)\n    shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n\n    dataset = dataset[shuffled_indices, :]\n\n    train_size = int(train_fraction * dataset.shape[0])\n    val_size = int(val_fraction * dataset.shape[0])\n    test_size = dataset.shape[0] - (train_size + val_size)\n\n    training_samples, validation_samples, testing_samples = split_dataset(dataset,\n                                                                        train_size,\n                                                                        val_size,\n                                                                        test_size)\n\n    return (training_samples, validation_samples, testing_samples)","metadata":{"id":"QDB0_32RrnHk","execution":{"iopub.status.busy":"2023-08-18T09:16:02.410953Z","iopub.execute_input":"2023-08-18T09:16:02.411440Z","iopub.status.idle":"2023-08-18T09:16:02.433703Z","shell.execute_reply.started":"2023-08-18T09:16:02.411395Z","shell.execute_reply":"2023-08-18T09:16:02.431390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)","metadata":{"id":"GTnXBwd6Sa-U","execution":{"iopub.status.busy":"2023-08-18T09:16:04.555904Z","iopub.execute_input":"2023-08-18T09:16:04.556308Z","iopub.status.idle":"2023-08-18T09:16:05.367869Z","shell.execute_reply.started":"2023-08-18T09:16:04.556277Z","shell.execute_reply":"2023-08-18T09:16:05.366508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n\nprint(f\"Training dataset shape: {train_dataset.shape}\")\nprint(f\"Validation dataset shape: {val_dataset.shape}\")\nprint(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n\nprint(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")","metadata":{"id":"AyRVodwIhkuQ","outputId":"d0bfc21f-2835-45a6-de7e-07f430eabcd0","execution":{"iopub.status.busy":"2023-08-18T09:16:10.023227Z","iopub.execute_input":"2023-08-18T09:16:10.023696Z","iopub.status.idle":"2023-08-18T09:16:10.034244Z","shell.execute_reply.started":"2023-08-18T09:16:10.023660Z","shell.execute_reply":"2023-08-18T09:16:10.033121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Tokenizing","metadata":{"id":"dGP0eDXMEhdF"}},{"cell_type":"markdown","source":"Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n\n1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n2. Replace the single and double opening and closing quotes like `‘ → \\u2018`, `’ → \\u2019`, `“ → \\u201c` and `” → \\u201d` by `'` and `\"` respectively.\n3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\nIn this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n\n`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n1. Remove the `START_TOKEN` and `END_TOKEN` from the text. So that tokenizer does not standardize them.\n2. Standardize the text with `standardizer`.\n3. Add back the `START_TOKEN` and `END_TOKEN` because you want your tokenizer to learn them.\n4. Remove unwanted spaces in between words.\n5. Split the text into words which are seperated by ' '.\n6. Strip each of the words in the sentence. Finally, return it.","metadata":{"id":"TcZCpJ9hCyqf"}},{"cell_type":"code","source":"# Standardize the text data\ndef standardizer(text):\n    '''Standardize the text provided to the function\n    The text is lower cased. Then, the opening and closing quotes are removed. I add spaces before the\n    punctuations like `don't` becomes `don ' t`, ignoring the numerical values so that `1.78` does not become\n    `1 . 78`. Finally, it strips the text and removes any type of unwanted spaces in it.\n\n    Argument:\n    text: str, the text to standardize\n\n    Returns:\n    returns the standadized text\n    '''\n\n    # Lower case the text\n    text = text.lower()\n\n    # Replace the special single and double opening and closing quotes\n    text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n    text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n\n    # Add space before punctuations and ignore floating point numbers.\n    text = re.sub(r'(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n                  r' \\1 ', text)  # It used to also remove commas after numbers like '27,' will be removed\n\n    # Remove spaces after sentence end and other unwanted spaces from text\n    text = text.strip()\n    text = re.sub('\\s{2,}', ' ', text)\n\n    return text\n\n# custom analyzer for the Tokenizer class\ndef custom_analyzer(text):\n    '''Custom analyzer to provide to the `Tokenizer` class when creating the tokenizer.\n\n    Argument:\n    text: str, the text that will be tokenized\n\n    Returns:\n    returns the splitted sentence\n    '''\n    # Remove START and END before standardizing\n    if START_TOKEN in text:\n        text = re.sub(f'{START_TOKEN} ', '', text)\n    if END_TOKEN in text:\n        text = re.sub(f'{END_TOKEN} ', '', text)\n\n    # Standardize the text first\n    text = standardizer(text)\n\n    # Add back the START and END tokens\n    text = ' '.join([START_TOKEN, text, END_TOKEN])\n\n    # Split the sentence into words to tokenize\n    words = text.split(' ')\n    words = [word.strip() for word in words]\n\n    return words","metadata":{"id":"FD2h0OiAxJP_","execution":{"iopub.status.busy":"2023-08-18T09:16:53.387862Z","iopub.execute_input":"2023-08-18T09:16:53.388270Z","iopub.status.idle":"2023-08-18T09:16:53.401937Z","shell.execute_reply.started":"2023-08-18T09:16:53.388236Z","shell.execute_reply":"2023-08-18T09:16:53.400612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n                \"U.S won the world cup and bagged 1.78 million dollars.\",\n                \"India had M.S. Dhoni won made it this far.\",\n                \"My email address is arupjana7365@gmail.com.\",\n                \"It can take care of dailymail single opening quote’ also.\",\n                \"I have 10,000 Rs in my bank\",\n                \"This sentence has , after a number 12,\",\n                \"This sentence contains <START> token and <END> token.\"]\n\nprint(f\"After Standardizing the sample texts:\\n{[standardizer(text) for text in sample_texts]}\\n\")\nprint(f\"After applying custom analyzer on sample texts:\\n{[custom_analyzer(text) for text in sample_texts]}\")","metadata":{"id":"AVi3kZhcLXS7","outputId":"432ed183-e302-49c9-a704-c6a4257854ad","execution":{"iopub.status.busy":"2023-08-18T09:16:55.494719Z","iopub.execute_input":"2023-08-18T09:16:55.496184Z","iopub.status.idle":"2023-08-18T09:16:55.509183Z","shell.execute_reply.started":"2023-08-18T09:16:55.496138Z","shell.execute_reply":"2023-08-18T09:16:55.507632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n\nWhen fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab.","metadata":{"id":"IiiWRFR64jTP"}},{"cell_type":"code","source":"def get_tokenizer(texts, num_words, oov_token=None, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n    '''This will create the tokenizer needed for the task in hand.\n    The tokenizer will be trained on the `texts`. Tokenizer will have vocabulary length `num_words`.\n    The `oov_token` will be used as the token represent the out-of-vocabulary words. The `filters` are\n    the ones which the tokenizer will remove when tokenizing any sentence given to it. The returned\n    tokenizer is using a custom analyzer that can standardize the sentence before tokenizing using the\n    `standardizer` function and then splits the sentence into words. After that it tokenizes the sentence.\n    As for the vocabulary, the returned tokenizer's vocabulary does not contain any number, as I have removed\n    them before feeding them into `Tokenizer.fit_on_texts` method.\n\n    Arguments:\n    texts: list of strings, the tokenizer will be trained on this strings\n    num_words: int, number of vocabulary words the tokenizer will consider\n    oov_token: str, token to represent out-of-vocabulary words\n    filters: str, all the characters that the tokenizer will remove before tokenizing\n\n    Returns:\n    tokenzier of the `Tokenizer` class after learning vocabulary from `texts`\n    '''\n\n    # Create the tokenizer usinf Tokenizer class\n    tokenizer = Tokenizer(num_words=num_words,\n                        filters=filters,\n                        oov_token=oov_token,\n                        analyzer=custom_analyzer)\n\n    # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n    texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n\n    # Fit the data with fit_on_texts method\n    tokenizer.fit_on_texts(texts)\n\n    return tokenizer","metadata":{"id":"tR1FD3SESd0G","execution":{"iopub.status.busy":"2023-08-18T09:17:08.165112Z","iopub.execute_input":"2023-08-18T09:17:08.165562Z","iopub.status.idle":"2023-08-18T09:17:08.177690Z","shell.execute_reply.started":"2023-08-18T09:17:08.165510Z","shell.execute_reply":"2023-08-18T09:17:08.176401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")","metadata":{"id":"UsmErFoxqimM","outputId":"1625c203-569e-4a43-f69e-2da2f4d215ba","execution":{"iopub.status.busy":"2023-08-18T09:17:10.479242Z","iopub.execute_input":"2023-08-18T09:17:10.479722Z","iopub.status.idle":"2023-08-18T09:17:10.534230Z","shell.execute_reply.started":"2023-08-18T09:17:10.479685Z","shell.execute_reply":"2023-08-18T09:17:10.533285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the `tokenizer` using the articles from training dataset by using `train_dataset[:, 0]`, with a vocabulary size of `VOCAB_SIZE` and use `OOV_TOKEN` token to represent out-of-vocabulary words.","metadata":{"id":"6KBXcUV2pYrm"}},{"cell_type":"code","source":"tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)","metadata":{"id":"151rEpqo7Pof","execution":{"iopub.status.busy":"2023-08-18T09:24:47.903390Z","iopub.execute_input":"2023-08-18T09:24:47.904802Z","iopub.status.idle":"2023-08-18T09:37:27.205193Z","shell.execute_reply.started":"2023-08-18T09:24:47.904753Z","shell.execute_reply":"2023-08-18T09:37:27.203953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\\n\\n\")\n\n\nprint(f\"{OOV_TOKEN} word has index: {tokenizer.word_index[OOV_TOKEN]}\")\nprint(f\"{START_TOKEN} word has index: {tokenizer.word_index[START_TOKEN]}\")\nprint(f\"{END_TOKEN} word has index: {tokenizer.word_index[END_TOKEN]}\\n\\n\")\n\n\nprint(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\\n\")\n\nprint(f\"Text:\\n{train_dataset[0, 0]}\\n\\n\")\nsample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\nprint(f\"Text to Sequence of the first article:\\n{sample_sequence}\\n\")\nprint(f\"Sequence to Text of the first acrticle:\\n{tokenizer.sequences_to_texts(sample_sequence)}\")","metadata":{"id":"YUNreIjr8Kng","outputId":"ccabd900-e673-452c-fed3-5ce16481d655","execution":{"iopub.status.busy":"2023-08-18T09:37:27.207587Z","iopub.execute_input":"2023-08-18T09:37:27.207907Z","iopub.status.idle":"2023-08-18T09:37:27.220110Z","shell.execute_reply.started":"2023-08-18T09:37:27.207879Z","shell.execute_reply":"2023-08-18T09:37:27.219166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\nNO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that.","metadata":{"id":"crn5t_zk6iBN"}},{"cell_type":"code","source":"list(tokenizer.word_index.keys())[21000]","metadata":{"id":"qhttL-heeP-P","outputId":"3561bbef-d7ac-4c1d-e55d-0c6ce951703b","execution":{"iopub.status.busy":"2023-08-18T09:37:27.221252Z","iopub.execute_input":"2023-08-18T09:37:27.222048Z","iopub.status.idle":"2023-08-18T09:37:27.266863Z","shell.execute_reply.started":"2023-08-18T09:37:27.222014Z","shell.execute_reply":"2023-08-18T09:37:27.265576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oov_word = list(tokenizer.word_index.keys())[21000]\nsample_text = f\"This example is to test the above fact with the word `{oov_word}`\"\nsample_sequence = tokenizer.texts_to_sequences([sample_text])\n\nprint(f\"Text: {sample_text}\\n\\n\")\nprint(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\nprint(f\"Sequence: {sample_sequence}\")","metadata":{"id":"S_Dnkcig7SIX","outputId":"663e3047-151e-47d0-d44a-0dc2988e5ca9","execution":{"iopub.status.busy":"2023-08-18T09:37:27.270127Z","iopub.execute_input":"2023-08-18T09:37:27.270522Z","iopub.status.idle":"2023-08-18T09:37:27.306918Z","shell.execute_reply.started":"2023-08-18T09:37:27.270489Z","shell.execute_reply":"2023-08-18T09:37:27.305848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`.","metadata":{"id":"xvNeazXb-Yq9"}},{"cell_type":"code","source":"sample_text = \"What happens when I add a number 2.1 in this sentence!\"\nsample_sequence = tokenizer.texts_to_sequences([sample_text])\n\nprint(f\"Text: {sample_text}\\n\\n\")\nprint(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\nprint(f\"Sequence: {sample_sequence}\")","metadata":{"id":"ng5MZ_DZ84kk","outputId":"5006fd56-40c0-4876-d780-a341a68044ba","execution":{"iopub.status.busy":"2023-08-18T09:37:27.308424Z","iopub.execute_input":"2023-08-18T09:37:27.308866Z","iopub.status.idle":"2023-08-18T09:37:27.320552Z","shell.execute_reply.started":"2023-08-18T09:37:27.308836Z","shell.execute_reply":"2023-08-18T09:37:27.319478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = \"What happens when I add parenthesis (I am inside it!).\"\nsample_sequence = tokenizer.texts_to_sequences([sample_text])\n\nprint(f\"Text: {sample_text}\\n\\n\")\nprint(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\nprint(f\"Sequence: {sample_sequence}\")","metadata":{"id":"ImlMHniH-Jnt","outputId":"92b365af-3b23-4643-ad61-aa18e60daa47","execution":{"iopub.status.busy":"2023-08-18T09:37:27.322283Z","iopub.execute_input":"2023-08-18T09:37:27.322704Z","iopub.status.idle":"2023-08-18T09:37:27.334237Z","shell.execute_reply.started":"2023-08-18T09:37:27.322673Z","shell.execute_reply":"2023-08-18T09:37:27.333089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to prepare the data for feeding to the model","metadata":{"id":"EpMm2cNIEm_L"}},{"cell_type":"markdown","source":"We have the `tokenizer` to tokenize the articles and summaries. We need to pad those sequences to fit the requirements.\n\nIn the paper, the articles are limited to have 400 tokens and summary has, 100 tokens at training and 120 tokens for testing.\n\nI will be using `pad_sequences` method to pad or truncate the articles and summaries based on their length.\n\nNOTE: I am using same tokenizer for article and summary. But, later I might change that to 2 different tokenizers each having different `num_words`.","metadata":{"id":"jza9oQYKXB0m"}},{"cell_type":"code","source":"def tokenize_pad(texts, tokenizer, padding, truncating, maxlen):\n    '''Tokenize the `texts` using the tokenizer. Then, pad the sequences or truncate the sequences\n    depending the length. If the length exceeds `maxlen` then it will be truncated and if not then it will be\n    padded. The padding and truncating can happend at the beginning or at the end of the sequence depending\n    on the value of `padding` and `truncating` respectively.\n\n    Arguments:\n    texts: list of strings, the sentences to tokenize and pad\n    tokenizer: Tokenizer class object, helps in tokenizing the `texts`\n    padding: str, can take 2 values `pre` or `post`. If `pre` then padding will happen at the beginning,\n    if `post` then padding will happen at the end.\n    truncating: str, can take 2 values `pre` or 'truncating`, works the same as `padding`\n    maxlen: int, maximum length after padding or truncating\n\n    Returns:\n    returns the tokenized and padded sentences\n    '''\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n\n    return padded_sequences","metadata":{"id":"Alo70WYjW-tA","execution":{"iopub.status.busy":"2023-08-18T09:37:27.335638Z","iopub.execute_input":"2023-08-18T09:37:27.336130Z","iopub.status.idle":"2023-08-18T09:37:27.347465Z","shell.execute_reply.started":"2023-08-18T09:37:27.336099Z","shell.execute_reply":"2023-08-18T09:37:27.346346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_texts","metadata":{"id":"rttUwCiT4FzJ","outputId":"e0db10da-c722-4147-b6c0-3845d3103957","execution":{"iopub.status.busy":"2023-08-18T09:37:27.348636Z","iopub.execute_input":"2023-08-18T09:37:27.348930Z","iopub.status.idle":"2023-08-18T09:37:27.366506Z","shell.execute_reply.started":"2023-08-18T09:37:27.348904Z","shell.execute_reply":"2023-08-18T09:37:27.365290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenize_pad(sample_texts, tokenizer, padding=\"post\", truncating=\"post\", maxlen=20)","metadata":{"id":"QNuBKpTq4lZW","outputId":"0d73781e-8327-4369-c9a6-8918cd4f259f","execution":{"iopub.status.busy":"2023-08-18T09:37:27.368035Z","iopub.execute_input":"2023-08-18T09:37:27.368425Z","iopub.status.idle":"2023-08-18T09:37:27.384282Z","shell.execute_reply.started":"2023-08-18T09:37:27.368394Z","shell.execute_reply":"2023-08-18T09:37:27.383357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Model Architecture & Training","metadata":{"id":"Gt31OiYpEymZ"}},{"cell_type":"markdown","source":"After this, we need the model that we can train on this dataset. The model archietecture will be 3-\n1. Base-line model: Seq-Seq model with attention mechanism.\n2. Pointer Generetor model: With seq-seq attention model will be implementing the pointer generator that can either copy words from article or generate words from the pre-defined vocabulary.\n3. Coverage mechanism: Along with the pointer generator that will take case of the out-of-vocabulary words. Coverage mechanism will help prevent the repetition of the words in the summary.","metadata":{"id":"yM6KveyaUmfL"}},{"cell_type":"markdown","source":"### Base-Line Model: Seq-seq with Attention","metadata":{"id":"UarmKLUIVkjC"}},{"cell_type":"markdown","source":"#### Creating `tf_train_dataset`, `tf_val_dataset` using tf.data API\n\nNow, I need write the `generate_example` function that can help me generate model inputs for training, validation and testing set. For different type of dataset, we will create different generator with the help of the `example_generator` method to create `tf.data.Dataset` object for our model.\n\nWe can use `tf.data` API to create the input data pipeline for our model. I will use the `tf.data.Dataset` class to get the the examples from the `train_example_generator` function which uses `generate_example`, we can save the generator inside `example_gen` which we can iterate over later to get the examples. We can yield the examples according to the need of the problem.\n\nRemember, along with input article tokens and input summary tokens, we need the initial states as an input to the model. So, as we process the examples we can create this zero-value tensors and yield them along with 2 original inputs.\n\nFor more about datasets from generator, refer to [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator).","metadata":{"id":"usV8UchPA35e"}},{"cell_type":"markdown","source":"##### `generate_example_v1` function creation\nCreate a Geneator function that generates the source and target for training the model.","metadata":{"id":"ZUtzIfXm6UnX"}},{"cell_type":"code","source":"def generate_example_v1(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n    '''Generates examples for the model. Processes the `inputs` and `targets` with their respective\n    tokenizers and tokenize them to `input_len` and `target_len` length.\n\n    Arguments:\n    inputs: list of input sentences\n    targets: list of target sentences\n    input_tokenizer: Tokenizer class object, tokenizer for inputs\n    target_tokenizer: Tokenizer class object, tokenizer for targets\n    input_len: int, the length of the tokenization for inputs\n    target_len: int, the length of the tokenization for targets\n\n    Returns:\n    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n    another numpy array target_tokens[1:]\n    '''\n\n    for inp, tar in zip(inputs, targets):\n        # Tokenizing article words\n        inp_tokens = tokenize_pad([inp],\n                                  input_tokenizer,\n                                  padding=\"post\",\n                                  truncating=\"post\",\n                                  maxlen=input_len)\n\n        # Tokenizing summary words\n        tar_tokens = tokenize_pad([tar],\n                     target_tokenizer,\n                     padding=\"post\",\n                     truncating=\"post\",\n                     maxlen=target_len)\n\n        yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]","metadata":{"id":"8647M-ey6n19","outputId":"0783fcd2-3e99-45dc-cff0-102a8f9572f5","execution":{"iopub.status.busy":"2023-08-18T09:37:27.402462Z","iopub.execute_input":"2023-08-18T09:37:27.403176Z","iopub.status.idle":"2023-08-18T09:37:27.416380Z","shell.execute_reply.started":"2023-08-18T09:37:27.403134Z","shell.execute_reply":"2023-08-18T09:37:27.415523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Example with the generator function","metadata":{"id":"-nytCw0dFO2Y"}},{"cell_type":"code","source":"print(f\"Example generated by the generator v1:\")\n\n# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\nexample_gen = generate_example_v1(list(train_dataset[:, 0]),\n                               list(train_dataset[:, 1]),\n                               input_tokenizer=tokenizer,\n                               target_tokenizer=tokenizer,\n                               input_len=MAX_ARTICLE_TOKENS,\n                               target_len=MAX_SUMMARY_TOKENS)\n\ninps, tar = next(example_gen)\nprint(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\nprint(f\"Target:\\n{tar}\")","metadata":{"id":"BKKyIeVa5B20","outputId":"f424eca8-5f29-4dd0-915b-7f5e362399ec","execution":{"iopub.status.busy":"2023-08-18T09:37:27.417967Z","iopub.execute_input":"2023-08-18T09:37:27.418713Z","iopub.status.idle":"2023-08-18T09:37:27.503510Z","shell.execute_reply.started":"2023-08-18T09:37:27.418672Z","shell.execute_reply":"2023-08-18T09:37:27.502280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shapes of the Inputs:\\n{inps[0].shape}\\n{inps[1].shape}\\n\\n\")\nprint(f\"Shape of the Target:\\n{tar.shape}\")","metadata":{"id":"E-w0hmecDcho","outputId":"dc67edf1-88c5-4f30-935e-e3ea04f8b0f2","execution":{"iopub.status.busy":"2023-08-18T09:37:27.505520Z","iopub.execute_input":"2023-08-18T09:37:27.506011Z","iopub.status.idle":"2023-08-18T09:37:27.514929Z","shell.execute_reply.started":"2023-08-18T09:37:27.505956Z","shell.execute_reply":"2023-08-18T09:37:27.513822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Data Type of the Inputs data:\\n{inps[0].dtype}\\n{inps[1].dtype}\\n\\n\")\nprint(f\"Data Type of the Target data:\\n{tar.dtype}\")","metadata":{"id":"suZmEEhfDvcu","outputId":"0e2da90a-79b0-4b89-a5da-5b28d899609c","execution":{"iopub.status.busy":"2023-08-18T09:37:27.516463Z","iopub.execute_input":"2023-08-18T09:37:27.516866Z","iopub.status.idle":"2023-08-18T09:37:27.533503Z","shell.execute_reply.started":"2023-08-18T09:37:27.516835Z","shell.execute_reply":"2023-08-18T09:37:27.532368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inps, tar = next(example_gen)\nprint(f\"Second Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\nprint(f\"Second Target:\\n{tar}\")","metadata":{"id":"82m4io2PSyt3","outputId":"d95bc6aa-4364-4c1f-c3de-8b0a11ce5f25","execution":{"iopub.status.busy":"2023-08-18T09:37:27.534756Z","iopub.execute_input":"2023-08-18T09:37:27.535172Z","iopub.status.idle":"2023-08-18T09:37:27.553354Z","shell.execute_reply.started":"2023-08-18T09:37:27.535130Z","shell.execute_reply":"2023-08-18T09:37:27.552141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Making training, validation set examples for baseline model","metadata":{"id":"9m3U7a0pFULD"}},{"cell_type":"code","source":"def train_example_generetor_v1():\n    example_gen = generate_example_v1(list(train_dataset[:, 0]),\n                                list(train_dataset[:, 1]),\n                                input_tokenizer=tokenizer,\n                                target_tokenizer=tokenizer,\n                                input_len=MAX_ARTICLE_TOKENS,\n                                target_len=MAX_SUMMARY_TOKENS)\n\n    for example in example_gen:\n        s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n        c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n\n        (input_0, input_1), target = example\n        yield (input_0, input_1, s0, c0), target","metadata":{"id":"J_0r7ajRR8wk","outputId":"367e274d-63e4-4e55-ae53-a7a96011f899","execution":{"iopub.status.busy":"2023-08-18T09:37:27.554682Z","iopub.execute_input":"2023-08-18T09:37:27.555213Z","iopub.status.idle":"2023-08-18T09:37:27.594413Z","shell.execute_reply.started":"2023-08-18T09:37:27.555181Z","shell.execute_reply":"2023-08-18T09:37:27.593373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_signature = (\n    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n)\n\ntf_train_dataset = tf.data.Dataset.from_generator(generator=train_example_generetor_v1,\n                                                  output_signature=output_signature)\ntf_train_dataset = tf_train_dataset.shuffle(BUFFER_SIZE)\ntf_train_dataset = tf_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntf_train_dataset = tf_train_dataset.prefetch(1)","metadata":{"id":"Zj-H_fEh5Zu9","outputId":"524e02fb-448c-4b19-b09f-4d81b72f57a2","execution":{"iopub.status.busy":"2023-08-18T09:37:27.596828Z","iopub.execute_input":"2023-08-18T09:37:27.597340Z","iopub.status.idle":"2023-08-18T09:37:27.852509Z","shell.execute_reply.started":"2023-08-18T09:37:27.597257Z","shell.execute_reply":"2023-08-18T09:37:27.851314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_example_generetor_v1():\n    example_gen = generate_example_v1(list(val_dataset[:, 0]),\n                                list(val_dataset[:, 1]),\n                                input_tokenizer=tokenizer,\n                                target_tokenizer=tokenizer,\n                                input_len=MAX_ARTICLE_TOKENS,\n                                target_len=MAX_SUMMARY_TOKENS)\n\n    for example in example_gen:\n        s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n        c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n\n        (input_0, input_1), target = example\n        yield (input_0, input_1, s0, c0), target\n\n\noutput_signature = (\n    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n)\n\ntf_val_dataset = tf.data.Dataset.from_generator(generator=val_example_generetor_v1,\n                                                  output_signature=output_signature)\ntf_val_dataset = tf_val_dataset.shuffle(BUFFER_SIZE)\ntf_val_dataset = tf_val_dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"id":"-2qJcIbjRZXd","outputId":"89e83c15-798b-4136-818d-bac52016ad60","execution":{"iopub.status.busy":"2023-08-18T09:37:27.853893Z","iopub.execute_input":"2023-08-18T09:37:27.854245Z","iopub.status.idle":"2023-08-18T09:37:27.900216Z","shell.execute_reply.started":"2023-08-18T09:37:27.854200Z","shell.execute_reply":"2023-08-18T09:37:27.899290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (art_inp, sum_inp, s0, c0), sum_tar in tf_train_dataset.take(1):\n    print(f\"Input tokenized article shape: {art_inp.shape}\")\n    print(f\"Input tokenized summary shape: {sum_inp.shape}\\n\")\n\n    print(f\"Target tokenized summary shape: {sum_tar.shape}\")","metadata":{"id":"no4oMl22Fm6r","outputId":"d3c68da1-cb08-4c1b-8e09-b769ccfcbb97","execution":{"iopub.status.busy":"2023-08-18T09:37:27.901579Z","iopub.execute_input":"2023-08-18T09:37:27.902108Z","iopub.status.idle":"2023-08-18T09:37:42.549532Z","shell.execute_reply.started":"2023-08-18T09:37:27.902077Z","shell.execute_reply":"2023-08-18T09:37:42.548337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A small demonstration of how Dot layer works","metadata":{"id":"-2S_Jxhf25-g"}},{"cell_type":"markdown","source":"#### Attention Mechanism","metadata":{"id":"BVsZ_DeVmaie"}},{"cell_type":"code","source":"def one_time_attention_v1(a, s_prev,\n                       repeater, concatenator, densor_1, densor_2, softmax_layer, dotter):\n    '''Calculates the attention score and returns the context for the current timestep in the decoder.\n    Attention mechanism uses encoder outputs `a` of shape `(batch, timesteps, features)` and decoder\n    previous hidden state `s_prev` of shape `(batch, features)`, then calculates alignment scores `alphas`\n    for each encoder timestep with the help of energies computed with 2 dense layers using `a` and `s_prev`.\n\n    Arguments:\n    a: tf.Tensor object, encoder output of shape `(batch, timesteps, features)` or `(batch, Tx, 2*n_a)`\n    s_prev: tf.Tensor object, decoder previous hidden state of shape `(batch, features)` or `(batch, n_s)`\n    repeater: RepeatVector layer, repeat the `s_prev` `Tx` times\n    concatenator: Concatenate layer, concatenates `a` and repeated `s_prev`, Concatenates along axis=-1\n    densor_1: Dense layer, calculates the pertial energies `e`, with `units=d1_units`\n    refer to `baseline_model` function for details about this variable\n    densor_2: Dense layer, calculated the energies `energies`, with `units=d2_units`\n    refer to `baseline_model` function for details about this variable\n    softmax_layer: Activation layer, computes softmax of the energies and calculates `alphas`, with\n    `units=article_vocab_size` refer to `baseline_model` function for details about this variable\n    dotter: Dot layer, Performs dot operation between `alphas` and `a` along axis=1\n\n    Returns:\n    returns the context of shape `(batch, features)`\n    '''\n\n    # Repeat the `s_prev` `Tx` times\n    s_prev = repeater(s_prev) # (batch, Tx, n_s)\n\n    # Concatenate `a` and `s_prev` along axis=-1\n    concat = concatenator([a, s_prev]) # (batch, Tx, n_a + n_s)\n\n    # Apply dense layer to get partial energies e\n    e = densor_1(concat) # (batch, Tx, d1_units)\n\n    # Apply dense layer again to get energies\n    energies = densor_2(e) # (batch, Tx, d2_units)\n\n    # Apply softmax over the energies\n    alphas = softmax_layer(energies) # (batch, Tx, d2_units)\n\n    # Dot the alphas and a along axes=1\n    context = dotter([alphas, a]) # (batch, d2_units, 2*n_a)\n\n    return context","metadata":{"id":"B_rPlnWardTC","outputId":"2b856755-06a5-47b7-90fb-30f2001b4690","execution":{"iopub.status.busy":"2023-08-18T09:37:42.551580Z","iopub.execute_input":"2023-08-18T09:37:42.552201Z","iopub.status.idle":"2023-08-18T09:37:42.563684Z","shell.execute_reply.started":"2023-08-18T09:37:42.552150Z","shell.execute_reply":"2023-08-18T09:37:42.562411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoder-decoder Model using Attention mechanism","metadata":{"id":"QxkE_-Ifn2k3"}},{"cell_type":"code","source":"def baseline_model(Tx, Ty,\n                   emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n                   article_vocab_size, summary_vocab_size):\n    '''This implements the bas-line model archietecture for summarization.\n    It is a seq-seq model with attention mechanism implemented in it. The encoder take an input\n    with `Tx` time-steps and summarizes with the help of decoder into Ty words. The encoder and decoder\n    hidden states are `n_a` and `n_s` dimension respectively. The words are taken from the vocabulary of\n    article and summary `article_vocab` and `summary_vocab` with size `article_vocab_size` and\n    `summary_vocab_size` respectively.\n\n    Arguments:\n    Tx: int, length of the input article\n    Ty: int, length of the output summary\n    n_a: int, dimension of the encoder hidden states\n    n_s: int, dimension of the deocder hidden states\n    d1_units: int, units for the first dense layer in attention mechanism\n    d2_units: int, units for the second dense layer in attention mechanism\n    d_units: int, units for the dense layer before output layer\n    article_vocab_size: int, length of the article vocabulary\n    summary_vocab_size: int, length of the summary vocabulary\n\n    Returns:\n    returns the base line model\n    '''\n    # Defining the input for our model with shape (None, Tx) and (None, Ty) for encoder input and decoder input\n    X_inp = Input(shape=(Tx))\n    X_tar = Input(shape=(Ty))\n\n    # Initialize s0\n    s0 = Input(shape=(n_s, ), name=\"s0\")\n    # Initialize c0\n    c0 = Input(shape=(n_s, ), name=\"c0\")\n\n    # Initialize the a and s with a0 and s0\n    s = s0 # (batch, n_s)\n    c = c0 # (batch, n_s)\n\n    # Define the outputs as empty list\n    outputs = []\n\n    # First embedding layer for the article input\n    encoder_inp = Embedding(article_vocab_size, emb_dim)(X_inp) # (batch, Tx, emb_dim)\n\n    # Encoder: Bidirectional layer with LSTM cells\n    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(encoder_inp) # (batch, Tx, n_a)\n\n    # Define the embedding for decoder\n    decoder_inp = Embedding(summary_vocab_size, emb_dim)(X_tar) # (batch, Ty, emb_dim)\n\n    # Define the layers for Attention so that we can use the same weights for all decoder timesteps\n    repeater = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    attn_densor1 = Dense(units=d1_units, activation='tanh')\n    attn_densor2 = Dense(units=d2_units, activation='linear', use_bias=False)\n    softmax_layer = Activation('softmax', name=\"attention_weights\")\n    dotter = Dot(axes=1)\n\n    # Define the Decoder unidirectional LSTM for shared weights\n    post_attention_lstm = LSTM(units=n_s, return_state=True)\n\n    # Define the last dense layer before output layer with linear activation\n    densor = Dense(units=d_units, activation='linear')\n\n    # Define the output layer so that it does not initalize again and again for shared weights\n    output_layer = Dense(units=summary_vocab_size, activation='softmax')\n\n    # Decoder: Appends outputs from the output layer in each timestep\n    for t in range(Ty):\n        # Get the decoder input for current timestep\n        curr_dec_in = decoder_inp[:, t:t+1, :] # (batch, 1, emb_dim)\n\n        # Get the context from the attention mechanism\n        context = one_time_attention_v1(a, s, # (batch, d2_units, 2*n_a)\n                                     repeater, concatenator, attn_densor1, attn_densor2, softmax_layer, dotter)\n\n        concat = Concatenate(axis=-1)([curr_dec_in, context]) # (batch, d2_units, emb_dim+2*n_a); d2_units=1 otherwise error\n        _, s, c = post_attention_lstm(concat, initial_state=[s, c]) # _, (batch, n_s), (batch, n_s)\n\n        # Calculate the output after using 2 linear dense layers\n        out = densor(s) # (batch, d_units)\n        out = densor(out) # (batch, d_units)\n        # Use the output_layer to get the output\n        out  = output_layer(out) # (batch, summary_vocab_size)\n\n        # Append the final output to the outputs list\n        outputs.append(out)\n\n    # Stack the list of each timesteps output along axis=1\n    outputs = tf.stack(outputs, axis=1) # (batch, Ty, summary_vocab_size)\n\n    model = Model(inputs=[X_inp, X_tar, s0, c0], outputs=outputs)\n\n    return model","metadata":{"id":"mudpzB78HDvZ","outputId":"2fb0d213-077a-45c1-ada0-1648edd12e2d","execution":{"iopub.status.busy":"2023-08-18T09:37:42.566015Z","iopub.execute_input":"2023-08-18T09:37:42.566586Z","iopub.status.idle":"2023-08-18T09:37:42.587713Z","shell.execute_reply.started":"2023-08-18T09:37:42.566522Z","shell.execute_reply":"2023-08-18T09:37:42.586653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset states generated by Keras\ntf.keras.backend.clear_session()","metadata":{"id":"APJAr_NwcRRj","outputId":"d41ea3e1-367f-462a-f038-33ffdb99bc6b","execution":{"iopub.status.busy":"2023-08-18T09:37:42.589594Z","iopub.execute_input":"2023-08-18T09:37:42.593710Z","iopub.status.idle":"2023-08-18T09:37:42.611377Z","shell.execute_reply.started":"2023-08-18T09:37:42.593669Z","shell.execute_reply":"2023-08-18T09:37:42.610214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline Model Creation and Training","metadata":{"id":"moztj-HfDOls"}},{"cell_type":"code","source":"Tx = MAX_ARTICLE_TOKENS\nTy = MAX_SUMMARY_TOKENS - 1\nemb_dim = EMB_OUT\nn_a = ENCODER_STATE_DIM\nn_s= DECODER_STATE_DIM\nd1_units = DENSE1_UNITS\nd2_units = DENSE2_UNITS\nd_units = DENSE_UNITS\narticle_vocab_size = VOCAB_SIZE\nsummary_vocab_size = VOCAB_SIZE\n\nbase_model = baseline_model(Tx, Ty,\n                       emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n                       article_vocab_size, summary_vocab_size)","metadata":{"id":"V3ZAtG5I8mZN","execution":{"iopub.status.busy":"2023-08-18T09:37:42.613434Z","iopub.execute_input":"2023-08-18T09:37:42.613848Z","iopub.status.idle":"2023-08-18T09:38:21.062046Z","shell.execute_reply.started":"2023-08-18T09:37:42.613815Z","shell.execute_reply":"2023-08-18T09:38:21.060982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model has {base_model.count_params():,} parameters.\")","metadata":{"id":"2eMakRHeZzrF","outputId":"08d0700c-d496-412d-fff3-456f4d3e5dd0","execution":{"iopub.status.busy":"2023-08-18T09:38:21.063696Z","iopub.execute_input":"2023-08-18T09:38:21.064011Z","iopub.status.idle":"2023-08-18T09:38:21.075495Z","shell.execute_reply.started":"2023-08-18T09:38:21.063984Z","shell.execute_reply":"2023-08-18T09:38:21.074409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look into how model will output on the above input.","metadata":{"id":"sN_nIO_-LdDp"}},{"cell_type":"code","source":"sample_model_out = base_model((art_inp, sum_inp, s0, c0))\n\nprint(f\"Model output has a type: {type(sample_model_out)}\")\nprint(f\"Model Output list for the Inputs above are of length: {len(sample_model_out)}\")\nprint(f\"Model Output list has each output of shape: {sample_model_out[0].shape}\")","metadata":{"id":"xzqmZfKoLcfJ","outputId":"32287225-4fc5-4b3a-dc15-8b2c005978ab","execution":{"iopub.status.busy":"2023-08-18T09:38:21.077324Z","iopub.execute_input":"2023-08-18T09:38:21.078230Z","iopub.status.idle":"2023-08-18T09:38:30.152565Z","shell.execute_reply.started":"2023-08-18T09:38:21.078190Z","shell.execute_reply":"2023-08-18T09:38:30.151416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating Custom Loss and Accuracy Version 1","metadata":{"id":"C62CukniFhmx"}},{"cell_type":"code","source":"def custom_loss_v1(y_true, y_pred):\n    '''Calculates the loss for the baseline model. The loss is calculated by taking the negative\n    log-likelihood of the target word(w*_t) in the current timestep. Then the overall loss\n    is the summation over all timesteps divided by T (not Ty because it would include paddings also).\n\n    Arguments:\n    y_true: tf.Tensor object, true values for the target\n    y_pred: list of tf.Tensor objects, predicted probablities of the summary words\n\n    Returns:\n    returns the loss on the predicted values for the model\n    '''\n    # Calculate the loss for each item in the batch.\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n    loss = loss_fn(y_true, y_pred)\n\n    # Remove the paddings from calculation of loss\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    # Divide the total loss after masking out paddings divided by total words which are not paddings\n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\ndef custom_accuracy_v1(y_true, y_pred):\n    '''Calculates accuracy of the baseline model. The accuracy is calculated by matching how many correct\n    words were predicted excluding the paddings. Then, just add those which are correct and you will get the\n    the accuracy and then just divide it by total words not including padding.\n\n    Arguments:\n    y_true: tf.Tensor object, expected target values\n    y_pred: list of tf.Tensor object, predicted target values by model\n\n    Returns:\n    returns the total accuracy over the batch of data\n    '''\n    # Find the word index with maximum probablity\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    # Count the words that matches with true values\n    match = tf.cast(y_pred == y_true, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n\n    # Mask out the paddings\n    match *= mask\n\n    return tf.reduce_sum(match) / tf.reduce_sum(mask)","metadata":{"id":"dw-z3KccJdpj","outputId":"194cd324-9fd0-44e9-8aa1-e8a4f3b73e50","execution":{"iopub.status.busy":"2023-08-18T09:38:30.154136Z","iopub.execute_input":"2023-08-18T09:38:30.154484Z","iopub.status.idle":"2023-08-18T09:38:30.166453Z","shell.execute_reply.started":"2023-08-18T09:38:30.154453Z","shell.execute_reply":"2023-08-18T09:38:30.165507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Testing with loss and accuracy","metadata":{"id":"BR02DedPFrDk"}},{"cell_type":"code","source":"print(f\"Sample y true values: {sum_tar}\")\nprint(f\"Sample y pred values(first 10 values of first 2 timestep): {sample_model_out[:2]}\")\n\nsample_loss = custom_loss_v1(sum_tar, sample_model_out)\nprint(f\"Loss of the sample y_true and y_pred: {sample_loss}\")\n\nsample_acc = custom_accuracy_v1(sum_tar, sample_model_out)\nprint(f\"Accuracy of the sample y_true and y_pred: {sample_acc}\")","metadata":{"id":"SR25pZHrACmA","outputId":"ee0770bc-19c8-4c74-d6cd-be5a51114c64","execution":{"iopub.status.busy":"2023-08-18T09:38:30.168206Z","iopub.execute_input":"2023-08-18T09:38:30.168801Z","iopub.status.idle":"2023-08-18T09:38:31.464456Z","shell.execute_reply.started":"2023-08-18T09:38:30.168667Z","shell.execute_reply":"2023-08-18T09:38:31.463372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Compiling base line model","metadata":{"id":"cs0e3XjqFwdR"}},{"cell_type":"code","source":"lr = LEARNING_RATE\ninitial_accumulator_value = INIT_ACC_VAL\nclipnorm = MAX_GRAD_NORM\n\nopt = Adagrad(learning_rate=lr,\n              initial_accumulator_value=initial_accumulator_value,\n              clipnorm=clipnorm)\n\nbase_model.compile(loss=custom_loss_v1, optimizer=opt, metrics=[custom_loss_v1, custom_accuracy_v1])","metadata":{"id":"bOTfC4BNnFgj","outputId":"48f21c46-7ca9-461a-c987-c53e1689dae8","execution":{"iopub.status.busy":"2023-08-18T09:41:24.534450Z","iopub.execute_input":"2023-08-18T09:41:24.534833Z","iopub.status.idle":"2023-08-18T09:41:24.992874Z","shell.execute_reply.started":"2023-08-18T09:41:24.534800Z","shell.execute_reply":"2023-08-18T09:41:24.991476Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating callbacks for model","metadata":{"id":"_P1vbkDSF0T3"}},{"cell_type":"code","source":"# Mention the checkpoint path and it's directory where you will save the model\ncheckpoint_path = BASELINE_MODEL_CHECKPOINT\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Calculate no of batches, I am taking floor because when creating the training data I used drop_remainder\nn_batches = int(train_dataset.shape[0] / BATCH_SIZE)\n\n# Create the checkpoint for model saving, monitoring val_custom_accuracy_v1 and save only weights of the model\nsaving_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                monitor='val_custom_accuracy_v1',\n                                                verbose=1,\n                                                save_weights_only=True,\n                                                save_freq=n_batches//2)\n\n# Create the checkpoint for stopping early after noticing that val_custom_accuracy_v1 is not increasing even after 5 consecutive epochs\nearlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_custom_accuracy_v1',\n                                                    patience=PATIENCE,\n                                                    mode='max',\n                                                    )\n\n# Store the checkpoints in a list\ncallbacks = [saving_cb, earlystop_cb]","metadata":{"id":"_OFR6b5hD10E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Training baseline model","metadata":{"id":"tLyxWD4MF6W-"}},{"cell_type":"code","source":"epochs = EPOCHS\nsteps_per_epoch = STEPS_PER_EPOCHS\n\nhistory = base_model.fit(tf_train_dataset.repeat(),\n                    epochs=epochs,\n                    validation_data=tf_val_dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    callbacks=callbacks\n                    )","metadata":{"id":"OQE9gr_Bd9cJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pointer Generetor model: Adding Pointer Generator to avoid getting OOV tokens in the summary","metadata":{"id":"cDBfIISTS0j3"}},{"cell_type":"markdown","source":"#### Creating `tf_train_dataset`, `tf_val_dataset` dataset using tf.data API\n\nI already have created to the `generate_example` function, I can use it again to generate examples for training and validation respectively.\n\nBut this time I have to also consider the `<OOV>` with token value `1`. Instead of using `1`, I have to generate a new token value for each of the `<OOV>` words.\n\nHow to do it?","metadata":{"id":"0Y9oJUdnUM_Z"}},{"cell_type":"markdown","source":"##### `find_oovs`, `map_oovs` & `tokenize_oovs` function","metadata":{"id":"nJY4-VAaEfuF"}},{"cell_type":"code","source":"def find_oovs(sent, sent_token, sent_len):\n    '''Finds the out of vocabulary words from `sent` with the help of already tokenized `sent_tokens`.\n\n    Arguments:\n    sent: str, sentence to find the oov from\n    sent_token: 2D np.array, the tokenized form of the sentence with sent_len length\n    sent_len: int, length of the tokenized sentence\n\n    Returns:\n    Returns a list of all oov words in the `sent`\n    '''\n    analyzed_sent = custom_analyzer(sent)\n    oov_words = [w for i, w in enumerate(analyzed_sent[:sent_len]) if (sent_token[0][i] == 1)]\n    return oov_words\n\ndef map_oovs(oovs, oov_start_token):\n    '''Stores the out of vocabulary words in a dictionary and sets the values of each oov key to\n    a temporary unique tokens.\n\n    Arguments:\n    oovs: list of oov words\n    oov_start_token: int, the first value to use as oov token then increase by 1\n\n    Returns:\n    dictionary of (oov, token) as (key, value) pairs\n    '''\n    unique_oovs = list(set(oovs))\n    oov_tokens = [oov_start_token+i for i in range(len(unique_oovs))]\n\n    oov_dict = dict(zip(unique_oovs, oov_tokens))\n\n    return oov_dict\n\ndef tokenize_oovs(sent, sent_token, oov_dict, sent_len):\n    '''Tokenize the sent by replacing the oov tokens by new unique tokens from oov_dict.\n\n    Arguments:\n    sent: str, sentence to handle the oovs\n    sent_token: 2D np.array of tokens\n    oov_dict: dictionary, oov words and their tokens are stored here\n    sent_len: int, length of the sentence token array\n\n    Returns:\n    tokenized sentence with oov words tokenized to temporary oov tokens\n    '''\n    analyzed_sent = custom_analyzer(sent)\n\n    for i, w in enumerate(analyzed_sent[:sent_len]):\n        if w in oov_dict.keys():\n            sent_token[0, i] = oov_dict[w]\n\n    return sent_token","metadata":{"id":"WkfC7nfwghMI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Applying functions on Examples","metadata":{"id":"MygLMzLXEouR"}},{"cell_type":"code","source":"print(f\"Article:\\n{sample_article}\\n\\n\")\n\nsample_article_tokens = tokenize_pad([sample_article],\n                              tokenizer,\n                              padding=\"post\",\n                              truncating=\"post\",\n                              maxlen=MAX_ARTICLE_TOKENS)\nprint(f\"sample article tokens:\\n{sample_article_tokens}\\n\\n\")\n\nsample_oovs = find_oovs(sample_article, sample_article_tokens, MAX_ARTICLE_TOKENS)\nprint(f\"OOVs in the sample article:\\n{sample_oovs}\\n\\n\")\n\nsample_oov_dict = map_oovs(sample_oovs, VOCAB_SIZE)\nprint(f\"OOV dictionary:\\n{sample_oov_dict}\\n\\n\")\n\nsample_article_tokens_with_oovs = tokenize_oovs(sample_article, sample_article_tokens, sample_oov_dict, MAX_ARTICLE_TOKENS)\nprint(f\"sample article tokens with oov tokens:\\n{sample_article_tokens_with_oovs}\")","metadata":{"id":"cqzj5WV-ihFC","outputId":"9870ca93-d734-4d2d-bf84-bdd7e64fae10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### `generate_example_v2` function creation","metadata":{"id":"b4U6JUL9ExLh"}},{"cell_type":"code","source":"def generate_example_v2(inputs, targets,\n                        input_tokenizer, target_tokenizer,\n                        input_len, target_len,\n                        padding=\"post\", truncating=\"post\",\n                        vocab_size=VOCAB_SIZE):\n\n    '''Generates examples for the Pointer Generator model. Processes the `inputs` and `targets`\n    with their respective tokenizers and tokenize them to `input_len` and `target_len` length.\n    After tokenizing the article words, it looks for the out-of-vocabulary words and creates unique tokens\n    for each of those OOV words. Then, instead of keeping the oov word tokens as 1, it replaces them\n    with their respective newly generated tokens. This tokens are temporary\n\n    Arguments:\n    inputs: list of input sentences\n    targets: list of target sentences\n    input_tokenizer: Tokenizer class object, tokenizer for inputs\n    target_tokenizer: Tokenizer class object, tokenizer for targets\n    input_len: int, the length of the tokenization for inputs\n    target_len: int, the length of the tokenization for targets\n\n    Returns:\n    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n    another numpy array target_tokens[1:]\n    '''\n\n    for inp, tar in zip(inputs, targets):\n        # Tokenizing article words\n        inp_token = tokenize_pad([inp],\n                                  input_tokenizer,\n                                  padding=padding,\n                                  truncating=truncating,\n                                  maxlen=input_len)\n\n        oov_words = find_oovs(inp, inp_token, input_len)\n        oov_dict = map_oovs(oov_words, oov_start_token=vocab_size)\n        inp_token = tokenize_oovs(inp, inp_token, oov_dict, input_len)\n\n        # Tokenizing summary words\n        tar_token = tokenize_pad([tar],\n                     target_tokenizer,\n                     padding=padding,\n                     truncating=truncating,\n                     maxlen=target_len)\n        tar_token = tokenize_oovs(tar, tar_token, oov_dict, target_len)\n\n        yield (inp_token[0], tar_token[0][:-1]), tar_token[0][1:]","metadata":{"id":"bWzy1fmHTG_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Examples with the generator function","metadata":{"id":"g66B6U8bE3jA"}},{"cell_type":"code","source":"print(f\"Example generated by the generator v2:\")\n\n# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\nexample_gen = generate_example_v2(list(train_dataset[:, 0]),\n                               list(train_dataset[:, 1]),\n                               input_tokenizer=tokenizer,\n                               target_tokenizer=tokenizer,\n                               input_len=MAX_ARTICLE_TOKENS,\n                               target_len=MAX_SUMMARY_TOKENS)\n\ninps, tar = next(example_gen)\nprint(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\nprint(f\"Target:\\n{tar}\")","metadata":{"id":"eA3joMNfbcjL","outputId":"45e75c51-882f-4132-99de-f5ba713c43d7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inps, tar = next(example_gen)\nprint(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\nprint(f\"Target:\\n{tar}\")","metadata":{"id":"hIuAkQbO7Bzz","outputId":"37076516-319c-4750-a9f7-917e0ff937b0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Making training, validation set examples for pointer generator model","metadata":{"id":"Ix6RsiveGSQS"}},{"cell_type":"code","source":"def train_example_generetor_v2():\n    example_gen = generate_example_v2(list(train_dataset[:, 0]),\n                                list(train_dataset[:, 1]),\n                                input_tokenizer=tokenizer,\n                                target_tokenizer=tokenizer,\n                                input_len=MAX_ARTICLE_TOKENS,\n                                target_len=MAX_SUMMARY_TOKENS)\n\n    for example in example_gen:\n        s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n        c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n\n        (input_0, input_1), target = example\n        yield (input_0, input_1, s0, c0), target","metadata":{"id":"ZCRXKTLZGX_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_signature = (\n    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n)\n\ntf_train_dataset = tf.data.Dataset.from_generator(generator=train_example_generetor_v2,\n                                                  output_signature=output_signature)\ntf_train_dataset = tf_train_dataset.shuffle(BUFFER_SIZE)\ntf_train_dataset = tf_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntf_train_dataset = tf_train_dataset.prefetch(1)","metadata":{"id":"NBdNpR7rGw7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_example_generetor_v2():\n    example_gen = generate_example_v2(list(val_dataset[:, 0]),\n                                list(val_dataset[:, 1]),\n                                input_tokenizer=tokenizer,\n                                target_tokenizer=tokenizer,\n                                input_len=MAX_ARTICLE_TOKENS,\n                                target_len=MAX_SUMMARY_TOKENS)\n\n    for example in example_gen:\n        s0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n        c0 = np.zeros((DECODER_STATE_DIM, ), dtype=np.int32)\n\n        (input_0, input_1), target = example\n        yield (input_0, input_1, s0, c0), target\n\n\noutput_signature = (\n    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32),\n     tf.TensorSpec(shape=(DECODER_STATE_DIM, ), dtype=tf.int32)),\n    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n)\n\ntf_val_dataset = tf.data.Dataset.from_generator(generator=val_example_generetor_v2,\n                                                  output_signature=output_signature)\ntf_val_dataset = tf_val_dataset.shuffle(BUFFER_SIZE)\ntf_val_dataset = tf_val_dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"id":"byX6PXAKGw7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (art_inp, sum_inp, s0, c0), sum_tar in tf_train_dataset.take(1):\n    print(f\"Input tokenized article shape: {art_inp.shape}\")\n    print(f\"Input tokenized summary shape: {sum_inp.shape}\\n\")\n\n    print(f\"Target tokenized summary shape: {sum_tar.shape}\")","metadata":{"outputId":"e0bdb027-6b00-4229-ebe4-f55c93e2964d","id":"2fZlevYZGw7y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Attention Mechanism for Pointer Generator Model\n\nIt now involves the attention weights also because they will be used to calculate the final distribution.","metadata":{"id":"bokn5sDnsnEI"}},{"cell_type":"code","source":"def one_time_attention_v2(a, s_prev,\n                       repeater, concatenator, densor_1, densor_2, softmax_layer, dotter):\n    '''Calculates the attention score and returns the context and attention distribution for the current\n    timestep in the decoder.\n    Attention mechanism uses encoder outputs `a` of shape `(batch, timesteps, features)` and decoder\n    previous hidden state `s_prev` of shape `(batch, features)`, then calculates alignment scores `alphas`\n    for each encoder timestep with the help of energies computed with 2 dense layers using `a` and `s_prev`.\n\n    Arguments:\n    a: tf.Tensor object, encoder output of shape `(batch, timesteps, features)` or `(batch, Tx, 2*n_a)`\n    s_prev: tf.Tensor object, decoder previous hidden state of shape `(batch, features)` or `(batch, n_s)`\n    repeater: RepeatVector layer, repeat the `s_prev` `Tx` times\n    concatenator: Concatenate layer, concatenates `a` and repeated `s_prev`, Concatenates along axis=-1\n    densor_1: Dense layer, calculates the pertial energies `e`, with `units=d1_units`\n    refer to `baseline_model` function for details about this variable\n    densor_2: Dense layer, calculated the energies `energies`, with `units=d2_units`\n    refer to `baseline_model` function for details about this variable\n    softmax_layer: Activation layer, computes softmax of the energies and calculates `alphas`, with\n    `units=article_vocab_size` refer to `baseline_model` function for details about this variable\n    dotter: Dot layer, Performs dot operation between `alphas` and `a` along axis=1\n\n    Returns:\n    returns the context of shape `(batch, 1, 2*n_a)` and\n    attention distribution of shape `(batch, Tx, d2_units)`\n    '''\n\n    # Repeat the `s_prev` `Tx` times\n    s_prev = repeater(s_prev) # (batch, Tx, n_s)\n\n    # Concatenate `a` and `s_prev` along axis=-1\n    concat = concatenator([a, s_prev]) # (batch, Tx, n_a + n_s)\n\n    # Apply dense layer to get partial energies e\n    e = densor_1(concat) # (batch, Tx, d1_units)\n\n    # Apply dense layer again to get energies\n    energies = densor_2(e) # (batch, Tx, d2_units)\n\n    # Apply softmax over the energies\n    alphas = softmax_layer(energies) # (batch, Tx, d2_units)\n\n    # Dot the alphas and a along axes=1\n    context = dotter([alphas, a]) # (batch, d2_units, 2*n_a)\n\n    return context, alphas","metadata":{"id":"aDPDm6MJLIEz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pointer Generator\n\nThe function `generate_pointer` will return the $p_{gen}$. $p_{gen}$ will be used to decide whether the next word needs to be copied from the article or we should generate a new word from the vocabulary. This decision will be taken using the equation- $$P(w) = p_{gen}*P_{vocab}(w) + (1-p_{gen})*\\sum_{i:w_i=w}a^t_i$$\n\nWhere\n- $P$ is the extended vocabulary containing words from vocabulary and article both.\n- $a^t_i$ is the attention $t^{th}$ time step need to give to $i^{th}$ article word.\n- $P_{vocab}$ is the previous vocabulary distribution without considering words from article.","metadata":{"id":"j7wikoNOuKsd"}},{"cell_type":"markdown","source":"##### Pointer Generator Network\n\nPointer network takes the context vector ($h^{*(t)}$), decoder embedding input($x^{(t)}$) and decoder hidden state ($s^{(t)}$). The equation of the pointer network is - $$p_{gen} = \\sigma(W_h^Th^{*(t)}+W_x^Tx^{(t)}+W_s^Ts^{(t)}+b_{ptr})$$","metadata":{"id":"PlMlegsIEbsG"}},{"cell_type":"code","source":"def pointer_generator_v1(context_vector, decoder_state, decoder_inp):\n    '''Generates the p_gen needed to calculate the final vocabulary distribution.\n    It takes the context, decoder hidden state and decoder embedding input as it's input and then\n    passes them through a dense layer to get the pointer generator.\n\n    Arguments:\n    context_vector: Tensor of shape (batch, n_a)\n    decoder_state: Tensor of shape (batch, n_s)\n    decoder_inp: Tensor of shape (batch, emb_dim)\n\n    Returns:\n    returns the pointer generator p_gen\n    '''\n    concat = Concatenate(axis=-1)([context_vector, decoder_state, decoder_inp])\n    p_gen = tf.keras.layers.Dense(units=1, activation='sigmoid')(concat)\n\n    return p_gen","metadata":{"id":"P7y1utw3mAZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Pointer Generator Model","metadata":{"id":"AKSoa2YRENVV"}},{"cell_type":"markdown","source":"A demo on how to use `scatter_nd` method from tensorflow to map the copy probablities to the appropriate indices of final extended vocabulary distribution.\n\nThis example simulates the situation of already having encoder input(`x`), vocabulary distribution(`vocab_dist`) and copy probablities using pointer generator(`copy_dist`). Now, we just need to find the final extended vocabulary distribution `final_dist`.\n\nThe simulated example has `vocabulary size = 100`, `encoder input length is 10`, `batch size = 5`, `encoder input has 3 OOV words`.\n\nThe main logic here is- $$P_{final}[X[i]]+=P_{copy}[i]$$\nP.S we are not considering batch dimension.\n\nProgramming this concept in tensorflow:\n\n1. Convert the input tensor(`x`) to another 2D tensor(`x_ind`) with `shape=(Batch size * Input Length, 2)`, where $$x_ind[i*BatchSize+j] = [i, x[i][j]] where\\ iϵ[0, Batch Size-1], jϵ[0, Input Length-1]$$\n2. Flatten the `copy_dist`.\n3. Use `tf.scatter_nd` method.","metadata":{"id":"FWfZ0s_mwDn4"}},{"cell_type":"code","source":"x = tf.constant([[ 2, 100,  50, 10,  67,  16,  101,  23,  102,  3],\n                [ 2, 45, 100,  91,  99,  35,  101, 48,  102,  3],\n                [ 2,  100, 67,  95,  65,  101,  90,  102,  9,  3],\n                [ 2, 102, 100, 101,  90,  36,  96,  35,  98,  3],\n                [ 2,  91,  91,  16, 101, 100,  87,  94,  102,  3]], dtype=tf.int32)\nprint(f\"The simulated input to encoder:\\n{x}\")","metadata":{"id":"oqA-OnHrwC2r","outputId":"1b433749-abb2-4301-bdb0-04483c8da8c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_dist = tf.random.uniform(shape=[5, 100])\nprint(f\"The vocabulary distribution without OOV:\\n{vocab_dist}\")","metadata":{"id":"6HiLv8ody5A9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_dist = tf.random.uniform(shape=[5, 10])\nprint(f\"The copy distribution is:\\n{copy_dist}\")","metadata":{"id":"BQPY3-kL2nqk","outputId":"1fcbb37c-f1da-41b0-b0dd-77e5b1afe193"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_dist = tf.concat([vocab_dist, tf.zeros_like(x, dtype=float)], axis=-1)\nprint(f\"The shape of the final distribution is {final_dist.shape}\")","metadata":{"id":"Be5dQGAD210m","outputId":"2bd3f46f-2f61-461e-8c41-9e8ffa8c909a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows, cols = tf.meshgrid(tf.range(x.shape[0]), tf.range(x.shape[1]), indexing='ij')\nrows","metadata":{"id":"0ukZjfdY5ANG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_reshaped = tf.reshape(x, [-1])\nx_reshaped","metadata":{"id":"0gL7N4gIDJTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices_to_update = tf.stack([tf.reshape(rows, [-1]), x_reshaped], axis=1)\nindices_to_update","metadata":{"id":"nTeHuxIyDPzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.scatter_nd(indices=indices_to_update, updates=tf.reshape(copy_dist, [-1]), shape=final_dist.shape)","metadata":{"id":"wXu-uc6344Vs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at whether we can actually add the `copy_dist` to `final_dist`.\n\nWe are looking at the word, from first example input of the batch, which has token `50`. It is the 3rd word of the input `x[0, 2] = 50`.\n\nThis corresponds to `copy_word[0, 2]`, `final_dist[0, 50]`.","metadata":{"id":"LhCAQ46wEhvc"}},{"cell_type":"code","source":"final_dist[0, 50]","metadata":{"id":"GRWF5dCNEPiB","outputId":"add0ff60-9966-4a88-ef3a-4dd7ba02067e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_dist[0, 2]","metadata":{"id":"UhpXrTw_EbmR","outputId":"52b8fe11-dc0e-4277-eef0-039e38bdd1c2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_dist[0, 50]+copy_dist[0, 2]","metadata":{"id":"DoaOgSiAFv1z","outputId":"6dc31238-4919-439f-e103-6d343451fa8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices_to_update[2]","metadata":{"id":"uStJXJe2GRpN","outputId":"44d240c6-458a-4903-9a67-356aac5752cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.tensor_scatter_nd_add(tensor=final_dist, indices=indices_to_update, updates=tf.reshape(copy_dist, [-1]))[0, 50]","metadata":{"id":"Cq6KSsrgDtxv","outputId":"0ce919ce-da5f-4498-c89f-d26046ecbe0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_final_distribution(final_dist, copy_dist, inp_tokens):\n    rows_range = tf.range(tf.shape(inp_tokens)[0])\n    cols_range = tf.range(tf.shape(inp_tokens)[1])\n    rows, _ = tf.meshgrid(rows_range, cols_range, indexing='ij')\n\n    indices_to_update = tf.stack([tf.reshape(rows, [-1]), tf.reshape(inp_tokens, [-1])], axis=1)\n\n    return tf.tensor_scatter_nd_add(tensor=final_dist,\n                                  indices=indices_to_update,\n                                  updates=tf.reshape(copy_dist, [-1]))","metadata":{"id":"CzPfYicOG7sF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pointer_gen_model(Tx, Ty,\n                   emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n                   article_vocab_size, summary_vocab_size):\n    '''This implements the bas-line model archietecture for summarization.\n    It is a seq-seq model with attention mechanism implemented in it. The encoder take an input\n    with `Tx` time-steps and summarizes with the help of decoder into Ty words. The encoder and decoder\n    hidden states are `n_a` and `n_s` dimension respectively. The words are taken from the vocabulary of\n    article and summary `article_vocab` and `summary_vocab` with size `article_vocab_size` and\n    `summary_vocab_size` respectively.\n\n    Arguments:\n    Tx: int, length of the input article\n    Ty: int, length of the output summary\n    n_a: int, dimension of the encoder hidden states\n    n_s: int, dimension of the deocder hidden states\n    d1_units: int, units for the first dense layer in attention mechanism\n    d2_units: int, units for the second dense layer in attention mechanism\n    d_units: int, units for the dense layer before output layer\n    article_vocab_size: int, length of the article vocabulary\n    summary_vocab_size: int, length of the summary vocabulary\n\n    Returns:\n    returns the base line model\n    '''\n    # Defining the input for our model with shape (None, Tx) and (None, Ty) for encoder input and decoder input\n    X_inp = Input(shape=(Tx), dtype=tf.int32)\n    X_tar = Input(shape=(Ty), dtype=tf.int32)\n\n    # Initialize s0\n    s0 = Input(shape=(n_s, ), name=\"s0\")\n    # Initialize c0\n    c0 = Input(shape=(n_s, ), name=\"c0\")\n\n    # Initialize the a and s with a0 and s0\n    s = s0 # (batch, n_s)\n    c = c0 # (batch, n_s)\n\n    # Define the outputs as empty list\n    outputs = []\n\n    # First embedding layer for the article input\n    encoder_inp = Embedding(article_vocab_size+Tx, emb_dim)(X_inp) # (batch, Tx, emb_dim)\n\n    # Encoder: Bidirectional layer with LSTM cells\n    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(encoder_inp) # (batch, Tx, n_a)\n\n    # Define the embedding for decoder\n    decoder_inp = Embedding(summary_vocab_size+Tx, emb_dim)(X_tar) # (batch, Ty, emb_dim)\n\n    # Define the layers for Attention so that we can use the same weights for all decoder timesteps\n    repeater = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    attn_densor1 = Dense(units=d1_units, activation='tanh')\n    attn_densor2 = Dense(units=d2_units, activation='linear', use_bias=False)\n    softmax_layer = Activation('softmax', name=\"attention_weights\")\n    dotter = Dot(axes=1)\n\n    # Define the Decoder unidirectional LSTM for shared weights\n    post_attention_lstm = LSTM(units=n_s, return_state=True)\n\n    # Define the last dense layer before output layer with linear activation\n    densor = Dense(units=d_units, activation='linear')\n\n    # Define the output layer so that it does not initalize again and again for shared weights\n    output_layer = Dense(units=summary_vocab_size, activation='softmax')\n\n    # Initialize the extension\n    extension = tf.zeros_like(X_inp, dtype=float)\n\n    # Decoder: Appends outputs from the output layer in each timestep\n    for t in range(Ty):\n        # Get the decoder input for current timestep\n        curr_dec_in = decoder_inp[:, t:t+1, :] # (batch, 1, emb_dim)\n\n        # Get the context from the attention mechanism\n        context, attn_dist = one_time_attention_v2(a, s, # (batch, d2_units, 2*n_a), (batch, Tx, d2_units)\n                                     repeater, concatenator, attn_densor1, attn_densor2, softmax_layer, dotter)\n\n        concat = Concatenate(axis=-1)([curr_dec_in, context]) # (batch, d2_units, emb_dim+2*n_a); d2_units=1 otherwise error\n        _, s, c = post_attention_lstm(concat, initial_state=[s, c]) # _, (batch, n_s), (batch, n_s)\n\n        # Calculate the output after using 2 linear dense layers\n        den1 = densor(s) # (batch, d_units)\n        den2 = densor(den1) # (batch, d_units)\n        # Use the output_layer to get the vocabulary distribution\n        P_vocab  = output_layer(den2) # (batch, summary_vocab_size)\n\n        # Generate pointer\n        p_gen = pointer_generator_v1(context[:, 0, :], s, curr_dec_in[:, 0, :]) # (batch, 1)\n\n        # Calculate the total probablity of copying words from source text\n        P_copy = (1 - p_gen) * attn_dist[:, :, 0] # (batch, Tx)\n\n        # Calculate the probablity to generate new word\n        P_final = p_gen * P_vocab # (batch, summary_vocab_size)\n\n        # Extend the final vocabulary to take new temporary words from source text\n        P_final = Concatenate(axis=-1)([P_final, extension]) # (batch, summary_vocab_size+Tx)\n\n        # Calculate the extended vocabulary distribution\n        P_final = calculate_final_distribution(P_final, P_copy, X_inp)\n\n        # Append the final output to the outputs list\n        outputs.append(P_final)\n\n    # Stack the list of each timesteps output along axis=1\n    outputs = tf.stack(outputs, axis=1) # (batch, Ty, summary_vocab_size)\n\n    model = Model(inputs=[X_inp, X_tar, s0, c0], outputs=outputs)\n\n    return model","metadata":{"id":"hLrsABvkICua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pointer Generator Model Creation and Training","metadata":{"id":"l5lRYuUrDih7"}},{"cell_type":"code","source":"Tx = MAX_ARTICLE_TOKENS\nTy = MAX_SUMMARY_TOKENS - 1\nemb_dim = EMB_OUT\nn_a = ENCODER_STATE_DIM\nn_s= DECODER_STATE_DIM\nd1_units = DENSE1_UNITS\nd2_units = DENSE2_UNITS\nd_units = DENSE_UNITS\narticle_vocab_size = VOCAB_SIZE\nsummary_vocab_size = VOCAB_SIZE\n\npointer_model = pointer_gen_model(Tx, Ty,\n                          emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n                          article_vocab_size, summary_vocab_size)","metadata":{"id":"n7-mN-LfD69k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model has {pointer_model.count_params():,} parameters.\")","metadata":{"id":"AkrDZxHbIjFE","outputId":"21ab711c-7c82-4a2f-f55f-28dbe5340521"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look into how model will output on the above input.","metadata":{"id":"4pJCgf-3IwUW"}},{"cell_type":"code","source":"sample_model_out = pointer_model((art_inp, sum_inp, s0, c0))\n\nprint(f\"Model output has a type: {type(sample_model_out)}\")\nprint(f\"Model Output list for the Inputs above are of length: {len(sample_model_out)}\")\nprint(f\"Model Output list has each output of shape: {sample_model_out[0].shape}\")","metadata":{"outputId":"0b6914cd-aac6-494d-9aa1-0f1a780ee600","id":"jl_ewDpoIwUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Custom Loss and Accuracy Version 2","metadata":{"id":"7Vgp0zJ0EDkq"}},{"cell_type":"code","source":"def custom_loss_v2(y_true, y_pred):\n    '''Calculates the loss for the baseline model. The loss is calculated by taking the negative\n    log-likelihood of the target word(w*_t) in the current timestep. Then the overall loss\n    is the summation over all timesteps divided by T (not Ty because it would include paddings also).\n\n    Arguments:\n    y_true: tf.Tensor object, true values for the target\n    y_pred: list of tf.Tensor objects, predicted probablities of the summary words\n\n    Returns:\n    returns the loss on the predicted values for the model\n    '''\n    # Calculate the loss for each item in the batch.\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n    loss = loss_fn(y_true, y_pred)\n\n    # Remove the paddings from calculation of loss\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    # Divide the total loss after masking out paddings divided by total words which are not paddings\n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\ndef custom_accuracy_v2(y_true, y_pred):\n    '''Calculates accuracy of the baseline model. The accuracy is calculated by matching how many correct\n    words were predicted excluding the paddings. Then, just add those which are correct and you will get the\n    the accuracy and then just divide it by total words not including padding.\n\n    Arguments:\n    y_true: tf.Tensor object, expected target values\n    y_pred: list of tf.Tensor object, predicted target values by model\n\n    Returns:\n    returns the total accuracy over the batch of data\n    '''\n    # Find the word index with maximum probablity\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    # Count the words that matches with true values\n    match = tf.cast(y_pred == y_true, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n\n    # Mask out the paddings\n    match *= mask\n\n    return tf.reduce_sum(match) / tf.reduce_sum(mask)","metadata":{"id":"E_SW6SzNDH1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Testing with loss and accuracy","metadata":{"id":"ShbQdiQqILXu"}},{"cell_type":"code","source":"print(f\"Sample y true values: {sum_tar}\")\nprint(f\"Sample y pred values(first 10 values of first 2 timestep): {sample_model_out[:2]}\")\n\nsample_loss = custom_loss_v2(sum_tar, sample_model_out)\nprint(f\"Loss of the sample y_true and y_pred: {sample_loss}\")\n\nsample_acc = custom_accuracy_v2(sum_tar, sample_model_out)\nprint(f\"Accuracy of the sample y_true and y_pred: {sample_acc}\")","metadata":{"outputId":"5b2bc229-776f-4259-9605-4f003e2e2c4c","id":"NOapo56oILX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Compiling pointer generator model","metadata":{"id":"nT2RqhLFMlHp"}},{"cell_type":"code","source":"lr = LEARNING_RATE\ninitial_accumulator_value = INIT_ACC_VAL\nclipnorm = MAX_GRAD_NORM\n\nopt = Adagrad(learning_rate=lr,\n              initial_accumulator_value=initial_accumulator_value,\n              clipnorm=clipnorm)\n\npointer_model.compile(loss=custom_loss_v2, optimizer=opt, metrics=[custom_loss_v2, custom_accuracy_v2])","metadata":{"id":"WTayhfQFMlH0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating callbacks for model","metadata":{"id":"pQSFBBfYMlH0"}},{"cell_type":"code","source":"# Mention the checkpoint path and it's directory where you will save the model\ncheckpoint_path = POINTER_MODEL_CHECKPOINT\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Calculate no of batches, I am taking floor because when creating the training data I used drop_remainder\nn_batches = int(train_dataset.shape[0] / BATCH_SIZE)\n\n# Create the checkpoint for model saving, monitoring val_custom_accuracy_v1 and save only weights of the model\nsaving_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                monitor='val_custom_accuracy_v2',\n                                                verbose=1,\n                                                save_weights_only=True,\n                                                save_freq=n_batches//2)\n\n# Create the checkpoint for stopping early after noticing that val_custom_accuracy_v1 is not increasing even after 5 consecutive epochs\nearlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_custom_accuracy_v2',\n                                                    patience=PATIENCE,\n                                                    mode='max',\n                                                    )\n\n# Store the checkpoints in a list\ncallbacks = [saving_cb, earlystop_cb]","metadata":{"id":"BXuC_RPbMlH0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Training pointer generator model","metadata":{"id":"S4iHgb8KMlH0"}},{"cell_type":"code","source":"epochs = POINTER_EPOCHS\nsteps_per_epoch = POINTER_STEPS_PER_EPOCHS\n\nhistory = pointer_model.fit(tf_train_dataset.repeat(),\n                    epochs=epochs,\n                    validation_data=tf_val_dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    callbacks=callbacks\n                    )","metadata":{"id":"z0ZekuBoMlH0"},"execution_count":null,"outputs":[]}]}