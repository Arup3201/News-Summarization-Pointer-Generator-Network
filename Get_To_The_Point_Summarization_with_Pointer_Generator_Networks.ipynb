{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0Pm8QlH6zDkmBNgaCWE6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "J2_Q63jmdSp3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "x6QucUCZ0q2V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "529c4ff1-d55e-4c81-df71-1016057dfbb2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pathlib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# For tokenizing and processing the examples for the model training\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Layers for the Encoder, Attention and Decoder\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.layers import RepeatVector, Concatenate, Activation, Dot\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# For model initialization\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "path_to_dailymail_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        "    extract=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "hr2kPGPSLi5m",
        "outputId": "d1d3695b-8f5f-4018-d1fb-e1529383c236"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\n",
            "158577824/158577824 [==============================] - 3s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\n",
            "375893739/375893739 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories, path_to_dailymail_stories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uamve0operH0",
        "outputId": "0be6cede-3719-4d10-d0fe-12bcb3f6eeb9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.keras/datasets/cnn_stories.tgz',\n",
              " '/root/.keras/datasets/dailymail_stories.tgz')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /root/.keras/datasets"
      ],
      "metadata": {
        "id": "WT84BmPll24S",
        "outputId": "b083fc1e-0925-468b-e6eb-1d197bd934b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 521956\n",
            "drwxr-xr-x 3 root root      4096 Jul 28 05:38 cnn\n",
            "-rw-r--r-- 1 root root 158577824 Jul 28 05:38 cnn_stories.tgz\n",
            "drwxr-xr-x 3 root root      4096 Jul 28 05:39 dailymail\n",
            "-rw-r--r-- 1 root root 375893739 Jul 28 05:39 dailymail_stories.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir = pathlib.Path('/root/.keras/datasets/cnn/stories')\n",
        "dailymail_stories_dir = pathlib.Path('/root/.keras/datasets/dailymail/stories')"
      ],
      "metadata": {
        "id": "ag5ubSt7l88l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "19d75469-79ce-4384-eacb-eda4b1275de3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir, dailymail_stories_dir"
      ],
      "metadata": {
        "id": "UmMCM--Yhm3B",
        "outputId": "fc04ddac-bfa8-4fa6-f926-ea431488dfff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/root/.keras/datasets/cnn/stories'),\n",
              " PosixPath('/root/.keras/datasets/dailymail/stories'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_filenames(dir_path, num_files=5):\n",
        "  '''Prints the name of the files that are present at `dir_path`.\n",
        "  Maximum `num_files` number of files are shown.\n",
        "\n",
        "  Arguments:\n",
        "    dir_path: PosixPath, pointing to the directory of which the user\n",
        "              wants to prints the file names.\n",
        "    num_files: int, number of files user wants to print.\n",
        "\n",
        "  returns:\n",
        "    nothing\n",
        "  '''\n",
        "\n",
        "  count = 0\n",
        "  for f in dir_path.glob('*.story'):\n",
        "    print(f.name)\n",
        "    count += 1\n",
        "\n",
        "    if count == num_files:\n",
        "      break\n",
        "  else:\n",
        "    print(f\"Less than {num_files} is present!\")"
      ],
      "metadata": {
        "id": "dZFFXG7Pg86H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "34d4020e-dc28-49fe-d335-1ae5d71382b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(cnn_stories_dir)"
      ],
      "metadata": {
        "id": "nv3g7-M316yL",
        "outputId": "6a6a28b6-0bd6-43c2-b8b8-7014d63bb567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dfc761dc9979d48f81622209420ec256c27b064b.story\n",
            "7a91fabdfe78b9357173135abd8a1eb3d6f54883.story\n",
            "7366de0ba2f4d1d1dea009d28770643cc373f0f0.story\n",
            "a71b9b280bd0fd28e47407956cde2844c812acd4.story\n",
            "ede76765d99a416d0a353c858267e0be3ce44f53.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(dailymail_stories_dir)"
      ],
      "metadata": {
        "id": "WcyW1wJa2EaN",
        "outputId": "23448ee9-d0f1-4c75-cfe2-3dc917703da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fee8d56ace20661cec362490adaa5d3af2860a19.story\n",
            "6de92207db5537a42078856ece9fd190f5d961ea.story\n",
            "b842f69a37319d3e9dcb9e3068023275fd10adc7.story\n",
            "af7a438d642ea2be63ac4a16f6dce42cde0344bc.story\n",
            "8803ae457b25fd727e922c03730c7f1631688996.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
        "              dm_single_close_quote, dm_double_close_quote, \")\"]\n",
        "\n",
        "# Maximum stories to process from cnn and dailymail each\n",
        "MAX_STORIES = 50000\n",
        "\n",
        "# From the total data how to split into train, val and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VAL_SIZE = 0.1\n",
        "TEST_SIZE = 0.1\n",
        "\n",
        "# For tokenization\n",
        "VOCAB_SIZE = 20000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# For standardization\n",
        "PAD_TOKEN = '<PAD>'\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "# For the number of tokens to use in representing articles and summaries, hyperparameters\n",
        "MAX_ARTICLE_TOKENS = 400\n",
        "MAX_SUMMARY_TOKENS = 100\n",
        "\n",
        "# For dataset creation hyperparameters\n",
        "BUFFER_SIZE = 5000\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "## Model Archietecture hyperparameters\n",
        "# Embedding output dimension\n",
        "EMB_OUT = 32\n",
        "# Encoder hidden(also cell) state dimension\n",
        "ENCODER_STATE_DIM = 32\n",
        "# Decoder hidden(also cell) state dimension\n",
        "DECODER_STATE_DIM = 64\n",
        "# Attention first dense layer units(calculates partial energy)\n",
        "DENSE1_UNITS = 16\n",
        "# Attention secodn dense layer units(calculated final energy)\n",
        "DENSE2_UNITS = 1\n",
        "# Units of the Dense layers before output layer\n",
        "DENSE_UNITS = 16"
      ],
      "metadata": {
        "id": "cubLAm1Q3SuG",
        "outputId": "b2218d98-c05e-401f-976a-753682481e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a sample .story file from cnn stories\n",
        "sample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\n",
        "sample_filedir = cnn_stories_dir\n",
        "\n",
        "sample_filepath = sample_filedir / sample_filename\n",
        "with open(sample_filepath, 'r') as f:\n",
        "  sample_story = f.read()\n",
        "\n",
        "print(sample_story)"
      ],
      "metadata": {
        "id": "Uw0kqchX3X7X",
        "outputId": "6754911d-4128-488d-e123-d7f0486c1d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\n",
            "\n",
            "The figure represents a 0.7% increase from last year, adding 2,250,129 people to the U.S. population since the start of 2011, and a 1.3% increase since Census Day, April 1, 2010.\n",
            "\n",
            "The agency estimates that beginning in January, one American will be born every eight seconds and one will die every 12 seconds.\n",
            "\n",
            "U.S.-bound immigrants are also expected to add one person every 46 seconds.\n",
            "\n",
            "That combination of births, deaths and migration is expected to add a single person to the U.S. population every 17 seconds, the Census Bureau said.\n",
            "\n",
            "Meanwhile, millions are set to ring in the new year.\n",
            "\n",
            "In New York, authorities are preparing for large crowds in Manhattan's Times Square, where Lady Gaga is expected to join Mayor Michael Bloomberg to push the button that drops the Waterford Crystal ball at 11:59 p.m. ET on New Year's Eve.\n",
            "\n",
            "\"And I'm so looking forward to performing on NYE+dropping the Ball with Mayor Bloomberg!\" the pop star posted on Twitter. \"What an honor as a New Yorker.\"\n",
            "\n",
            "Past guests have included Muhammad Ali, Rudy Giuliani, Colin Powell and Bill and Hillary Clinton.\n",
            "\n",
            "On Thursday, officials conducted New York's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above Times Square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square.\n",
            "\n",
            "The Big Apple this year edged out Las Vegas for the first time in seven years as the top travel U.S. destination for those celebrating the new year, according to a December travel booking website poll.\n",
            "\n",
            "Seven New York neighborhoods made the top 10 list, with two districts in Las Vegas and one in New Orleans making up the other three, according to the Priceline poll.\n",
            "\n",
            "\"It appears that New York City will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman Brian Ek.\n",
            "\n",
            "@highlight\n",
            "\n",
            "Census Bureau: U.S. population is expected to be 312.8 million on New Year's Day\n",
            "\n",
            "@highlight\n",
            "\n",
            "That figure represents a 0.7% increase from last year\n",
            "\n",
            "@highlight\n",
            "\n",
            "Lady Gaga, mayor to activate ball drop at Times Square on New Year's Eve\n",
            "\n",
            "@highlight\n",
            "\n",
            "NYC has supplanted Las Vegas as the top New Year's destination, Priceline poll says\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n",
        "\n",
        "These are the steps -\n",
        "1. Check if line contains `@highlight`, if True then just return the line.\n",
        "2. Check if line is empty, then return line as it is.\n",
        "3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n",
        "4. Only is none of the above conditions match then append `.` to the current line."
      ],
      "metadata": {
        "id": "iZPt8EVBzJ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_missing_period(line, end_tokens=END_TOKENS):\n",
        "  '''function to fix the missing periods for some story lines which do not end with\n",
        "  any of the end_tokens mentioned.\n",
        "\n",
        "  Argument:\n",
        "    line: string, line of the story to fix the missing the period of.\n",
        "    end_tokens: list of strings, all the tokens that are considered as line end.\n",
        "\n",
        "  Returns:\n",
        "    new line with fixed the ending part by adding an ending token if not present.\n",
        "  '''\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  elif line == \"\":\n",
        "    return line\n",
        "  elif line[-1] in end_tokens:\n",
        "    return line\n",
        "\n",
        "  return line + '.'"
      ],
      "metadata": {
        "id": "i-S-Hss12TPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fe603c11-916b-4d00-f272-d36de7d8be09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fix_missing_period(sample_story.split('\\n')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L1hiNysxnmAO",
        "outputId": "2b86963e-2fbd-49cd-95e3-899c11ab8828"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `split_article_summary` which will split the story into article and summary parts.\n",
        "\n",
        "The function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n",
        "\n",
        "The steps to follow are -\n",
        "1. Split the story by new line `\\n`. I will get a list of lines.\n",
        "2. Strip the lines by using list comprehension.\n",
        "3. Use list comprehension to make lower case each line by using `.lower()`.\n",
        "4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n",
        "5. Make 2 empty list for `article` and `summary`.\n",
        "6. Go through each line. In each line, I need to check 4 things,\n",
        "  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n",
        "  * line is `\"\"` empty or not, if True then ignore.\n",
        "  * `next_highlight` is True or not, if True then append the line to `summary`.\n",
        "  * If non of the ebove then append to `article`.\n",
        "7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method."
      ],
      "metadata": {
        "id": "5-tqWtoowXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_article_summary(story):\n",
        "  '''Splits the story into 2 parts, one for article and other for summary of that\n",
        "  article. Returns the article and summary.\n",
        "\n",
        "  Argument:\n",
        "    story: string file that contains both article and summary combiningly.\n",
        "\n",
        "  Returns:\n",
        "    article, summary seperately from the story.\n",
        "\n",
        "  '''\n",
        "  lines = story.split('\\n')\n",
        "  lines = [line.strip() for line in lines]\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  # Fix the ending period\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # List to contain the article and summary lines\n",
        "  article = []\n",
        "  summary = []\n",
        "\n",
        "  # Indicator of whether the next line is the summary or not\n",
        "  next_highlight = False\n",
        "\n",
        "  for line in lines:\n",
        "    if \"@highlight\" in line:\n",
        "      next_highlight = True\n",
        "    elif line==\"\":\n",
        "      continue\n",
        "    elif next_highlight:\n",
        "      summary.append(line)\n",
        "    else:\n",
        "      article.append(line)\n",
        "\n",
        "  article = ' '.join(article)\n",
        "  summary = ' '.join(summary)\n",
        "\n",
        "  return article, summary"
      ],
      "metadata": {
        "id": "-X4eMltQnf10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "16cc177e-6165-4341-8dc9-4945e4f7f895"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_article_summary(sample_story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nuUjOaN9orGU",
        "outputId": "d3cb3ce4-a9ea-42e2-b890-f9be7cd7ea57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.',\n",
              " \"census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n",
        "\n",
        "This function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n",
        "\n",
        "The process is simple. We will follow this steps -\n",
        "1. Create 2 empty lists of `articles` and `summaries`.\n",
        "2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n",
        "3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n",
        "4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n",
        "5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n",
        "6. Return the 2 lists."
      ],
      "metadata": {
        "id": "oTEa0m3Huxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_summaries(stories_dir, max_stories):\n",
        "  '''stores the stories from stories_dir folder into a list and returns the list\n",
        "\n",
        "  Arguments:\n",
        "    stories_dir: Posix string, the directory where the stories are stored\n",
        "    max_stories: maximum number of stories to store\n",
        "\n",
        "  Returns:\n",
        "    list of stories.\n",
        "\n",
        "  '''\n",
        "  articles = []\n",
        "  summaries = []\n",
        "\n",
        "  count = 0\n",
        "  for f in stories_dir.glob(\"*.story\"):\n",
        "    count += 1\n",
        "    with open(f, 'r') as reader:\n",
        "      story = reader.read()\n",
        "\n",
        "      article, summary = split_article_summary(story)\n",
        "\n",
        "      articles.append(article)\n",
        "      summaries.append(summary)\n",
        "\n",
        "    if count == max_stories:\n",
        "      break\n",
        "\n",
        "  return articles, summaries"
      ],
      "metadata": {
        "id": "4VUmbYSpnjAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3254b1ef-ef5f-4f00-d1fc-db5ad63a2e30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "cnn\n",
        "  stories\n",
        "    438411e10e1ef79b47cc48cd95296d85798c1e38.story\n",
        "    e453e379e8a70af2d3dff1c75c41b0a35edbe9cc.story\n",
        "    2079f35aca44978a7985afe0ddacdf02bedf98f2.story\n",
        "    4702f28c198223157bb8f69665b039d560eebb0f.story\n",
        "    db3e2ea79323a98379228b17cd3b9dec17dbd2cb.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "dailymail\n",
        "  stories\n",
        "    f4ba18635997139c751311b9f2ad18f455dd7c98.story\n",
        "    4a3ef32cff589c85ad0d22724e2ed747c0dacf87.story\n",
        "    5375ed75939108c72001b043d3b4799c47f32be9.story\n",
        "    fe9e57c21e21fb4ec26e394f0e92824f38d18a95.story\n",
        "    6a544b5cdd2384be6cc657b265d7aa2de72a99e0.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them."
      ],
      "metadata": {
        "id": "Uk-BuCM4rIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)\n",
        "\n",
        "len(cnn_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aa9ZDQntpHQZ",
        "outputId": "7a137d59-8127-4343-e04e-c548e7405ca8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "-q_i-69YqJnj",
        "outputId": "ca5d5545-0696-4b3c-8211-1bf65f8d7850"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: (cnn)here's a tip for traveling in busan: whatever time you've allotted for a venue or attraction, triple it. no, quadruple it. everything in south korea's second largest city (after seoul) is better than you think it's going to be, and you'll want more time to explore and enjoy. case in point is spa land centum city, a gigantic, modern jjimjilbang (korean-style sauna/spa) located inside shinsegae department store centum city, reportedly the world's largest department store, which opened four years ago. before seeing spa land, i had set aside an hour for a visit -- the usual amount of time i allot for soaking and scrubbing at a korean bathhouse. coming from seoul, i was skeptical about how different one jjimjilbang could be from any other. the capital, after all, is well known for its extravagant spas. boy, was i mistaken. a spa like no other. centum city's spa land takes the jjimjilbang concept to an entirely new place. while seoul jjimjilbangs tend to be mobbed 24 hours a day with families and groups of friends, spa land presents a more refined, upscale and leisurely experience. despite state-of-the-art facilities, admission fees are reasonable -- ₩12,000 ($10.50) on weekdays and ₩14,000 ($12.25) on weekends. students get a slight discount and -- here's an interesting kicker -- children under 13 aren't allowed inside, an unheard of concept in korea, where families often make weekend rituals of jjimjilbang visits. another unusual feature is the four-hour limit on stays. most korean jjimjilbangs are open 24 hours and often serve as a crash pad for over-indulgent partiers, many of whom prefer to sleep off the effects of the night in a spa rather than face the repercussions of coming home late and smelling like the inside of a brewery vat. how to do spa land right. the first thing koreans want to know about a jjimjilbang is whether the water is special enough to bathe in. the water in spa land's baths and pools is pumped from two types of hot springs that lie 100 meters underground, and which are sterilized 36 times a day. the \"sodium bicarbonate hot spring\" is the \"beauty bathtub,\" said by the spa to \"remove your dead skin cells and make your skin and hair shiny.\" the \"sodium chloride hot spring\" is \"similar to seawater\" and has a \"heat-preservation effect\" that is \"good for blood circulation and helps to relieve pain from neuralgia and backache.\" the spa ritual itself is familiar. visitors change into cotton shirts and pants handed out at reception. then they head to communal areas. there's a tarot card reading station set up near the entrance, but most people beeline past this and head straight for the outdoor foot bath area. here there's a large heated pool for wading back and forth, as well as private booths where couples often play games on their phones while perched precariously above pools of water. theme rooms. spa land has dreamed up a variety of themes for its many steam and sauna rooms. these range from extremely hot (i couldn't enter this one without yelping in pain) to extremely cold (the ice room has a cute, fake jellyfish aquarium) to the gimmicky. how gimmicky? the walls of the pyramid room are set at a 52-degree angle, \"which has been said to be the easiest angle to collect energies from the universe,\" according to the spa. the sev room \"radiates electrons from sev\" meant to \"metabolize your body rapidly.\" the theme rooms are fun to take pictures in. the downstairs snack bar serves bingsu (a beloved korean shaved ice dessert) and various vinegar drinks said to be good for the skin. customers can take the snacks and eat them all around the bathhouse. upstairs there's a restaurant and cafe run by chefs from the westin chosun hotel. alcohol consumption is limited to 500 ml per person, to prevent sauna accidents and overly rambunctious parties from disrupting the austere atmosphere. bath time. my favorite spot in the spa is the outdoor rock pool in the women-only bathing area. i soaked under its sodium bicarbonate waterfall for a good 20 minutes before my appointment with the seshin ajumma (scrub ladies) in the scrub room. for 25 minutes i beached myself on a plastic slab, and gave myself over to the capable hands of a professional scrubber. clad in black bras and panties (standard scrub uniform), she scoured my entire body with two loofahs. \"young ladies are the most sensitive,\" she said in an amused voice when i squeaked a little. \"the older ladies always ask for the hardest pressure.\" i emerged red and raw, but wonderfully clean. it was the best extra ₩20,000 ($18) i've ever spent in spa. next time i'll plan on three hours for a visit. make that four. spa land, centum city, 35 centumnam-daero, haeundae-gu, busan, south korea; +82 51 745 2900; open daily, 6 a.m.-midnight.\n",
            "\n",
            "\n",
            "The summary of this article: spa land centum city offers refined, upscale spa experience. even spa-jaded koreans are awestruck by this state-of-the-art spa. water in spa land's baths comes from hot springs 100 meters underground. walls of pyramid room are set at a 52-degree angle to \"collect energies from the universe\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n",
        "                                                                 MAX_STORIES)"
      ],
      "metadata": {
        "id": "KT4Hrp6nqeKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8c8b552c-6ec4-4f0a-cc19-f09c873a2b5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(dailymail_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {dailymail_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "id": "nkzwSP9kh4VK",
        "outputId": "d4ef854e-e81a-402d-f9c4-0407f27bc229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: by. helen pow. the first pictures have emerged of a 4-year-old girl who was tragically killed wednesday when a driver crashed a vehicle into her florida childcare center as she was waiting for her afternoon snack and then fled off. little lily quintus was killed and 14 others were injured in the horror crash at kindercare daycare center in winter park. on thursday afternoon, police announced robert alex corchado - the man they believe caused the fatal wreck - had turned himself in. lily was sitting at a table waiting. for her afternoon snack when the car crashed into the building, and. her heartbroken mother, nicole quintus, said a teacher called her soon after screaming but. unable to say what happened. scroll down for video. in custody: on thursday afternoon, authorities announced robert alex corchado, left, the man they believe caused the fatal wreck that killed lily quintus, right, had turned himself in. tragic: the first pictures have emerged of a 4-year-old lily quintus, pictured, who was tragically killed wednesday when a driver crashed a vehicle into her florida childcare center. 'one minute everything was normal and. the next there was an explosion and smoke and screams,' she said. thursday. quintus said three surgeons fought to save her daughter's life. but it was too late. the florida highway patrol said police agencies statewide were alerted to look for 26-year-old corchado, who has a long criminal history. at about 4:30 p.m., officials confirmed corchado was being processed at the orange county jail. he is accused of sparking the incident when his dodge durango hit a convertible, which smashed into the building the children were in. little princess: lily (pictured left and right) was sitting at a table waiting for her afternoon snack when the car came barreling through the wall. fhp major cindy williams had revealed corchado has ties to the latin kings gang and likely has friends who have helped him hide since wednesday's crash. authorities had issued a $5,000 reward for information leading to his arrest. earlier, nicole quintus had pleaded for corchado to turn himself in. 'come forward,' she said in an. interview with the associated press. 'families are. emotionally destroyed because of what he did.' the. mother softly sobbed as she spoke of her daughter. she said lily loved. princesses, 'star wars,' the tv series 'doctor who' and ranch dressing. on seemingly everything she ate - even pizza and hot dogs. crime scene: a child is wheeled on a stretcher from the day care center to a waiting ambulance after wednesday's crash. impact: police say a silver or grayish dodge crashed into a vehicle that then struck the goldenrod road kindercare at north goldenrod road and university boulevard in winter park. criminal: this undated photo provided by the florida department of corrections shows robert corchado during a previous arrest. the girl's 7-year-old brother is an aspiring engineer who wants to design a time machine to bring lily back, the mother added. 'she was beautiful and passionate and innocent and she deserved so much more.' a gofundme page has been set up to raise money for the girl's family. lily. quintus was on the minds of those who turned up at the day care, where. the gaping hole was boarded up with plywood and a cluster of stuffed. animals, flowers and candles were left in memoriam. ralph. velez, 48, left a stuffed bear from his 5-year-old son xavier, who goes. to the day care but was unhurt. the bear was a gift from a few. christmases ago, and xavier - who can't stop talking about the crash -. wanted to give it to honor his friends, velez said. 'he'll. say, \"stupid car,\" or \"stupid driver,\"' velez said. 'he told us last. night that he remembers the driver getting out and saying, \"what did i. do? what did i do?\"' local television footage showed small. children and infants in cribs being taken outside to the day care's. playground in the wednesday incident. several. of those injured were carried out on stretchers. parents were later. seen waiting to pick up their children, then clutching them in their. arms as authorities escorted them to their vehicles. corchado. has been arrested eight times since 2000. department of corrections. records show he served prison time for trafficking cocaine and. extortion. chaos: the inside of the kindercare classroom is a shambles after the car was towed out of the scene where several children were injured. disbelief: rescue officials say 15 people, including 11 children, were injured including lily in the horror wreck. he most. recently was arrested in december on a misdemeanor charge of leaving the. scene of a crash involving damage, a felony charge of selling. narcotics, and felony marijuana possession. he was released on more than. $10,000 bond and pleaded not guilty to the charges. he was due back in. court in may. jack kaleita,. an attorney representing corchado in that case, did not return a call. seeking comment thursday. a man who answered the phone at a number. listed for corchado hung up. in. all, 13 people were hospitalized and two others were treated at the. scene, authorities said. eleven of the injured were children, said john. mulhall, a spokesman for the orange county fire rescue. besides. quintus, seven victims were brought to the arnold palmer hospital for. children, where one remained in critical condition thursday and two in. serious condition. the day. care's website says the center provides childcare and learning. opportunities for children up to 12 years old and has been in the. community for more than 25 years. sorry we are not currently accepting comments on this article.\n",
            "\n",
            "\n",
            "The summary of this article: little lily quintus was killed and 14 others were injured in the crash at the kindercare daycare center in winter park, florida, wednesday. authorities caught robert alex corchado - the man they believe caused the fatal wreck - thursday afternoon after a widespread search. lily was sitting at a table waiting for her afternoon snack when the car crashed into the building. her heartbroken mother, nicole quintus, said a teacher called her soon after, screaming but unable to say what happened. police said 26-year-old corchado has a long criminal history.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1, 2] + [3, 4]"
      ],
      "metadata": {
        "id": "ToPn8sAtpyOX",
        "outputId": "3cf28ec5-6e3a-4962-a311-cae9ee02165a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0) # Keeps the shuffling same as before\n",
        "random.sample([1, 2, 3, 4, 5], 5)"
      ],
      "metadata": {
        "id": "eFZO1v0NqIJM",
        "outputId": "60eed5e7-e27b-442a-8daa-0ed6d990ca0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article, sample_summary = split_article_summary(sample_story)"
      ],
      "metadata": {
        "id": "ZJCTOgIJmyXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a9b95e6f-b9c1-4efc-9cac-47a02b2335ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article"
      ],
      "metadata": {
        "id": "STljZePPnGy9",
        "outputId": "ebf0ee25-d59c-4b85-e8e5-e7f78a2a702c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating another function -\n",
        "`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples."
      ],
      "metadata": {
        "id": "8ea-PhS3iIJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_size, val_size, test_size):\n",
        "  first_split = train_size\n",
        "  second_split = train_size+val_size\n",
        "  third_split = train_size+val_size+test_size\n",
        "  return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]"
      ],
      "metadata": {
        "id": "v-iZDbUCqkdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fa8f5b92-f48b-4ada-f548-88de68f92161"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize the 4 functions created above into one function called `make_datasets`. This function will -\n",
        "1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\n",
        "Objective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n",
        "```python\n",
        "[1, 2] + [3, 4] = [1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n",
        "```python\n",
        "  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n",
        "```\n",
        "4. Shuffle the dataset using `random.sample` method.\n",
        "```python\n",
        "random.seed(seed_value) # To make sure that everytime it gives the same shuffle\n",
        "random.sample(list_to_shuffle, len(list_to_shuffle))\n",
        "```\n",
        "5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`."
      ],
      "metadata": {
        "id": "FTB5eNzJrqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n",
        "  '''Create 3 datasets each for training, validation and testing respectively.\n",
        "  This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n",
        "  them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n",
        "  summaries using random.sample method (although we have a helper function for it). Finally we do the\n",
        "  splitting of the whole dataset. Remember here the returned values become tensors.\n",
        "\n",
        "  Arguments:\n",
        "    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n",
        "    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n",
        "    train_size: float, specifying how much fraction of the original dataset to take for training.\n",
        "    val_size: float, specifying how much fraction of the original dataset to take for validation.\n",
        "    test_size: float, specifying how much fraction of the original dataset to take for testing.\n",
        "\n",
        "  Returns:\n",
        "    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n",
        "    with the specified amount of data in it.\n",
        "    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n",
        "  '''\n",
        "  articles = cnn_stories[0] + dailymail_stories[0]\n",
        "  summaries = cnn_stories[1] + dailymail_stories[1]\n",
        "\n",
        "  articles = np.array(articles, dtype=object)\n",
        "  summaries = np.array(summaries, dtype=object)\n",
        "\n",
        "  dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n",
        "\n",
        "  random.seed(seed_value)\n",
        "  shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n",
        "\n",
        "  dataset = dataset[shuffled_indices, :]\n",
        "\n",
        "  train_size = int(train_fraction * dataset.shape[0])\n",
        "  val_size = int(val_fraction * dataset.shape[0])\n",
        "  test_size = dataset.shape[0] - (train_size + val_size)\n",
        "\n",
        "  training_samples, validation_samples, testing_samples = split_dataset(dataset,\n",
        "                                                                        train_size,\n",
        "                                                                        val_size,\n",
        "                                                                        test_size)\n",
        "\n",
        "  return (training_samples, validation_samples, testing_samples)"
      ],
      "metadata": {
        "id": "QDB0_32RrnHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5ea8a745-09ff-408f-cb3d-b8553c82b97b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ],
      "metadata": {
        "id": "GTnXBwd6Sa-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "31daad87-b838-430e-82af-04639de8a40b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n",
        "\n",
        "print(f\"Training dataset shape: {train_dataset.shape}\")\n",
        "print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
        "print(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n",
        "\n",
        "print(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")"
      ],
      "metadata": {
        "id": "AyRVodwIhkuQ",
        "outputId": "7535d8f6-2948-4299-f368-0a9b8346ea2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the datasets: <class 'numpy.ndarray'>\n",
            "\n",
            "Training dataset shape: (80000, 2)\n",
            "Validation dataset shape: (10000, 2)\n",
            "Testing dataset shape: (10000, 2)\n",
            "\n",
            "First example in the training dataset looks like: \n",
            " [\"in steven gerrard’s dreams, wednesday night ends with him celebrating like it is 2009. liverpool blow real madrid away, their stuttering season catches fire and more memories are created to last a lifetime. his nightmares, however, involve an embarrassment, of liverpool finding themselves on the receiving end of a scoreline similar to the 4-0 drubbing they administered to real, when these teams last collided five years ago. the first scenario, he believes, is possible, particularly if brendan rodgers’ side are swept along on a wave of emotion. if liverpool use the energy that a highly-charged anfield crowd will give them, gerrard hopes they can swarm all over los blancos. video\\xa0scroll down to watch liverpool host real in european epic. steven gerrard has told his liverpool team-mates not to shy away against real madrid on wednesday night. gerrard believes the atmosphere at anfield can play its part in beating carlo ancelotti's real like it did in 2009. cristiano ronaldo gestures in the rain at anfield. cristiano ronaldo and his real madrid team-mates braved stormy conditions during a training session at anfield on tuesday evening. the portugal international was well prepared for the adverse weather and wore a snood on his head as cover for the duration of the session. click here for the full story. but the second picture, gerrard recognises, is also possible, especially if one member of liverpool’s starting line-up approaches this contest with the mindset that they can coast through 90 minutes, not pushing themselves to the limit. for liverpool to defy the odds, no passengers are permitted. ‘that applies to me personally as well,’ gerrard said. ‘everyone who is picked to play — from the 11 in the starting line-up to the subs on the bench — everyone has to be on their game. we are playing against the best side in europe and if we have two or three passengers, we get beat convincingly. ‘it can become embarrassing against sides like this who are deadly away from home on the counter-attack. we need to be right on it. but when big questions get asked of this team — especially last year when big hitters came to town — we raised our game. ‘the fans raised their game, we raised ours and put in some really good performances. i think a (big) performance is due now. qpr wasn’t good enough, everyone knows that, but everyone will respect that when real madrid come into town it’s a different game to get up for. ‘every player in the dressing room realises we can’t have any passengers or we will get beat convincingly. i can understand that (many people are predicting a madrid win). most places they go, with the current players and the form they are in, they are expected to win most games they play. ‘i understand those opinions but shocks are there, upsets happen. yes, we understand we are the underdogs but with the atmosphere we have here and the players raising their game for a night like this, i think it is possible to cause an upset.’ gerrard  celebrates scoring his team's third goal during the champions league win against real madrid in 2009. gerrard hopes liverpool can recreate another magical night like the one in 2009 against real at anfield. liverpool best beware of real star cristiano ronaldo. given the way liverpool played at loftus road on sunday, many would find it difficult to see why the captain has such optimism. liverpool’s play has been scruffy and inconsistent this season and logic dictates that it should be no different, especially against a side of madrid’s undoubted class. but this, remember, is anfield on a european night. this is the stadium where logic is often defied on such occasions, when the improbable has been made possible. gerrard has been a key protagonist in the drama since the turn of this century, not least on march 10, 2009. ‘looking back at the game, the reason we came out on top was because we were aggressive with and without the ball,’ said gerrard, who scored two of liverpool’s four goals. ‘we pressed from the front and had a world-class talent in (fernando) torres and we forced them into mistakes. i can’t give away tactics but with due respect to the team in 2009 individually real madrid have improved so we have huge respect for their current team. ‘we need to enjoy the game but you only enjoy games like this when you play as a team. we’re looking to shock a few people.’ there were times during a five-year absence from the champions league when gerrard admitted to being consumed with envy towards those who had taken liverpool’s place and it is why he makes a point of re-emphasising this is not simply an occasion to enjoy. ‘i think all the hard work brendan has done and the lads have done (means) we deserve nights like these,’ said gerrard, for whom this will be a 127th european appearance. ‘i just hope no-one from the outside thinks we are just going into these games to enjoy ourselves. i hope nobody thinks we are settling for being in the champions league. ‘madrid are top opposition and probably the best side in europe but we want to give a good account of ourselves. this is a great opportunity for us to put in a really strong performance.’ it is time, then, for liverpool to prove that the spellbinding play of last season was not a flash in the pan. they might be comparative novices in this competition but, as 34-year-old gerrard points out, that will count for nothing when the whistle blows. gerrard and manager brendan rodgers spoke to the press ahead of liverpool's clash with real madrid. arsenal's new striker danny welbeck sent steven gerrard a message on instagram ahead of real madrid's visit, writing: 'good luck for tomorrow!' ‘i don’t think any of us expect a year’s leeway,’ said gerrard. ‘in the time i’ve been here i have never experienced supporters saying “oh it is just nice to be in champions league; it doesn’t matter how you do, you will get judged next year”. ‘we will get judged against the best side in europe. the message to the players is simple: we need a performance. ‘i am pretty confident that we can surprise a few people but that is how i am. i go into games with optimism, rather than fearing individuals and the opposition.’ his team-mates must follow suit if gerrard’s dream is to come true. gerrard believes liverpool can pull off a win against la liga giants real madrid on wednesday night. gerrard does not want his liverpool side to be embarrassed and has called for no hiding away on wednesday. video liverpool host real in european epic.\"\n",
            " \"liverpool beat real madrid 4-0 in 2009 at anfield in the champions league. carlo ancelotti's side face liverpool on wednesday night in group b. liverpool captain steven gerrard says 'everyone has to be on their game'\"]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n",
        "\n",
        "1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n",
        "2. Replace the single and double opening and closing quotes like `‘ → \\u2018`, `’ → \\u2019`, `“ → \\u201c` and `” → \\u201d` by `'` and `\"` respectively.\n",
        "3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\n",
        "In this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n",
        "4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n",
        "\n",
        "`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n",
        "1. Remove the `START_TOKEN` and `END_TOKEN` from the text. So that tokenizer does not standardize them.\n",
        "2. Standardize the text with `standardizer`.\n",
        "3. Add back the `START_TOKEN` and `END_TOKEN` because you want your tokenizer to learn them.\n",
        "4. Remove unwanted spaces in between words.\n",
        "5. Split the text into words which are seperated by ' '.\n",
        "6. Strip each of the words in the sentence. Finally, return it."
      ],
      "metadata": {
        "id": "TcZCpJ9hCyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the text data\n",
        "def standardizer(text):\n",
        "  '''Standardize the text provided to the function\n",
        "  The text is lower cased. Then, the opening and closing quotes are removed. I add spaces before the\n",
        "  punctuations like `don't` becomes `don ' t`, ignoring the numerical values so that `1.78` does not become\n",
        "  `1 . 78`. Finally, it strips the text and removes any type of unwanted spaces in it.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text to standardize\n",
        "\n",
        "  Returns:\n",
        "    returns the standadized text\n",
        "  '''\n",
        "\n",
        "  # Lower case the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Replace the special single and double opening and closing quotes\n",
        "  text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n",
        "  text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n",
        "\n",
        "  # Add space before punctuations and ignore floating point numbers.\n",
        "  text = re.sub(r'(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n",
        "                  r' \\1 ', text)  # It used to also remove commas after numbers like '27,' will be removed\n",
        "\n",
        "  # Remove spaces after sentence end and other unwanted spaces from text\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# custom analyzer for the Tokenizer class\n",
        "def custom_analyzer(text):\n",
        "  '''Custom analyzer to provide to the `Tokenizer` class when creating the tokenizer.\n",
        "\n",
        "  Argument:\n",
        "    text: str, the text that will be tokenized\n",
        "\n",
        "  Returns:\n",
        "    returns the splitted sentence\n",
        "  '''\n",
        "  # Remove START and END before standardizing\n",
        "  if START_TOKEN in text:\n",
        "    text = re.sub(f'{START_TOKEN} ', '', text)\n",
        "  if END_TOKEN in text:\n",
        "    text = re.sub(f'{END_TOKEN} ', '', text)\n",
        "\n",
        "  # Standardize the text first\n",
        "  text = standardizer(text)\n",
        "\n",
        "  # Add back the START and END tokens\n",
        "  text = ' '.join([START_TOKEN, text, END_TOKEN])\n",
        "\n",
        "  # Split the sentence into words to tokenize\n",
        "  words = text.split(' ')\n",
        "  words = [word.strip() for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "FD2h0OiAxJP_",
        "outputId": "62e12256-7874-45e1-f315-cf5eb1af4127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n",
        "                \"U.S won the world cup and bagged 1.78 million dollars.\",\n",
        "                \"India had M.S. Dhoni won made it this far.\",\n",
        "                \"My email address is arupjana7365@gmail.com.\",\n",
        "                \"It can take care of dailymail single opening quote’ also.\",\n",
        "                \"I have 10,000 Rs in my bank\",\n",
        "                \"This sentence has , after a number 12,\",\n",
        "                \"This sentence contains <START> token and <END> token.\"]\n",
        "\n",
        "print(f\"After Standardizing the sample texts:\\n{[standardizer(text) for text in sample_texts]}\\n\")\n",
        "print(f\"After applying custom analyzer on sample texts:\\n{[custom_analyzer(text) for text in sample_texts]}\")"
      ],
      "metadata": {
        "id": "AVi3kZhcLXS7",
        "outputId": "86c65f90-34f0-40cc-a339-d41c9c9d1c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Standardizing the sample texts:\n",
            "['i have been working on , but never did it in this way .', 'u . s won the world cup and bagged 1.78 million dollars .', 'india had m . s . dhoni won made it this far .', 'my email address is arupjana7365@gmail . com .', \"it can take care of dailymail single opening quote ' also .\", 'i have 10,000 rs in my bank', 'this sentence has , after a number 12,', 'this sentence contains < start > token and < end > token .']\n",
            "After applying custom analyzer on sample texts:\n",
            "[['<START>', 'i', 'have', 'been', 'working', 'on', ',', 'but', 'never', 'did', 'it', 'in', 'this', 'way', '.', '<END>'], ['<START>', 'u', '.', 's', 'won', 'the', 'world', 'cup', 'and', 'bagged', '1.78', 'million', 'dollars', '.', '<END>'], ['<START>', 'india', 'had', 'm', '.', 's', '.', 'dhoni', 'won', 'made', 'it', 'this', 'far', '.', '<END>'], ['<START>', 'my', 'email', 'address', 'is', 'arupjana7365@gmail', '.', 'com', '.', '<END>'], ['<START>', 'it', 'can', 'take', 'care', 'of', 'dailymail', 'single', 'opening', 'quote', \"'\", 'also', '.', '<END>'], ['<START>', 'i', 'have', '10,000', 'rs', 'in', 'my', 'bank', '<END>'], ['<START>', 'this', 'sentence', 'has', ',', 'after', 'a', 'number', '12,', '<END>'], ['<START>', 'this', 'sentence', 'contains', 'token', 'and', 'token', '.', '<END>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n",
        "\n",
        "When fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab."
      ],
      "metadata": {
        "id": "IiiWRFR64jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(texts, num_words, oov_token=None, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n",
        "  '''This will create the tokenizer needed for the task in hand.\n",
        "  The tokenizer will be trained on the `texts`. Tokenizer will have vocabulary length `num_words`.\n",
        "  The `oov_token` will be used as the token represent the out-of-vocabulary words. The `filters` are\n",
        "  the ones which the tokenizer will remove when tokenizing any sentence given to it. The returned\n",
        "  tokenizer is using a custom analyzer that can standardize the sentence before tokenizing using the\n",
        "  `standardizer` function and then splits the sentence into words. After that it tokenizes the sentence.\n",
        "  As for the vocabulary, the returned tokenizer's vocabulary does not contain any number, as I have removed\n",
        "  them before feeding them into `Tokenizer.fit_on_texts` method.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the tokenizer will be trained on this strings\n",
        "    num_words: int, number of vocabulary words the tokenizer will consider\n",
        "    oov_token: str, token to represent out-of-vocabulary words\n",
        "    filters: str, all the characters that the tokenizer will remove before tokenizing\n",
        "\n",
        "  Returns:\n",
        "    tokenzier of the `Tokenizer` class after learning vocabulary from `texts`\n",
        "  '''\n",
        "\n",
        "  # Create the tokenizer usinf Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=num_words,\n",
        "                        filters=filters,\n",
        "                        oov_token=oov_token,\n",
        "                        analyzer=custom_analyzer)\n",
        "\n",
        "  # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n",
        "  texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n",
        "\n",
        "  # Fit the data with fit_on_texts method\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "tR1FD3SESd0G",
        "outputId": "7f888e05-d454-4f27-826a-2229710f58f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")"
      ],
      "metadata": {
        "id": "UsmErFoxqimM",
        "outputId": "456ea3b9-e24d-4fa5-d213-d9bb87c69878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length the articles dataset: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `tokenizer` using the articles from training dataset by using `train_dataset[:, 0]`, with a vocabulary size of `VOCAB_SIZE` and use `OOV_TOKEN` token to represent out-of-vocabulary words."
      ],
      "metadata": {
        "id": "6KBXcUV2pYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)"
      ],
      "metadata": {
        "id": "151rEpqo7Pof",
        "outputId": "8a2eacfa-dd56-48a9-adf8-8df34df63f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"{OOV_TOKEN} word has index: {tokenizer.word_index[OOV_TOKEN]}\")\n",
        "print(f\"{START_TOKEN} word has index: {tokenizer.word_index[START_TOKEN]}\")\n",
        "print(f\"{END_TOKEN} word has index: {tokenizer.word_index[END_TOKEN]}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\\n\")\n",
        "\n",
        "print(f\"Text:\\n{train_dataset[0, 0]}\\n\\n\")\n",
        "sample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\n",
        "print(f\"Text to Sequence of the first article:\\n{sample_sequence}\\n\")\n",
        "print(f\"Sequence to Text of the first acrticle:\\n{tokenizer.sequences_to_texts(sample_sequence)}\")"
      ],
      "metadata": {
        "id": "YUNreIjr8Kng",
        "outputId": "94dc0d47-79c5-4743-bf12-dd60982a130e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary for the tokenizer has a length 251348\n",
            "\n",
            "\n",
            "<OOV> word has index: 1\n",
            "<START> word has index: 76\n",
            "<END> word has index: 77\n",
            "\n",
            "\n",
            "'teacher' word has index: 1595\n",
            "\n",
            "Text:\n",
            "in steven gerrard’s dreams, wednesday night ends with him celebrating like it is 2009. liverpool blow real madrid away, their stuttering season catches fire and more memories are created to last a lifetime. his nightmares, however, involve an embarrassment, of liverpool finding themselves on the receiving end of a scoreline similar to the 4-0 drubbing they administered to real, when these teams last collided five years ago. the first scenario, he believes, is possible, particularly if brendan rodgers’ side are swept along on a wave of emotion. if liverpool use the energy that a highly-charged anfield crowd will give them, gerrard hopes they can swarm all over los blancos. video scroll down to watch liverpool host real in european epic. steven gerrard has told his liverpool team-mates not to shy away against real madrid on wednesday night. gerrard believes the atmosphere at anfield can play its part in beating carlo ancelotti's real like it did in 2009. cristiano ronaldo gestures in the rain at anfield. cristiano ronaldo and his real madrid team-mates braved stormy conditions during a training session at anfield on tuesday evening. the portugal international was well prepared for the adverse weather and wore a snood on his head as cover for the duration of the session. click here for the full story. but the second picture, gerrard recognises, is also possible, especially if one member of liverpool’s starting line-up approaches this contest with the mindset that they can coast through 90 minutes, not pushing themselves to the limit. for liverpool to defy the odds, no passengers are permitted. ‘that applies to me personally as well,’ gerrard said. ‘everyone who is picked to play — from the 11 in the starting line-up to the subs on the bench — everyone has to be on their game. we are playing against the best side in europe and if we have two or three passengers, we get beat convincingly. ‘it can become embarrassing against sides like this who are deadly away from home on the counter-attack. we need to be right on it. but when big questions get asked of this team — especially last year when big hitters came to town — we raised our game. ‘the fans raised their game, we raised ours and put in some really good performances. i think a (big) performance is due now. qpr wasn’t good enough, everyone knows that, but everyone will respect that when real madrid come into town it’s a different game to get up for. ‘every player in the dressing room realises we can’t have any passengers or we will get beat convincingly. i can understand that (many people are predicting a madrid win). most places they go, with the current players and the form they are in, they are expected to win most games they play. ‘i understand those opinions but shocks are there, upsets happen. yes, we understand we are the underdogs but with the atmosphere we have here and the players raising their game for a night like this, i think it is possible to cause an upset.’ gerrard  celebrates scoring his team's third goal during the champions league win against real madrid in 2009. gerrard hopes liverpool can recreate another magical night like the one in 2009 against real at anfield. liverpool best beware of real star cristiano ronaldo. given the way liverpool played at loftus road on sunday, many would find it difficult to see why the captain has such optimism. liverpool’s play has been scruffy and inconsistent this season and logic dictates that it should be no different, especially against a side of madrid’s undoubted class. but this, remember, is anfield on a european night. this is the stadium where logic is often defied on such occasions, when the improbable has been made possible. gerrard has been a key protagonist in the drama since the turn of this century, not least on march 10, 2009. ‘looking back at the game, the reason we came out on top was because we were aggressive with and without the ball,’ said gerrard, who scored two of liverpool’s four goals. ‘we pressed from the front and had a world-class talent in (fernando) torres and we forced them into mistakes. i can’t give away tactics but with due respect to the team in 2009 individually real madrid have improved so we have huge respect for their current team. ‘we need to enjoy the game but you only enjoy games like this when you play as a team. we’re looking to shock a few people.’ there were times during a five-year absence from the champions league when gerrard admitted to being consumed with envy towards those who had taken liverpool’s place and it is why he makes a point of re-emphasising this is not simply an occasion to enjoy. ‘i think all the hard work brendan has done and the lads have done (means) we deserve nights like these,’ said gerrard, for whom this will be a 127th european appearance. ‘i just hope no-one from the outside thinks we are just going into these games to enjoy ourselves. i hope nobody thinks we are settling for being in the champions league. ‘madrid are top opposition and probably the best side in europe but we want to give a good account of ourselves. this is a great opportunity for us to put in a really strong performance.’ it is time, then, for liverpool to prove that the spellbinding play of last season was not a flash in the pan. they might be comparative novices in this competition but, as 34-year-old gerrard points out, that will count for nothing when the whistle blows. gerrard and manager brendan rodgers spoke to the press ahead of liverpool's clash with real madrid. arsenal's new striker danny welbeck sent steven gerrard a message on instagram ahead of real madrid's visit, writing: 'good luck for tomorrow!' ‘i don’t think any of us expect a year’s leeway,’ said gerrard. ‘in the time i’ve been here i have never experienced supporters saying “oh it is just nice to be in champions league; it doesn’t matter how you do, you will get judged next year”. ‘we will get judged against the best side in europe. the message to the players is simple: we need a performance. ‘i am pretty confident that we can surprise a few people but that is how i am. i go into games with optimism, rather than fearing individuals and the opposition.’ his team-mates must follow suit if gerrard’s dream is to come true. gerrard believes liverpool can pull off a win against la liga giants real madrid on wednesday night. gerrard does not want his liverpool side to be embarrassed and has called for no hiding away on wednesday. video liverpool host real in european epic.\n",
            "\n",
            "\n",
            "Text to Sequence of the first article:\n",
            "[[76, 10, 2412, 3866, 5, 13, 3906, 4, 295, 203, 3276, 22, 74, 3546, 95, 19, 18, 1, 850, 2709, 347, 1230, 222, 4, 44, 1, 294, 12027, 396, 8, 56, 3245, 34, 938, 6, 82, 7, 3665, 2, 25, 11679, 4, 272, 4, 4955, 37, 7668, 4, 9, 850, 1593, 766, 17, 3, 2215, 239, 9, 7, 18464, 781, 6, 3, 1, 1, 35, 8013, 6, 347, 4, 54, 162, 1330, 82, 8055, 201, 88, 309, 2, 3, 86, 5192, 4, 20, 1048, 4, 18, 542, 4, 1197, 73, 5044, 3465, 5, 301, 34, 4515, 446, 17, 7, 2853, 9, 5693, 2, 73, 850, 250, 3, 888, 14, 7, 1714, 11, 632, 5279, 1305, 48, 374, 89, 4, 3866, 1321, 35, 66, 15812, 65, 85, 946, 1, 2, 1, 106, 6, 447, 850, 1354, 347, 10, 660, 6436, 2, 2412, 3866, 32, 83, 25, 850, 177, 11, 3547, 40, 6, 5665, 222, 125, 347, 1230, 17, 295, 203, 2, 3866, 1048, 3, 2834, 26, 5279, 66, 362, 84, 183, 10, 2421, 7228, 7165, 5, 13, 347, 95, 19, 146, 10, 1, 4776, 2428, 9266, 10, 3, 1849, 26, 5279, 2, 4776, 2428, 8, 25, 347, 1230, 177, 11, 3547, 16229, 13606, 1056, 119, 7, 642, 2107, 26, 5279, 17, 273, 1199, 2, 3, 3834, 252, 16, 150, 1771, 15, 3, 10177, 998, 8, 2555, 7, 1, 17, 25, 289, 24, 1254, 15, 3, 11033, 9, 3, 2107, 2, 2567, 223, 15, 3, 416, 486, 2, 33, 3, 205, 791, 4, 3866, 18308, 4, 18, 67, 542, 4, 921, 73, 53, 694, 9, 850, 5, 13, 1399, 507, 11, 58, 6870, 38, 3025, 22, 3, 12279, 14, 35, 66, 895, 142, 1, 452, 4, 40, 2786, 766, 6, 3, 2542, 2, 15, 850, 6, 14337, 3, 3658, 4, 81, 993, 34, 6755, 2, 5, 14, 8522, 6, 124, 3386, 24, 150, 4, 5, 3866, 21, 2, 5, 647, 41, 18, 1813, 6, 362, 856, 30, 3, 1, 10, 3, 1399, 507, 11, 58, 6, 3, 7933, 17, 3, 3677, 856, 647, 32, 6, 31, 17, 44, 261, 2, 42, 34, 665, 125, 3, 233, 301, 10, 803, 8, 73, 42, 28, 69, 60, 110, 993, 4, 42, 112, 1152, 1, 2, 5, 19, 66, 372, 5504, 125, 1772, 95, 38, 41, 34, 1998, 222, 30, 109, 17, 3, 2764, 11, 315, 2, 42, 245, 6, 31, 135, 17, 19, 2, 33, 54, 299, 906, 112, 282, 9, 38, 177, 856, 921, 82, 59, 54, 299, 1, 230, 6, 551, 856, 42, 1072, 103, 261, 2, 5, 3, 523, 1072, 44, 261, 4, 42, 1072, 8941, 8, 227, 10, 91, 221, 194, 4011, 2, 27, 166, 7, 50, 299, 49, 1128, 18, 575, 94, 2, 4495, 603, 5, 51, 194, 441, 4, 647, 1535, 14, 4, 33, 647, 48, 1621, 14, 54, 347, 1230, 216, 78, 551, 19, 5, 13, 7, 384, 261, 6, 112, 58, 15, 2, 5, 235, 645, 10, 3, 4262, 499, 1, 42, 66, 5, 51, 28, 134, 993, 60, 42, 48, 112, 1152, 1, 2, 27, 66, 1015, 14, 50, 126, 63, 34, 10198, 7, 1230, 358, 49, 2, 116, 1408, 35, 170, 4, 22, 3, 796, 438, 8, 3, 812, 35, 34, 10, 4, 35, 34, 513, 6, 358, 116, 520, 35, 362, 2, 5, 27, 1015, 130, 1986, 33, 13993, 34, 62, 4, 1, 1028, 2, 1430, 4, 42, 1015, 42, 34, 3, 1, 33, 22, 3, 2834, 42, 28, 223, 8, 3, 438, 2395, 44, 261, 15, 7, 203, 95, 38, 4, 27, 166, 19, 18, 542, 6, 767, 37, 2216, 2, 5, 3866, 2776, 1981, 25, 177, 5, 13, 467, 590, 119, 3, 1041, 306, 358, 125, 347, 1230, 10, 1, 3866, 1321, 850, 66, 11477, 172, 7480, 203, 95, 3, 53, 10, 1, 125, 347, 26, 5279, 2, 850, 233, 16359, 9, 347, 417, 4776, 2428, 2, 349, 3, 143, 850, 574, 26, 12866, 613, 17, 286, 4, 126, 64, 324, 19, 717, 6, 165, 305, 3, 1148, 32, 171, 8256, 2, 850, 5, 13, 362, 32, 46, 1, 8, 11502, 38, 294, 8, 10495, 18231, 14, 19, 176, 31, 81, 384, 4, 921, 125, 7, 301, 9, 1230, 5, 13, 1, 910, 2, 33, 38, 4, 1293, 4, 18, 5279, 17, 7, 660, 203, 2, 38, 18, 3, 1443, 105, 10495, 18, 479, 11196, 17, 171, 3780, 4, 54, 3, 19192, 32, 46, 120, 542, 2, 3866, 32, 46, 7, 787, 1, 10, 3, 2772, 145, 3, 797, 9, 38, 1217, 4, 40, 354, 17, 483, 1, 1, 5, 400, 108, 26, 3, 261, 4, 3, 976, 42, 230, 61, 17, 234, 16, 101, 42, 45, 3021, 22, 8, 267, 3, 912, 4, 5, 21, 3866, 4, 41, 1245, 69, 9, 850, 5, 13, 175, 1062, 2, 5, 42, 5573, 30, 3, 455, 8, 43, 7, 97, 11, 910, 2642, 10, 50, 4888, 49, 5998, 8, 42, 733, 89, 78, 4037, 2, 27, 66, 5, 51, 374, 222, 4410, 33, 22, 575, 1621, 6, 3, 177, 10, 1, 11168, 347, 1230, 28, 3400, 72, 42, 28, 786, 1621, 15, 44, 796, 177, 2, 5, 42, 245, 6, 1805, 3, 261, 33, 52, 107, 1805, 520, 95, 38, 54, 52, 362, 24, 7, 177, 2, 42, 5, 148, 400, 6, 1889, 7, 293, 63, 2, 5, 62, 45, 249, 119, 7, 201, 11, 59, 3985, 30, 3, 1041, 306, 54, 3866, 809, 6, 100, 7069, 22, 15084, 1215, 130, 41, 43, 254, 850, 5, 13, 218, 8, 19, 18, 305, 20, 727, 7, 378, 9, 148, 11, 1, 38, 18, 40, 1006, 37, 3619, 6, 1805, 2, 5, 27, 166, 65, 3, 435, 167, 5044, 32, 454, 8, 3, 9651, 28, 454, 50, 690, 49, 42, 4044, 4235, 95, 162, 4, 5, 21, 3866, 4, 15, 1364, 38, 48, 31, 7, 1, 660, 1570, 2, 5, 27, 79, 527, 81, 11, 53, 30, 3, 370, 2599, 42, 34, 79, 155, 78, 162, 520, 6, 1805, 3514, 2, 27, 527, 2472, 2599, 42, 34, 10097, 15, 100, 10, 3, 1041, 306, 2, 5, 1230, 34, 234, 1093, 8, 936, 3, 233, 301, 10, 803, 33, 42, 186, 6, 374, 7, 194, 1059, 9, 3514, 2, 38, 18, 7, 280, 1068, 15, 169, 6, 227, 10, 7, 221, 714, 1128, 2, 5, 19, 18, 71, 4, 118, 4, 15, 850, 6, 2011, 14, 3, 1, 362, 9, 82, 294, 16, 40, 7, 5017, 10, 3, 6416, 2, 35, 346, 31, 1, 1, 10, 38, 1375, 33, 4, 24, 1, 11, 102, 3866, 778, 61, 4, 14, 48, 2194, 15, 568, 54, 3, 6823, 8608, 2, 3866, 8, 638, 5044, 3465, 1074, 6, 3, 669, 624, 9, 850, 5, 13, 2451, 22, 347, 1230, 2, 1075, 5, 13, 68, 1376, 3470, 6502, 539, 2412, 3866, 7, 785, 17, 2694, 624, 9, 347, 1230, 5, 13, 712, 4, 1774, 23, 5, 194, 4222, 15, 2570, 300, 5, 5, 27, 158, 5, 51, 166, 134, 9, 169, 1332, 7, 59, 5, 13, 1, 4, 5, 21, 3866, 2, 5, 10, 3, 71, 27, 5, 213, 46, 223, 27, 28, 202, 2232, 1156, 287, 12, 2773, 19, 18, 79, 1837, 6, 31, 10, 1041, 306, 220, 19, 502, 5, 51, 813, 113, 52, 99, 4, 52, 48, 112, 7690, 198, 59, 12, 2, 5, 42, 48, 112, 7690, 125, 3, 233, 301, 10, 803, 2, 3, 785, 6, 3, 438, 18, 1671, 23, 42, 245, 7, 1128, 2, 5, 27, 318, 1201, 2283, 14, 42, 66, 1930, 7, 293, 63, 33, 14, 18, 113, 27, 318, 2, 27, 170, 78, 520, 22, 8256, 4, 742, 75, 7830, 1850, 8, 3, 1093, 2, 5, 25, 177, 11, 3547, 430, 1266, 1827, 73, 3866, 5, 13, 1685, 18, 6, 216, 1102, 2, 3866, 1048, 850, 66, 2287, 122, 7, 358, 125, 1542, 5177, 4474, 347, 1230, 17, 295, 203, 2, 3866, 363, 40, 186, 25, 850, 301, 6, 31, 6202, 8, 32, 185, 15, 81, 3493, 222, 17, 295, 2, 187, 850, 1354, 347, 10, 660, 6436, 2, 77]]\n",
            "\n",
            "Sequence to Text of the first acrticle:\n",
            "['<START> in steven gerrard \\' s dreams , wednesday night ends with him celebrating like it is <OOV> liverpool blow real madrid away , their <OOV> season catches fire and more memories are created to last a lifetime . his nightmares , however , involve an embarrassment , of liverpool finding themselves on the receiving end of a scoreline similar to the <OOV> <OOV> they administered to real , when these teams last collided five years ago . the first scenario , he believes , is possible , particularly if brendan rodgers \\' side are swept along on a wave of emotion . if liverpool use the energy that a highly - charged anfield crowd will give them , gerrard hopes they can swarm all over los <OOV> . <OOV> down to watch liverpool host real in european epic . steven gerrard has told his liverpool team - mates not to shy away against real madrid on wednesday night . gerrard believes the atmosphere at anfield can play its part in beating carlo ancelotti \\' s real like it did in <OOV> cristiano ronaldo gestures in the rain at anfield . cristiano ronaldo and his real madrid team - mates braved stormy conditions during a training session at anfield on tuesday evening . the portugal international was well prepared for the adverse weather and wore a <OOV> on his head as cover for the duration of the session . click here for the full story . but the second picture , gerrard recognises , is also possible , especially if one member of liverpool \\' s starting line - up approaches this contest with the mindset that they can coast through <OOV> minutes , not pushing themselves to the limit . for liverpool to defy the odds , no passengers are permitted . \\' that applies to me personally as well , \\' gerrard said . \\' everyone who is picked to play — from the <OOV> in the starting line - up to the subs on the bench — everyone has to be on their game . we are playing against the best side in europe and if we have two or three passengers , we get beat <OOV> . \\' it can become embarrassing against sides like this who are deadly away from home on the counter - attack . we need to be right on it . but when big questions get asked of this team — especially last year when big <OOV> came to town — we raised our game . \\' the fans raised their game , we raised ours and put in some really good performances . i think a ( big ) performance is due now . qpr wasn \\' t good enough , everyone knows that , but everyone will respect that when real madrid come into town it \\' s a different game to get up for . \\' every player in the dressing room <OOV> we can \\' t have any passengers or we will get beat <OOV> . i can understand that ( many people are predicting a madrid win ) . most places they go , with the current players and the form they are in , they are expected to win most games they play . \\' i understand those opinions but shocks are there , <OOV> happen . yes , we understand we are the <OOV> but with the atmosphere we have here and the players raising their game for a night like this , i think it is possible to cause an upset . \\' gerrard celebrates scoring his team \\' s third goal during the champions league win against real madrid in <OOV> gerrard hopes liverpool can recreate another magical night like the one in <OOV> against real at anfield . liverpool best beware of real star cristiano ronaldo . given the way liverpool played at loftus road on sunday , many would find it difficult to see why the captain has such optimism . liverpool \\' s play has been <OOV> and inconsistent this season and logic dictates that it should be no different , especially against a side of madrid \\' s <OOV> class . but this , remember , is anfield on a european night . this is the stadium where logic is often defied on such occasions , when the improbable has been made possible . gerrard has been a key <OOV> in the drama since the turn of this century , not least on march <OOV> <OOV> \\' looking back at the game , the reason we came out on top was because we were aggressive with and without the ball , \\' said gerrard , who scored two of liverpool \\' s four goals . \\' we pressed from the front and had a world - class talent in ( fernando ) torres and we forced them into mistakes . i can \\' t give away tactics but with due respect to the team in <OOV> individually real madrid have improved so we have huge respect for their current team . \\' we need to enjoy the game but you only enjoy games like this when you play as a team . we \\' re looking to shock a few people . \\' there were times during a five - year absence from the champions league when gerrard admitted to being consumed with envy towards those who had taken liverpool \\' s place and it is why he makes a point of re - <OOV> this is not simply an occasion to enjoy . \\' i think all the hard work brendan has done and the lads have done ( means ) we deserve nights like these , \\' said gerrard , for whom this will be a <OOV> european appearance . \\' i just hope no - one from the outside thinks we are just going into these games to enjoy ourselves . i hope nobody thinks we are settling for being in the champions league . \\' madrid are top opposition and probably the best side in europe but we want to give a good account of ourselves . this is a great opportunity for us to put in a really strong performance . \\' it is time , then , for liverpool to prove that the <OOV> play of last season was not a flash in the pan . they might be <OOV> <OOV> in this competition but , as <OOV> - old gerrard points out , that will count for nothing when the whistle blows . gerrard and manager brendan rodgers spoke to the press ahead of liverpool \\' s clash with real madrid . arsenal \\' s new striker danny welbeck sent steven gerrard a message on instagram ahead of real madrid \\' s visit , writing : \\' good luck for tomorrow ! \\' \\' i don \\' t think any of us expect a year \\' s <OOV> , \\' said gerrard . \\' in the time i \\' ve been here i have never experienced supporters saying \" oh it is just nice to be in champions league ; it doesn \\' t matter how you do , you will get judged next year \" . \\' we will get judged against the best side in europe . the message to the players is simple : we need a performance . \\' i am pretty confident that we can surprise a few people but that is how i am . i go into games with optimism , rather than fearing individuals and the opposition . \\' his team - mates must follow suit if gerrard \\' s dream is to come true . gerrard believes liverpool can pull off a win against la liga giants real madrid on wednesday night . gerrard does not want his liverpool side to be embarrassed and has called for no hiding away on wednesday . video liverpool host real in european epic . <END>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\n",
        "NO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that."
      ],
      "metadata": {
        "id": "crn5t_zk6iBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.keys())[21000]"
      ],
      "metadata": {
        "id": "qhttL-heeP-P",
        "outputId": "e3e36fba-c9d7-47b8-f013-caac3ad53966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'restraints'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov_word = list(tokenizer.word_index.keys())[21000]\n",
        "sample_text = f\"This example is to test the above fact with the word `{oov_word}`\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "S_Dnkcig7SIX",
        "outputId": "5f19a842-f49d-4095-f391-ed489c44e096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This example is to test the above fact with the word `restraints`\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> this example is to test the above fact with the word ` <OOV> ` <END>']\n",
            "Sequence: [[76, 38, 1134, 18, 6, 877, 3, 754, 548, 22, 3, 1290, 15392, 1, 15392, 77]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`."
      ],
      "metadata": {
        "id": "xvNeazXb-Yq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add a number 2.1 in this sentence!\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ng5MZ_DZ84kk",
        "outputId": "f6fbba87-bc38-41bb-d019-3e32cafe444e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add a number 2.1 in this sentence!\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add a number <OOV> in this sentence ! <END>']\n",
            "Sequence: [[76, 70, 2070, 54, 27, 1969, 7, 264, 1, 10, 38, 1112, 300, 77]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add parenthesis (I am inside it!).\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ImlMHniH-Jnt",
        "outputId": "2b8073ac-6a05-45b1-ebc5-56839b247d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add parenthesis (I am inside it!).\n",
            "\n",
            "\n",
            "Tokenized text: ['<START> what happens when i add <OOV> ( i am inside it ! ) . <END>']\n",
            "Sequence: [[76, 70, 2070, 54, 27, 1969, 1, 50, 27, 318, 506, 19, 300, 49, 2, 77]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the `tokenizer` to tokenize the articles and summaries. We need to pad those sequences to fit the requirements.\n",
        "\n",
        "In the paper, the articles are limited to have 400 tokens and summary has, 100 tokens at training and 120 tokens for testing.\n",
        "\n",
        "I will be using `pad_sequences` method to pad or truncate the articles and summaries based on their length.\n",
        "\n",
        "NOTE: I am using same tokenizer for article and summary. But, later I might change that to 2 different tokenizers each having different `num_words`."
      ],
      "metadata": {
        "id": "jza9oQYKXB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pad(texts, tokenizer, padding, truncating, maxlen):\n",
        "  '''Tokenize the `texts` using the tokenizer. Then, pad the sequences or truncate the sequences\n",
        "  depending the length. If the length exceeds `maxlen` then it will be truncated and if not then it will be\n",
        "  padded. The padding and truncating can happend at the beginning or at the end of the sequence depending\n",
        "  on the value of `padding` and `truncating` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    texts: list of strings, the sentences to tokenize and pad\n",
        "    tokenizer: Tokenizer class object, helps in tokenizing the `texts`\n",
        "    padding: str, can take 2 values `pre` or `post`. If `pre` then padding will happen at the beginning,\n",
        "    if `post` then padding will happen at the end.\n",
        "    truncating: str, can take 2 values `pre` or 'truncating`, works the same as `padding`\n",
        "    maxlen: int, maximum length after padding or truncating\n",
        "\n",
        "  Returns:\n",
        "    returns the tokenized and padded sentences\n",
        "  '''\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Alo70WYjW-tA",
        "outputId": "5bf80fd7-580b-4835-88ad-6f60116a58a1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "rttUwCiT4FzJ",
        "outputId": "0c8ab765-ea81-433e-a06f-7d4f3674eb74"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have been working on, \\nbut \\tnever did it in this way.',\n",
              " 'U.S won the world cup and bagged 1.78 million dollars.',\n",
              " 'India had M.S. Dhoni won made it this far.',\n",
              " 'My email address is arupjana7365@gmail.com.',\n",
              " 'It can take care of dailymail single opening quote’ also.',\n",
              " 'I have 10,000 Rs in my bank',\n",
              " 'This sentence has , after a number 12,',\n",
              " 'This sentence contains <START> token and <END> token.']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_pad(sample_texts, tokenizer, padding=\"post\", truncating=\"post\", maxlen=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "QNuBKpTq4lZW",
        "outputId": "3de78185-8a6d-48f7-a189-bb792d51c85a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   76,    27,    28,    46,   334,    17,     4,    33,   202,\n",
              "          146,    19,    10,    38,   143,     2,    77,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,   111,     2,    13,   297,     3,    97,   389,     8,\n",
              "        18336,     1,   161,  2108,     2,    77,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,  1039,    43,   136,     2,    13,     2,     1,   297,\n",
              "          120,    19,    38,   328,     2,    77,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,    87,  2646,  1263,    18,     1,     2,   621,     2,\n",
              "           77,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,    19,    66,   153,   360,     9, 19051,   871,  1078,\n",
              "         8151,     5,    67,     2,    77,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,    27,    28,     1, 17025,    10,    87,   881,    77,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,    38,  1112,    32,     4,    47,     7,   264,     1,\n",
              "           77,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   76,    38,  1112,  3876, 18031,     8, 18031,     2,    77,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Geneator function that generates the source and target for training the model."
      ],
      "metadata": {
        "id": "ZUtzIfXm6UnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n",
        "  '''Generates examples for the model. Processes the `inputs` and `targets` with their respective\n",
        "  tokenizers and tokenize them to `input_len` and `target_len` length.\n",
        "\n",
        "  Arguments:\n",
        "    inputs: list of input sentences\n",
        "    targets: list of target sentences\n",
        "    input_tokenizer: Tokenizer class object, tokenizer for inputs\n",
        "    target_tokenizer: Tokenizer class object, tokenizer for targets\n",
        "    input_len: int, the length of the tokenization for inputs\n",
        "    target_len: int, the length of the tokenization for targets\n",
        "\n",
        "  Returns:\n",
        "    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n",
        "    another numpy array target_tokens[1:]\n",
        "  '''\n",
        "\n",
        "  for inp, tar in zip(inputs, targets):\n",
        "    inp_tokens = tokenize_pad([inp],\n",
        "                              input_tokenizer,\n",
        "                              padding=\"post\",\n",
        "                              truncating=\"post\",\n",
        "                              maxlen=input_len)\n",
        "\n",
        "    tar_tokens = tokenize_pad([tar],\n",
        "                 target_tokenizer,\n",
        "                 padding=\"post\",\n",
        "                 truncating=\"post\",\n",
        "                 maxlen=target_len)\n",
        "\n",
        "    yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8647M-ey6n19",
        "outputId": "46e43183-db10-4bd0-c0b2-d6e5839ace56"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example generated by the generator :\")\n",
        "\n",
        "# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\n",
        "example_gen = generate_example(list(train_dataset[:, 0]),\n",
        "                               list(train_dataset[:, 1]),\n",
        "                               input_tokenizer=tokenizer,\n",
        "                               target_tokenizer=tokenizer,\n",
        "                               input_len=MAX_ARTICLE_TOKENS,\n",
        "                               target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "inps, tar = next(example_gen)\n",
        "print(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Target:\\n{tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "BKKyIeVa5B20",
        "outputId": "ef9f8b24-335f-4a5c-ade0-e42ef179f5d4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example generated by the generator :\n",
            "Inputs:\n",
            "[   76    10  2412  3866     5    13  3906     4   295   203  3276    22\n",
            "    74  3546    95    19    18     1   850  2709   347  1230   222     4\n",
            "    44     1   294 12027   396     8    56  3245    34   938     6    82\n",
            "     7  3665     2    25 11679     4   272     4  4955    37  7668     4\n",
            "     9   850  1593   766    17     3  2215   239     9     7 18464   781\n",
            "     6     3     1     1    35  8013     6   347     4    54   162  1330\n",
            "    82  8055   201    88   309     2     3    86  5192     4    20  1048\n",
            "     4    18   542     4  1197    73  5044  3465     5   301    34  4515\n",
            "   446    17     7  2853     9  5693     2    73   850   250     3   888\n",
            "    14     7  1714    11   632  5279  1305    48   374    89     4  3866\n",
            "  1321    35    66 15812    65    85   946     1     2     1   106     6\n",
            "   447   850  1354   347    10   660  6436     2  2412  3866    32    83\n",
            "    25   850   177    11  3547    40     6  5665   222   125   347  1230\n",
            "    17   295   203     2  3866  1048     3  2834    26  5279    66   362\n",
            "    84   183    10  2421  7228  7165     5    13   347    95    19   146\n",
            "    10     1  4776  2428  9266    10     3  1849    26  5279     2  4776\n",
            "  2428     8    25   347  1230   177    11  3547 16229 13606  1056   119\n",
            "     7   642  2107    26  5279    17   273  1199     2     3  3834   252\n",
            "    16   150  1771    15     3 10177   998     8  2555     7     1    17\n",
            "    25   289    24  1254    15     3 11033     9     3  2107     2  2567\n",
            "   223    15     3   416   486     2    33     3   205   791     4  3866\n",
            " 18308     4    18    67   542     4   921    73    53   694     9   850\n",
            "     5    13  1399   507    11    58  6870    38  3025    22     3 12279\n",
            "    14    35    66   895   142     1   452     4    40  2786   766     6\n",
            "     3  2542     2    15   850     6 14337     3  3658     4    81   993\n",
            "    34  6755     2     5    14  8522     6   124  3386    24   150     4\n",
            "     5  3866    21     2     5   647    41    18  1813     6   362   856\n",
            "    30     3     1    10     3  1399   507    11    58     6     3  7933\n",
            "    17     3  3677   856   647    32     6    31    17    44   261     2\n",
            "    42    34   665   125     3   233   301    10   803     8    73    42\n",
            "    28    69    60   110   993     4    42   112  1152     1     2     5\n",
            "    19    66   372  5504   125  1772    95    38    41    34  1998   222\n",
            "    30   109    17     3  2764    11   315     2    42   245     6    31\n",
            "   135    17    19     2]\n",
            "[  76  850 1152  347 1230    1   10    1   26 5279   10    3 1041  306\n",
            "    2 7228 7165    5   13  301  304  850   17  295  203   10  189 1751\n",
            "    2  850 1148 2412 3866  121    5  647   32    6   31   17   44  261\n",
            "    5   77    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n",
            "Target:\n",
            "[ 850 1152  347 1230    1   10    1   26 5279   10    3 1041  306    2\n",
            " 7228 7165    5   13  301  304  850   17  295  203   10  189 1751    2\n",
            "  850 1148 2412 3866  121    5  647   32    6   31   17   44  261    5\n",
            "   77    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the Inputs:\\n{inps[0].shape}\\n{inps[1].shape}\\n\\n\")\n",
        "print(f\"Shape of the Target:\\n{tar.shape}\")"
      ],
      "metadata": {
        "id": "E-w0hmecDcho",
        "outputId": "238949f3-c9e3-4564-dd4f-6a8f8f180ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the Inputs:\n",
            "(400,)\n",
            "(99,)\n",
            "\n",
            "\n",
            "Shape of the Target:\n",
            "(99,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data Type of the Inputs data:\\n{inps[0].dtype}\\n{inps[1].dtype}\\n\\n\")\n",
        "print(f\"Data Type of the Target data:\\n{tar.dtype}\")"
      ],
      "metadata": {
        "id": "suZmEEhfDvcu",
        "outputId": "697412eb-d1f2-468f-d10d-b087d2bb06a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Type of the Inputs data:\n",
            "int32\n",
            "int32\n",
            "\n",
            "\n",
            "Data Type of the Target data:\n",
            "int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inps, tar = next(example_gen)\n",
        "print(f\"Second Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Second Target:\\n{tar}\")"
      ],
      "metadata": {
        "id": "82m4io2PSyt3",
        "outputId": "432bb660-281a-4afb-8f2a-d795978c458a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second Inputs:\n",
            "[   76    19    16    53     9     3   116   790   563   608     9    84\n",
            "    71     8   523    45  2864    54    19  2872    30    44  5694  3376\n",
            "    47    79    69  3903     2    33    69     9     1     5   116    11\n",
            "  1193  2944   120     7  2284   570     6     3   750   361    38   163\n",
            "     6  2101     3   752     9     7    68  2978   544    55     3   341\n",
            "     9     3  7391     1   608     2     1  1244  1872  1647     1    11\n",
            "  3101     8    36  4865  6276     1  4641     4    69     9     3   196\n",
            "     5    13   116   790     1     4    45   349     7  6823    11   498\n",
            "   935     9    91     9   242     5    13   234  3295  7273     4   182\n",
            " 19193     1     4   105  1647     1    16  2796     7  3107  1911    22\n",
            "     5    36  9838     3  1155     5     8   349     7   720     6     1\n",
            "    58     6  1823     1     5   820  7969     5     2   541   106    15\n",
            "   187     2  1681     1     4    20     5    13   336   254   300     3\n",
            "     1   417  1462  1197 11386     9     3   820  7969     1     2     3\n",
            "     1     1  1872  3263    58    15     7   791    22     5     3  1155\n",
            "     5   119    36   712     6 19193     1     2   119    44 16475   935\n",
            "     9   242     4     3    69     1     4   257   135    22  6394 11011\n",
            "  2613     4    67   793     7   712     6  3295 16230     3   242  1108\n",
            "    50   123    49  2966    10     1  1759     1   523     7   720     6\n",
            "   165   113     3   790   608    16  2966    11     8   113     3   196\n",
            "     5    13  2036     1  7549    45    86  1755     2     3 15240   970\n",
            "     4    41    67   206     3    71     6  1805     7   973    17     3\n",
            "   242  1108     4    45  1361     6  3199     3  3129    11   404  2966\n",
            "    10     1     4     7  2978    29   329     1  1542     1     2    19\n",
            "  7448     3   563  9717     9     3   508     1  6394 11011  2613     4\n",
            "    41   284    10     1  1489     1    47   100  2099    22  6016     5\n",
            "    13     2     3   544  7435    17   790 10410    11  5468   608     1\n",
            "     8  1013     3  4171     9  2613     5    13  6514     1  4151  4666\n",
            "     4    57    45    86   120  1184    29     3   790 15240   608     2\n",
            "     1    18     7     1  4931   938    10     3     1     2     3  4931\n",
            "  9424    17     3   250     9  7132    11     1     1     4  2442    29\n",
            "  7292  8002  5527    11  3961    17     1  2831     2    19    16    86\n",
            "  1755    29     1  6394 11011  2613     4    25   118    11   302 16655\n",
            "     8    44   544   270]\n",
            "[   76  1647     1     8     1  4641    67   793     7   712     6     3\n",
            "   242  1108     2     3 15240   970    45  4700  2966    10     1     4\n",
            "     7    68  2978    55     1  6394 11011  2613     2   544  2776     3\n",
            "     1  2263     9     3   382     1   608     2    77     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "\n",
            "Second Target:\n",
            "[ 1647     1     8     1  4641    67   793     7   712     6     3   242\n",
            "  1108     2     3 15240   970    45  4700  2966    10     1     4     7\n",
            "    68  2978    55     1  6394 11011  2613     2   544  2776     3     1\n",
            "  2263     9     3   382     1   608     2    77     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the generator to generate the examples, we can use `tf.data` API to create the input data pipeline for our model. I will use the `tf.data.Dataset` class to get the the examples from the generator `generate_example`, we already have the generator object `example_gen`. We can use that in creating the dataset. For more about datasets from generator, refer to [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)."
      ],
      "metadata": {
        "id": "usV8UchPA35e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_example_generetor():\n",
        "  example_gen = generate_example(list(train_dataset[:, 0]),\n",
        "                                list(train_dataset[:, 1]),\n",
        "                                input_tokenizer=tokenizer,\n",
        "                                target_tokenizer=tokenizer,\n",
        "                                input_len=MAX_ARTICLE_TOKENS,\n",
        "                                target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "  for example in example_gen:\n",
        "    (input_0, input_1), target = example\n",
        "    yield (input_0, input_1), target"
      ],
      "metadata": {
        "id": "J_0r7ajRR8wk",
        "outputId": "ef3913fd-047f-499d-9481-6cfd0597858e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(MAX_ARTICLE_TOKENS, ), dtype=tf.int32),\n",
        "     tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_SUMMARY_TOKENS-1, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "tf_train_dataset = tf.data.Dataset.from_generator(generator=train_example_generetor,\n",
        "                                                  output_signature=output_signature)\n",
        "tf_train_dataset = tf_train_dataset.shuffle(BUFFER_SIZE)\n",
        "tf_train_dataset = tf_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "tf_train_dataset = tf_train_dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "Zj-H_fEh5Zu9",
        "outputId": "9f12fe7d-075b-474d-d559-27845fb588b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (art_inp, sum_inp), sum_tar in tf_train_dataset.take(1):\n",
        "  print(f\"Input tokenized article shape: {art_inp.shape}\")\n",
        "  print(f\"Input tokenized summary shape: {sum_inp.shape}\\n\")\n",
        "\n",
        "  print(f\"Target tokenized summary shape: {sum_tar.shape}\")"
      ],
      "metadata": {
        "id": "no4oMl22Fm6r",
        "outputId": "9309986b-f8f9-42c1-863f-921b6e9030c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokenized article shape: (16, 400)\n",
            "Input tokenized summary shape: (16, 99)\n",
            "\n",
            "Target tokenized summary shape: (16, 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, we need the model that we can train on this dataset. The model archietecture will be 3-\n",
        "1. Base-line model: Seq-Seq model with attention mechanism.\n",
        "2. Pointer Generetor model: With seq-seq attention model will be implementing the pointer generator that can either copy words from article or generate words from the pre-defined vocabulary.\n",
        "3. Coverage mechanism: Along with the pointer generator that will take case of the out-of-vocabulary words. Coverage mechanism will help prevent the repetition of the words in the summary."
      ],
      "metadata": {
        "id": "yM6KveyaUmfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base-Line Model: Seq-seq with Attention"
      ],
      "metadata": {
        "id": "UarmKLUIVkjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small demonstration of how Dot layer works"
      ],
      "metadata": {
        "id": "-2S_Jxhf25-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.arange(10).reshape(1, 2, 5)\n",
        "x2 = np.arange(10, 22).reshape(1, 2, 6)\n",
        "print(f\"x1: {x1}\\nx2: {x2}\")\n",
        "\n",
        "Dot(axes=1)([x1, x2])"
      ],
      "metadata": {
        "id": "_WLTScTp1x-R",
        "outputId": "41eecbbb-1d98-4fbe-88bc-c35d36c244b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: [[[0 1 2 3 4]\n",
            "  [5 6 7 8 9]]]\n",
            "x2: [[[10 11 12 13 14 15]\n",
            "  [16 17 18 19 20 21]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 5, 6), dtype=int64, numpy=\n",
              "array([[[ 80,  85,  90,  95, 100, 105],\n",
              "        [106, 113, 120, 127, 134, 141],\n",
              "        [132, 141, 150, 159, 168, 177],\n",
              "        [158, 169, 180, 191, 202, 213],\n",
              "        [184, 197, 210, 223, 236, 249]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_time_attention(a, s_prev,\n",
        "                       repeater, concatenator, densor_1, densor_2, softmax_layer, dotter):\n",
        "  '''Calculates the attention score and returns the context for the current timestep in the decoder.\n",
        "  Attention mechanism uses encoder outputs `a` of shape `(batch, timesteps, features)` and decoder\n",
        "  previous hidden state `s_prev` of shape `(batch, features)`, then calculates alignment scores `alphas`\n",
        "  for each encoder timestep with the help of energies computed with 2 dense layers using `a` and `s_prev`.\n",
        "\n",
        "  Arguments:\n",
        "    a: tf.Tensor object, encoder output of shape `(batch, timesteps, features)`\n",
        "    s_prev: tf.Tensor object, decoder previous hidden state\n",
        "    repeater: RepeatVector layer, repeat the `s_prev` `Tx` times\n",
        "    concatenator: Concatenate layer, concatenates `a` and repeated `s_prev`, Concatenates along axis=-1\n",
        "    densor_1: Dense layer, calculates the pertial energies `e`\n",
        "    densor_2: Dense layer, calculated the energies `energies`\n",
        "    softmax_layer: Activation layer, computes softmax of the energies and calculates `alphas`\n",
        "    dotter: Dot layer, Performs dot operation between `alphas` and `a` along axis=1\n",
        "\n",
        "  Returns:\n",
        "    returns the context of shape `(batch, features)`\n",
        "  '''\n",
        "\n",
        "  # Repeat the `s_prev` `Tx` times\n",
        "  s_prev = repeater(s_prev)\n",
        "\n",
        "  # Concatenate `a` and `s_prev` along axis=-1\n",
        "  concat = concatenator([a, s_prev])\n",
        "\n",
        "  # Apply dense layer to get partial energies e\n",
        "  e = densor_1(concat)\n",
        "\n",
        "  # Apply dense layer again to get energies\n",
        "  energies = densor_2(e)\n",
        "\n",
        "  # Apply softmax over the energies\n",
        "  alphas = softmax_layer(energies)\n",
        "\n",
        "  # Dot the alphas and a along axes=1, result shape (batch, densor_2_units, n_a*2)\n",
        "  context = dotter([alphas, a])\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "id": "B_rPlnWardTC",
        "outputId": "137e671d-677f-4b00-e46d-97550d2127da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model(Tx, Ty,\n",
        "                   emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                   article_vocab_size, summary_vocab_size):\n",
        "  '''This implements the bas-line model archietecture for summarization.\n",
        "  It is a seq-seq model with attention mechanism implemented in it. The encoder take an input\n",
        "  with `Tx` time-steps and summarizes with the help of decoder into Ty words. The encoder and decoder\n",
        "  hidden states are `n_a` and `n_s` dimension respectively. The words are taken from the vocabulary of\n",
        "  article and summary `article_vocab` and `summary_vocab` with size `article_vocab_size` and\n",
        "  `summary_vocab_size` respectively.\n",
        "\n",
        "  Arguments:\n",
        "    Tx: int, length of the input article\n",
        "    Ty: int, length of the output summary\n",
        "    n_a: int, dimension of the encoder hidden states\n",
        "    n_s: int, dimension of the deocder hidden states\n",
        "    article_vocab_size: int, length of the article vocabulary\n",
        "    summary_vocab_size: int, length of the summary vocabulary\n",
        "\n",
        "  Returns:\n",
        "    returns the base line model\n",
        "  '''\n",
        "  # Defining the input for our model with shape (None, Tx) and (None, Ty) for encoder input and decoder input\n",
        "  X_inp = Input(shape=(Tx))\n",
        "  X_tar = Input(shape=(Ty))\n",
        "\n",
        "  # Initialize s0\n",
        "  s0 = Input(shape=(n_s, ), name=\"s0\")\n",
        "  # Initialize c0\n",
        "  c0 = Input(shape=(n_s, ), name=\"c0\")\n",
        "\n",
        "  # Initialize the a and s with a0 and s0\n",
        "  s = s0\n",
        "  c = c0\n",
        "\n",
        "  # Define the outputs as empty list\n",
        "  outputs = []\n",
        "\n",
        "  # First embedding layer for the article input\n",
        "  encoder_inp = Embedding(article_vocab_size, emb_dim)(X_inp)\n",
        "\n",
        "  # Encoder: Bidirectional layer with LSTM cells\n",
        "  a = Bidirectional(LSTM(units=n_a, return_sequences=True))(encoder_inp)\n",
        "\n",
        "  # Define the embedding for decoder\n",
        "  decoder_inp = Embedding(summary_vocab_size, emb_dim)(X_tar)\n",
        "\n",
        "  # Define the layers for Attention so that we can use the same weights for all decoder timesteps\n",
        "  repeater = RepeatVector(Tx)\n",
        "  concatenator = Concatenate(axis=-1)\n",
        "  attn_densor1 = Dense(units=16, activation='tanh')\n",
        "  attn_densor2 = Dense(units=1, activation='linear', use_bias=False)\n",
        "  softmax_layer = Activation('softmax', name=\"attention_weights\")\n",
        "  dotter = Dot(axes=1)\n",
        "\n",
        "  # Define the Decoder unidirectional LSTM for shared weights\n",
        "  post_attention_lstm = LSTM(units=n_s, return_state=True)\n",
        "\n",
        "  # Define the last dense layer before output layer with linear activation\n",
        "  densor = Dense(units=16, activation='linear')\n",
        "\n",
        "  # Define the output layer so that it does not initalize again and again for shared weights\n",
        "  output_layer = Dense(units=summary_vocab_size, activation='softmax')\n",
        "\n",
        "  # Decoder: Appends outputs from the output layer in each timestep\n",
        "  for t in range(Ty):\n",
        "    # Get the decoder input for current timestep\n",
        "    curr_dec_in = decoder_inp[:, t:t+1, :]\n",
        "\n",
        "    # Get the context from the attention mechanism\n",
        "    context = one_time_attention(a, s,\n",
        "                                 repeater, concatenator, attn_densor1, attn_densor2, softmax_layer, dotter)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([curr_dec_in, context])\n",
        "    _, s, c = post_attention_lstm(concat, initial_state=[s, c])\n",
        "\n",
        "    # Calculate the output after using 2 linear dense layers\n",
        "    out = densor(s)\n",
        "    out = densor(s)\n",
        "    # Use the output_layer to get the output\n",
        "    out = output_layer(out)\n",
        "\n",
        "    # Append the final output to the outputs list\n",
        "    outputs.append(out)\n",
        "\n",
        "  model = Model(inputs=[X_inp, X_tar, s0, c0], outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "mudpzB78HDvZ",
        "outputId": "2a25418c-164f-4435-8128-1e86fdabbb4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tx = MAX_ARTICLE_TOKENS\n",
        "Ty = MAX_SUMMARY_TOKENS - 1\n",
        "emb_dim = EMB_OUT\n",
        "n_a = ENCODER_STATE_DIM\n",
        "n_s= DECODER_STATE_DIM\n",
        "d1_units = DENSE1_UNITS\n",
        "d2_units = DENSE2_UNITS\n",
        "d_units = DENSE_UNITS\n",
        "article_vocab_size = VOCAB_SIZE\n",
        "summary_vocab_size = VOCAB_SIZE\n",
        "\n",
        "model = baseline_model(Tx, Ty,\n",
        "                       emb_dim, n_a, n_s, d1_units, d2_units, d_units,\n",
        "                       article_vocab_size, summary_vocab_size)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "V3ZAtG5I8mZN",
        "outputId": "210dd1e2-060f-48fd-cbd6-81bfbb37fb2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 400)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, 400, 32)      640000      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 400, 64)     16640       ['embedding_8[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " repeat_vector_4 (RepeatVector)  (None, 400, 64)     0           ['s0[0][0]',                     \n",
            "                                                                  'lstm_9[0][1]',                 \n",
            "                                                                  'lstm_9[1][1]',                 \n",
            "                                                                  'lstm_9[2][1]',                 \n",
            "                                                                  'lstm_9[3][1]',                 \n",
            "                                                                  'lstm_9[4][1]',                 \n",
            "                                                                  'lstm_9[5][1]',                 \n",
            "                                                                  'lstm_9[6][1]',                 \n",
            "                                                                  'lstm_9[7][1]',                 \n",
            "                                                                  'lstm_9[8][1]',                 \n",
            "                                                                  'lstm_9[9][1]',                 \n",
            "                                                                  'lstm_9[10][1]',                \n",
            "                                                                  'lstm_9[11][1]',                \n",
            "                                                                  'lstm_9[12][1]',                \n",
            "                                                                  'lstm_9[13][1]',                \n",
            "                                                                  'lstm_9[14][1]',                \n",
            "                                                                  'lstm_9[15][1]',                \n",
            "                                                                  'lstm_9[16][1]',                \n",
            "                                                                  'lstm_9[17][1]',                \n",
            "                                                                  'lstm_9[18][1]',                \n",
            "                                                                  'lstm_9[19][1]',                \n",
            "                                                                  'lstm_9[20][1]',                \n",
            "                                                                  'lstm_9[21][1]',                \n",
            "                                                                  'lstm_9[22][1]',                \n",
            "                                                                  'lstm_9[23][1]',                \n",
            "                                                                  'lstm_9[24][1]',                \n",
            "                                                                  'lstm_9[25][1]',                \n",
            "                                                                  'lstm_9[26][1]',                \n",
            "                                                                  'lstm_9[27][1]',                \n",
            "                                                                  'lstm_9[28][1]',                \n",
            "                                                                  'lstm_9[29][1]',                \n",
            "                                                                  'lstm_9[30][1]',                \n",
            "                                                                  'lstm_9[31][1]',                \n",
            "                                                                  'lstm_9[32][1]',                \n",
            "                                                                  'lstm_9[33][1]',                \n",
            "                                                                  'lstm_9[34][1]',                \n",
            "                                                                  'lstm_9[35][1]',                \n",
            "                                                                  'lstm_9[36][1]',                \n",
            "                                                                  'lstm_9[37][1]',                \n",
            "                                                                  'lstm_9[38][1]',                \n",
            "                                                                  'lstm_9[39][1]',                \n",
            "                                                                  'lstm_9[40][1]',                \n",
            "                                                                  'lstm_9[41][1]',                \n",
            "                                                                  'lstm_9[42][1]',                \n",
            "                                                                  'lstm_9[43][1]',                \n",
            "                                                                  'lstm_9[44][1]',                \n",
            "                                                                  'lstm_9[45][1]',                \n",
            "                                                                  'lstm_9[46][1]',                \n",
            "                                                                  'lstm_9[47][1]',                \n",
            "                                                                  'lstm_9[48][1]',                \n",
            "                                                                  'lstm_9[49][1]',                \n",
            "                                                                  'lstm_9[50][1]',                \n",
            "                                                                  'lstm_9[51][1]',                \n",
            "                                                                  'lstm_9[52][1]',                \n",
            "                                                                  'lstm_9[53][1]',                \n",
            "                                                                  'lstm_9[54][1]',                \n",
            "                                                                  'lstm_9[55][1]',                \n",
            "                                                                  'lstm_9[56][1]',                \n",
            "                                                                  'lstm_9[57][1]',                \n",
            "                                                                  'lstm_9[58][1]',                \n",
            "                                                                  'lstm_9[59][1]',                \n",
            "                                                                  'lstm_9[60][1]',                \n",
            "                                                                  'lstm_9[61][1]',                \n",
            "                                                                  'lstm_9[62][1]',                \n",
            "                                                                  'lstm_9[63][1]',                \n",
            "                                                                  'lstm_9[64][1]',                \n",
            "                                                                  'lstm_9[65][1]',                \n",
            "                                                                  'lstm_9[66][1]',                \n",
            "                                                                  'lstm_9[67][1]',                \n",
            "                                                                  'lstm_9[68][1]',                \n",
            "                                                                  'lstm_9[69][1]',                \n",
            "                                                                  'lstm_9[70][1]',                \n",
            "                                                                  'lstm_9[71][1]',                \n",
            "                                                                  'lstm_9[72][1]',                \n",
            "                                                                  'lstm_9[73][1]',                \n",
            "                                                                  'lstm_9[74][1]',                \n",
            "                                                                  'lstm_9[75][1]',                \n",
            "                                                                  'lstm_9[76][1]',                \n",
            "                                                                  'lstm_9[77][1]',                \n",
            "                                                                  'lstm_9[78][1]',                \n",
            "                                                                  'lstm_9[79][1]',                \n",
            "                                                                  'lstm_9[80][1]',                \n",
            "                                                                  'lstm_9[81][1]',                \n",
            "                                                                  'lstm_9[82][1]',                \n",
            "                                                                  'lstm_9[83][1]',                \n",
            "                                                                  'lstm_9[84][1]',                \n",
            "                                                                  'lstm_9[85][1]',                \n",
            "                                                                  'lstm_9[86][1]',                \n",
            "                                                                  'lstm_9[87][1]',                \n",
            "                                                                  'lstm_9[88][1]',                \n",
            "                                                                  'lstm_9[89][1]',                \n",
            "                                                                  'lstm_9[90][1]',                \n",
            "                                                                  'lstm_9[91][1]',                \n",
            "                                                                  'lstm_9[92][1]',                \n",
            "                                                                  'lstm_9[93][1]',                \n",
            "                                                                  'lstm_9[94][1]',                \n",
            "                                                                  'lstm_9[95][1]',                \n",
            "                                                                  'lstm_9[96][1]',                \n",
            "                                                                  'lstm_9[97][1]']                \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 400, 128)     0           ['bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[0][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[1][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[2][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[3][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[4][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[5][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[6][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[7][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[8][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[9][0]',        \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[10][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[11][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[12][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[13][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[14][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[15][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[16][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[17][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[18][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[19][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[20][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[21][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[22][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[23][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[24][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[25][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[26][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[27][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[28][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[29][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[30][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[31][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[32][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[33][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[34][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[35][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[36][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[37][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[38][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[39][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[40][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[41][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[42][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[43][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[44][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[45][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[46][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[47][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[48][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[49][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[50][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[51][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[52][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[53][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[54][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[55][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[56][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[57][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[58][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[59][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[60][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[61][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[62][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[63][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[64][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[65][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[66][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[67][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[68][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[69][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[70][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[71][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[72][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[73][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[74][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[75][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[76][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[77][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[78][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[79][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[80][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[81][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[82][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[83][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[84][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[85][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[86][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[87][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[88][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[89][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[90][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[91][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[92][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[93][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[94][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[95][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[96][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[97][0]',       \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'repeat_vector_4[98][0]']       \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 400, 16)      2064        ['concatenate_7[0][0]',          \n",
            "                                                                  'concatenate_7[1][0]',          \n",
            "                                                                  'concatenate_7[2][0]',          \n",
            "                                                                  'concatenate_7[3][0]',          \n",
            "                                                                  'concatenate_7[4][0]',          \n",
            "                                                                  'concatenate_7[5][0]',          \n",
            "                                                                  'concatenate_7[6][0]',          \n",
            "                                                                  'concatenate_7[7][0]',          \n",
            "                                                                  'concatenate_7[8][0]',          \n",
            "                                                                  'concatenate_7[9][0]',          \n",
            "                                                                  'concatenate_7[10][0]',         \n",
            "                                                                  'concatenate_7[11][0]',         \n",
            "                                                                  'concatenate_7[12][0]',         \n",
            "                                                                  'concatenate_7[13][0]',         \n",
            "                                                                  'concatenate_7[14][0]',         \n",
            "                                                                  'concatenate_7[15][0]',         \n",
            "                                                                  'concatenate_7[16][0]',         \n",
            "                                                                  'concatenate_7[17][0]',         \n",
            "                                                                  'concatenate_7[18][0]',         \n",
            "                                                                  'concatenate_7[19][0]',         \n",
            "                                                                  'concatenate_7[20][0]',         \n",
            "                                                                  'concatenate_7[21][0]',         \n",
            "                                                                  'concatenate_7[22][0]',         \n",
            "                                                                  'concatenate_7[23][0]',         \n",
            "                                                                  'concatenate_7[24][0]',         \n",
            "                                                                  'concatenate_7[25][0]',         \n",
            "                                                                  'concatenate_7[26][0]',         \n",
            "                                                                  'concatenate_7[27][0]',         \n",
            "                                                                  'concatenate_7[28][0]',         \n",
            "                                                                  'concatenate_7[29][0]',         \n",
            "                                                                  'concatenate_7[30][0]',         \n",
            "                                                                  'concatenate_7[31][0]',         \n",
            "                                                                  'concatenate_7[32][0]',         \n",
            "                                                                  'concatenate_7[33][0]',         \n",
            "                                                                  'concatenate_7[34][0]',         \n",
            "                                                                  'concatenate_7[35][0]',         \n",
            "                                                                  'concatenate_7[36][0]',         \n",
            "                                                                  'concatenate_7[37][0]',         \n",
            "                                                                  'concatenate_7[38][0]',         \n",
            "                                                                  'concatenate_7[39][0]',         \n",
            "                                                                  'concatenate_7[40][0]',         \n",
            "                                                                  'concatenate_7[41][0]',         \n",
            "                                                                  'concatenate_7[42][0]',         \n",
            "                                                                  'concatenate_7[43][0]',         \n",
            "                                                                  'concatenate_7[44][0]',         \n",
            "                                                                  'concatenate_7[45][0]',         \n",
            "                                                                  'concatenate_7[46][0]',         \n",
            "                                                                  'concatenate_7[47][0]',         \n",
            "                                                                  'concatenate_7[48][0]',         \n",
            "                                                                  'concatenate_7[49][0]',         \n",
            "                                                                  'concatenate_7[50][0]',         \n",
            "                                                                  'concatenate_7[51][0]',         \n",
            "                                                                  'concatenate_7[52][0]',         \n",
            "                                                                  'concatenate_7[53][0]',         \n",
            "                                                                  'concatenate_7[54][0]',         \n",
            "                                                                  'concatenate_7[55][0]',         \n",
            "                                                                  'concatenate_7[56][0]',         \n",
            "                                                                  'concatenate_7[57][0]',         \n",
            "                                                                  'concatenate_7[58][0]',         \n",
            "                                                                  'concatenate_7[59][0]',         \n",
            "                                                                  'concatenate_7[60][0]',         \n",
            "                                                                  'concatenate_7[61][0]',         \n",
            "                                                                  'concatenate_7[62][0]',         \n",
            "                                                                  'concatenate_7[63][0]',         \n",
            "                                                                  'concatenate_7[64][0]',         \n",
            "                                                                  'concatenate_7[65][0]',         \n",
            "                                                                  'concatenate_7[66][0]',         \n",
            "                                                                  'concatenate_7[67][0]',         \n",
            "                                                                  'concatenate_7[68][0]',         \n",
            "                                                                  'concatenate_7[69][0]',         \n",
            "                                                                  'concatenate_7[70][0]',         \n",
            "                                                                  'concatenate_7[71][0]',         \n",
            "                                                                  'concatenate_7[72][0]',         \n",
            "                                                                  'concatenate_7[73][0]',         \n",
            "                                                                  'concatenate_7[74][0]',         \n",
            "                                                                  'concatenate_7[75][0]',         \n",
            "                                                                  'concatenate_7[76][0]',         \n",
            "                                                                  'concatenate_7[77][0]',         \n",
            "                                                                  'concatenate_7[78][0]',         \n",
            "                                                                  'concatenate_7[79][0]',         \n",
            "                                                                  'concatenate_7[80][0]',         \n",
            "                                                                  'concatenate_7[81][0]',         \n",
            "                                                                  'concatenate_7[82][0]',         \n",
            "                                                                  'concatenate_7[83][0]',         \n",
            "                                                                  'concatenate_7[84][0]',         \n",
            "                                                                  'concatenate_7[85][0]',         \n",
            "                                                                  'concatenate_7[86][0]',         \n",
            "                                                                  'concatenate_7[87][0]',         \n",
            "                                                                  'concatenate_7[88][0]',         \n",
            "                                                                  'concatenate_7[89][0]',         \n",
            "                                                                  'concatenate_7[90][0]',         \n",
            "                                                                  'concatenate_7[91][0]',         \n",
            "                                                                  'concatenate_7[92][0]',         \n",
            "                                                                  'concatenate_7[93][0]',         \n",
            "                                                                  'concatenate_7[94][0]',         \n",
            "                                                                  'concatenate_7[95][0]',         \n",
            "                                                                  'concatenate_7[96][0]',         \n",
            "                                                                  'concatenate_7[97][0]',         \n",
            "                                                                  'concatenate_7[98][0]']         \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 99)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 400, 1)       16          ['dense_16[0][0]',               \n",
            "                                                                  'dense_16[1][0]',               \n",
            "                                                                  'dense_16[2][0]',               \n",
            "                                                                  'dense_16[3][0]',               \n",
            "                                                                  'dense_16[4][0]',               \n",
            "                                                                  'dense_16[5][0]',               \n",
            "                                                                  'dense_16[6][0]',               \n",
            "                                                                  'dense_16[7][0]',               \n",
            "                                                                  'dense_16[8][0]',               \n",
            "                                                                  'dense_16[9][0]',               \n",
            "                                                                  'dense_16[10][0]',              \n",
            "                                                                  'dense_16[11][0]',              \n",
            "                                                                  'dense_16[12][0]',              \n",
            "                                                                  'dense_16[13][0]',              \n",
            "                                                                  'dense_16[14][0]',              \n",
            "                                                                  'dense_16[15][0]',              \n",
            "                                                                  'dense_16[16][0]',              \n",
            "                                                                  'dense_16[17][0]',              \n",
            "                                                                  'dense_16[18][0]',              \n",
            "                                                                  'dense_16[19][0]',              \n",
            "                                                                  'dense_16[20][0]',              \n",
            "                                                                  'dense_16[21][0]',              \n",
            "                                                                  'dense_16[22][0]',              \n",
            "                                                                  'dense_16[23][0]',              \n",
            "                                                                  'dense_16[24][0]',              \n",
            "                                                                  'dense_16[25][0]',              \n",
            "                                                                  'dense_16[26][0]',              \n",
            "                                                                  'dense_16[27][0]',              \n",
            "                                                                  'dense_16[28][0]',              \n",
            "                                                                  'dense_16[29][0]',              \n",
            "                                                                  'dense_16[30][0]',              \n",
            "                                                                  'dense_16[31][0]',              \n",
            "                                                                  'dense_16[32][0]',              \n",
            "                                                                  'dense_16[33][0]',              \n",
            "                                                                  'dense_16[34][0]',              \n",
            "                                                                  'dense_16[35][0]',              \n",
            "                                                                  'dense_16[36][0]',              \n",
            "                                                                  'dense_16[37][0]',              \n",
            "                                                                  'dense_16[38][0]',              \n",
            "                                                                  'dense_16[39][0]',              \n",
            "                                                                  'dense_16[40][0]',              \n",
            "                                                                  'dense_16[41][0]',              \n",
            "                                                                  'dense_16[42][0]',              \n",
            "                                                                  'dense_16[43][0]',              \n",
            "                                                                  'dense_16[44][0]',              \n",
            "                                                                  'dense_16[45][0]',              \n",
            "                                                                  'dense_16[46][0]',              \n",
            "                                                                  'dense_16[47][0]',              \n",
            "                                                                  'dense_16[48][0]',              \n",
            "                                                                  'dense_16[49][0]',              \n",
            "                                                                  'dense_16[50][0]',              \n",
            "                                                                  'dense_16[51][0]',              \n",
            "                                                                  'dense_16[52][0]',              \n",
            "                                                                  'dense_16[53][0]',              \n",
            "                                                                  'dense_16[54][0]',              \n",
            "                                                                  'dense_16[55][0]',              \n",
            "                                                                  'dense_16[56][0]',              \n",
            "                                                                  'dense_16[57][0]',              \n",
            "                                                                  'dense_16[58][0]',              \n",
            "                                                                  'dense_16[59][0]',              \n",
            "                                                                  'dense_16[60][0]',              \n",
            "                                                                  'dense_16[61][0]',              \n",
            "                                                                  'dense_16[62][0]',              \n",
            "                                                                  'dense_16[63][0]',              \n",
            "                                                                  'dense_16[64][0]',              \n",
            "                                                                  'dense_16[65][0]',              \n",
            "                                                                  'dense_16[66][0]',              \n",
            "                                                                  'dense_16[67][0]',              \n",
            "                                                                  'dense_16[68][0]',              \n",
            "                                                                  'dense_16[69][0]',              \n",
            "                                                                  'dense_16[70][0]',              \n",
            "                                                                  'dense_16[71][0]',              \n",
            "                                                                  'dense_16[72][0]',              \n",
            "                                                                  'dense_16[73][0]',              \n",
            "                                                                  'dense_16[74][0]',              \n",
            "                                                                  'dense_16[75][0]',              \n",
            "                                                                  'dense_16[76][0]',              \n",
            "                                                                  'dense_16[77][0]',              \n",
            "                                                                  'dense_16[78][0]',              \n",
            "                                                                  'dense_16[79][0]',              \n",
            "                                                                  'dense_16[80][0]',              \n",
            "                                                                  'dense_16[81][0]',              \n",
            "                                                                  'dense_16[82][0]',              \n",
            "                                                                  'dense_16[83][0]',              \n",
            "                                                                  'dense_16[84][0]',              \n",
            "                                                                  'dense_16[85][0]',              \n",
            "                                                                  'dense_16[86][0]',              \n",
            "                                                                  'dense_16[87][0]',              \n",
            "                                                                  'dense_16[88][0]',              \n",
            "                                                                  'dense_16[89][0]',              \n",
            "                                                                  'dense_16[90][0]',              \n",
            "                                                                  'dense_16[91][0]',              \n",
            "                                                                  'dense_16[92][0]',              \n",
            "                                                                  'dense_16[93][0]',              \n",
            "                                                                  'dense_16[94][0]',              \n",
            "                                                                  'dense_16[95][0]',              \n",
            "                                                                  'dense_16[96][0]',              \n",
            "                                                                  'dense_16[97][0]',              \n",
            "                                                                  'dense_16[98][0]']              \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 99, 32)       640000      ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " attention_weights (Activation)  (None, 400, 1)      0           ['dense_17[0][0]',               \n",
            "                                                                  'dense_17[1][0]',               \n",
            "                                                                  'dense_17[2][0]',               \n",
            "                                                                  'dense_17[3][0]',               \n",
            "                                                                  'dense_17[4][0]',               \n",
            "                                                                  'dense_17[5][0]',               \n",
            "                                                                  'dense_17[6][0]',               \n",
            "                                                                  'dense_17[7][0]',               \n",
            "                                                                  'dense_17[8][0]',               \n",
            "                                                                  'dense_17[9][0]',               \n",
            "                                                                  'dense_17[10][0]',              \n",
            "                                                                  'dense_17[11][0]',              \n",
            "                                                                  'dense_17[12][0]',              \n",
            "                                                                  'dense_17[13][0]',              \n",
            "                                                                  'dense_17[14][0]',              \n",
            "                                                                  'dense_17[15][0]',              \n",
            "                                                                  'dense_17[16][0]',              \n",
            "                                                                  'dense_17[17][0]',              \n",
            "                                                                  'dense_17[18][0]',              \n",
            "                                                                  'dense_17[19][0]',              \n",
            "                                                                  'dense_17[20][0]',              \n",
            "                                                                  'dense_17[21][0]',              \n",
            "                                                                  'dense_17[22][0]',              \n",
            "                                                                  'dense_17[23][0]',              \n",
            "                                                                  'dense_17[24][0]',              \n",
            "                                                                  'dense_17[25][0]',              \n",
            "                                                                  'dense_17[26][0]',              \n",
            "                                                                  'dense_17[27][0]',              \n",
            "                                                                  'dense_17[28][0]',              \n",
            "                                                                  'dense_17[29][0]',              \n",
            "                                                                  'dense_17[30][0]',              \n",
            "                                                                  'dense_17[31][0]',              \n",
            "                                                                  'dense_17[32][0]',              \n",
            "                                                                  'dense_17[33][0]',              \n",
            "                                                                  'dense_17[34][0]',              \n",
            "                                                                  'dense_17[35][0]',              \n",
            "                                                                  'dense_17[36][0]',              \n",
            "                                                                  'dense_17[37][0]',              \n",
            "                                                                  'dense_17[38][0]',              \n",
            "                                                                  'dense_17[39][0]',              \n",
            "                                                                  'dense_17[40][0]',              \n",
            "                                                                  'dense_17[41][0]',              \n",
            "                                                                  'dense_17[42][0]',              \n",
            "                                                                  'dense_17[43][0]',              \n",
            "                                                                  'dense_17[44][0]',              \n",
            "                                                                  'dense_17[45][0]',              \n",
            "                                                                  'dense_17[46][0]',              \n",
            "                                                                  'dense_17[47][0]',              \n",
            "                                                                  'dense_17[48][0]',              \n",
            "                                                                  'dense_17[49][0]',              \n",
            "                                                                  'dense_17[50][0]',              \n",
            "                                                                  'dense_17[51][0]',              \n",
            "                                                                  'dense_17[52][0]',              \n",
            "                                                                  'dense_17[53][0]',              \n",
            "                                                                  'dense_17[54][0]',              \n",
            "                                                                  'dense_17[55][0]',              \n",
            "                                                                  'dense_17[56][0]',              \n",
            "                                                                  'dense_17[57][0]',              \n",
            "                                                                  'dense_17[58][0]',              \n",
            "                                                                  'dense_17[59][0]',              \n",
            "                                                                  'dense_17[60][0]',              \n",
            "                                                                  'dense_17[61][0]',              \n",
            "                                                                  'dense_17[62][0]',              \n",
            "                                                                  'dense_17[63][0]',              \n",
            "                                                                  'dense_17[64][0]',              \n",
            "                                                                  'dense_17[65][0]',              \n",
            "                                                                  'dense_17[66][0]',              \n",
            "                                                                  'dense_17[67][0]',              \n",
            "                                                                  'dense_17[68][0]',              \n",
            "                                                                  'dense_17[69][0]',              \n",
            "                                                                  'dense_17[70][0]',              \n",
            "                                                                  'dense_17[71][0]',              \n",
            "                                                                  'dense_17[72][0]',              \n",
            "                                                                  'dense_17[73][0]',              \n",
            "                                                                  'dense_17[74][0]',              \n",
            "                                                                  'dense_17[75][0]',              \n",
            "                                                                  'dense_17[76][0]',              \n",
            "                                                                  'dense_17[77][0]',              \n",
            "                                                                  'dense_17[78][0]',              \n",
            "                                                                  'dense_17[79][0]',              \n",
            "                                                                  'dense_17[80][0]',              \n",
            "                                                                  'dense_17[81][0]',              \n",
            "                                                                  'dense_17[82][0]',              \n",
            "                                                                  'dense_17[83][0]',              \n",
            "                                                                  'dense_17[84][0]',              \n",
            "                                                                  'dense_17[85][0]',              \n",
            "                                                                  'dense_17[86][0]',              \n",
            "                                                                  'dense_17[87][0]',              \n",
            "                                                                  'dense_17[88][0]',              \n",
            "                                                                  'dense_17[89][0]',              \n",
            "                                                                  'dense_17[90][0]',              \n",
            "                                                                  'dense_17[91][0]',              \n",
            "                                                                  'dense_17[92][0]',              \n",
            "                                                                  'dense_17[93][0]',              \n",
            "                                                                  'dense_17[94][0]',              \n",
            "                                                                  'dense_17[95][0]',              \n",
            "                                                                  'dense_17[96][0]',              \n",
            "                                                                  'dense_17[97][0]',              \n",
            "                                                                  'dense_17[98][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dot_5 (Dot)                    (None, 1, 64)        0           ['attention_weights[0][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[1][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[2][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[3][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[4][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[5][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[6][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[7][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[8][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[9][0]',      \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[10][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[11][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[12][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[13][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[14][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[15][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[16][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[17][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[18][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[19][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[20][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[21][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[22][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[23][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[24][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[25][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[26][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[27][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[28][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[29][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[30][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[31][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[32][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[33][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[34][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[35][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[36][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[37][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[38][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[39][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[40][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[41][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[42][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[43][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[44][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[45][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[46][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[47][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[48][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[49][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[50][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[51][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[52][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[53][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[54][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[55][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[56][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[57][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[58][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[59][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[60][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[61][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[62][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[63][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[64][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[65][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[66][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[67][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[68][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[69][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[70][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[71][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[72][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[73][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[74][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[75][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[76][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[77][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[78][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[79][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[80][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[81][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[82][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[83][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[84][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[85][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[86][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[87][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[88][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[89][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[90][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[91][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[92][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[93][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[94][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[95][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[96][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[97][0]',     \n",
            "                                                                  'bidirectional_4[0][0]',        \n",
            "                                                                  'attention_weights[98][0]',     \n",
            "                                                                  'bidirectional_4[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 1, 96)        0           ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dot_5[0][0]']                  \n",
            "                                                                                                  \n",
            " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " lstm_9 (LSTM)                  [(None, 64),         41216       ['concatenate_8[0][0]',          \n",
            "                                 (None, 64),                      's0[0][0]',                     \n",
            "                                 (None, 64)]                      'c0[0][0]',                     \n",
            "                                                                  'concatenate_9[0][0]',          \n",
            "                                                                  'lstm_9[0][1]',                 \n",
            "                                                                  'lstm_9[0][2]',                 \n",
            "                                                                  'concatenate_10[0][0]',         \n",
            "                                                                  'lstm_9[1][1]',                 \n",
            "                                                                  'lstm_9[1][2]',                 \n",
            "                                                                  'concatenate_11[0][0]',         \n",
            "                                                                  'lstm_9[2][1]',                 \n",
            "                                                                  'lstm_9[2][2]',                 \n",
            "                                                                  'concatenate_12[0][0]',         \n",
            "                                                                  'lstm_9[3][1]',                 \n",
            "                                                                  'lstm_9[3][2]',                 \n",
            "                                                                  'concatenate_13[0][0]',         \n",
            "                                                                  'lstm_9[4][1]',                 \n",
            "                                                                  'lstm_9[4][2]',                 \n",
            "                                                                  'concatenate_14[0][0]',         \n",
            "                                                                  'lstm_9[5][1]',                 \n",
            "                                                                  'lstm_9[5][2]',                 \n",
            "                                                                  'concatenate_15[0][0]',         \n",
            "                                                                  'lstm_9[6][1]',                 \n",
            "                                                                  'lstm_9[6][2]',                 \n",
            "                                                                  'concatenate_16[0][0]',         \n",
            "                                                                  'lstm_9[7][1]',                 \n",
            "                                                                  'lstm_9[7][2]',                 \n",
            "                                                                  'concatenate_17[0][0]',         \n",
            "                                                                  'lstm_9[8][1]',                 \n",
            "                                                                  'lstm_9[8][2]',                 \n",
            "                                                                  'concatenate_18[0][0]',         \n",
            "                                                                  'lstm_9[9][1]',                 \n",
            "                                                                  'lstm_9[9][2]',                 \n",
            "                                                                  'concatenate_19[0][0]',         \n",
            "                                                                  'lstm_9[10][1]',                \n",
            "                                                                  'lstm_9[10][2]',                \n",
            "                                                                  'concatenate_20[0][0]',         \n",
            "                                                                  'lstm_9[11][1]',                \n",
            "                                                                  'lstm_9[11][2]',                \n",
            "                                                                  'concatenate_21[0][0]',         \n",
            "                                                                  'lstm_9[12][1]',                \n",
            "                                                                  'lstm_9[12][2]',                \n",
            "                                                                  'concatenate_22[0][0]',         \n",
            "                                                                  'lstm_9[13][1]',                \n",
            "                                                                  'lstm_9[13][2]',                \n",
            "                                                                  'concatenate_23[0][0]',         \n",
            "                                                                  'lstm_9[14][1]',                \n",
            "                                                                  'lstm_9[14][2]',                \n",
            "                                                                  'concatenate_24[0][0]',         \n",
            "                                                                  'lstm_9[15][1]',                \n",
            "                                                                  'lstm_9[15][2]',                \n",
            "                                                                  'concatenate_25[0][0]',         \n",
            "                                                                  'lstm_9[16][1]',                \n",
            "                                                                  'lstm_9[16][2]',                \n",
            "                                                                  'concatenate_26[0][0]',         \n",
            "                                                                  'lstm_9[17][1]',                \n",
            "                                                                  'lstm_9[17][2]',                \n",
            "                                                                  'concatenate_27[0][0]',         \n",
            "                                                                  'lstm_9[18][1]',                \n",
            "                                                                  'lstm_9[18][2]',                \n",
            "                                                                  'concatenate_28[0][0]',         \n",
            "                                                                  'lstm_9[19][1]',                \n",
            "                                                                  'lstm_9[19][2]',                \n",
            "                                                                  'concatenate_29[0][0]',         \n",
            "                                                                  'lstm_9[20][1]',                \n",
            "                                                                  'lstm_9[20][2]',                \n",
            "                                                                  'concatenate_30[0][0]',         \n",
            "                                                                  'lstm_9[21][1]',                \n",
            "                                                                  'lstm_9[21][2]',                \n",
            "                                                                  'concatenate_31[0][0]',         \n",
            "                                                                  'lstm_9[22][1]',                \n",
            "                                                                  'lstm_9[22][2]',                \n",
            "                                                                  'concatenate_32[0][0]',         \n",
            "                                                                  'lstm_9[23][1]',                \n",
            "                                                                  'lstm_9[23][2]',                \n",
            "                                                                  'concatenate_33[0][0]',         \n",
            "                                                                  'lstm_9[24][1]',                \n",
            "                                                                  'lstm_9[24][2]',                \n",
            "                                                                  'concatenate_34[0][0]',         \n",
            "                                                                  'lstm_9[25][1]',                \n",
            "                                                                  'lstm_9[25][2]',                \n",
            "                                                                  'concatenate_35[0][0]',         \n",
            "                                                                  'lstm_9[26][1]',                \n",
            "                                                                  'lstm_9[26][2]',                \n",
            "                                                                  'concatenate_36[0][0]',         \n",
            "                                                                  'lstm_9[27][1]',                \n",
            "                                                                  'lstm_9[27][2]',                \n",
            "                                                                  'concatenate_37[0][0]',         \n",
            "                                                                  'lstm_9[28][1]',                \n",
            "                                                                  'lstm_9[28][2]',                \n",
            "                                                                  'concatenate_38[0][0]',         \n",
            "                                                                  'lstm_9[29][1]',                \n",
            "                                                                  'lstm_9[29][2]',                \n",
            "                                                                  'concatenate_39[0][0]',         \n",
            "                                                                  'lstm_9[30][1]',                \n",
            "                                                                  'lstm_9[30][2]',                \n",
            "                                                                  'concatenate_40[0][0]',         \n",
            "                                                                  'lstm_9[31][1]',                \n",
            "                                                                  'lstm_9[31][2]',                \n",
            "                                                                  'concatenate_41[0][0]',         \n",
            "                                                                  'lstm_9[32][1]',                \n",
            "                                                                  'lstm_9[32][2]',                \n",
            "                                                                  'concatenate_42[0][0]',         \n",
            "                                                                  'lstm_9[33][1]',                \n",
            "                                                                  'lstm_9[33][2]',                \n",
            "                                                                  'concatenate_43[0][0]',         \n",
            "                                                                  'lstm_9[34][1]',                \n",
            "                                                                  'lstm_9[34][2]',                \n",
            "                                                                  'concatenate_44[0][0]',         \n",
            "                                                                  'lstm_9[35][1]',                \n",
            "                                                                  'lstm_9[35][2]',                \n",
            "                                                                  'concatenate_45[0][0]',         \n",
            "                                                                  'lstm_9[36][1]',                \n",
            "                                                                  'lstm_9[36][2]',                \n",
            "                                                                  'concatenate_46[0][0]',         \n",
            "                                                                  'lstm_9[37][1]',                \n",
            "                                                                  'lstm_9[37][2]',                \n",
            "                                                                  'concatenate_47[0][0]',         \n",
            "                                                                  'lstm_9[38][1]',                \n",
            "                                                                  'lstm_9[38][2]',                \n",
            "                                                                  'concatenate_48[0][0]',         \n",
            "                                                                  'lstm_9[39][1]',                \n",
            "                                                                  'lstm_9[39][2]',                \n",
            "                                                                  'concatenate_49[0][0]',         \n",
            "                                                                  'lstm_9[40][1]',                \n",
            "                                                                  'lstm_9[40][2]',                \n",
            "                                                                  'concatenate_50[0][0]',         \n",
            "                                                                  'lstm_9[41][1]',                \n",
            "                                                                  'lstm_9[41][2]',                \n",
            "                                                                  'concatenate_51[0][0]',         \n",
            "                                                                  'lstm_9[42][1]',                \n",
            "                                                                  'lstm_9[42][2]',                \n",
            "                                                                  'concatenate_52[0][0]',         \n",
            "                                                                  'lstm_9[43][1]',                \n",
            "                                                                  'lstm_9[43][2]',                \n",
            "                                                                  'concatenate_53[0][0]',         \n",
            "                                                                  'lstm_9[44][1]',                \n",
            "                                                                  'lstm_9[44][2]',                \n",
            "                                                                  'concatenate_54[0][0]',         \n",
            "                                                                  'lstm_9[45][1]',                \n",
            "                                                                  'lstm_9[45][2]',                \n",
            "                                                                  'concatenate_55[0][0]',         \n",
            "                                                                  'lstm_9[46][1]',                \n",
            "                                                                  'lstm_9[46][2]',                \n",
            "                                                                  'concatenate_56[0][0]',         \n",
            "                                                                  'lstm_9[47][1]',                \n",
            "                                                                  'lstm_9[47][2]',                \n",
            "                                                                  'concatenate_57[0][0]',         \n",
            "                                                                  'lstm_9[48][1]',                \n",
            "                                                                  'lstm_9[48][2]',                \n",
            "                                                                  'concatenate_58[0][0]',         \n",
            "                                                                  'lstm_9[49][1]',                \n",
            "                                                                  'lstm_9[49][2]',                \n",
            "                                                                  'concatenate_59[0][0]',         \n",
            "                                                                  'lstm_9[50][1]',                \n",
            "                                                                  'lstm_9[50][2]',                \n",
            "                                                                  'concatenate_60[0][0]',         \n",
            "                                                                  'lstm_9[51][1]',                \n",
            "                                                                  'lstm_9[51][2]',                \n",
            "                                                                  'concatenate_61[0][0]',         \n",
            "                                                                  'lstm_9[52][1]',                \n",
            "                                                                  'lstm_9[52][2]',                \n",
            "                                                                  'concatenate_62[0][0]',         \n",
            "                                                                  'lstm_9[53][1]',                \n",
            "                                                                  'lstm_9[53][2]',                \n",
            "                                                                  'concatenate_63[0][0]',         \n",
            "                                                                  'lstm_9[54][1]',                \n",
            "                                                                  'lstm_9[54][2]',                \n",
            "                                                                  'concatenate_64[0][0]',         \n",
            "                                                                  'lstm_9[55][1]',                \n",
            "                                                                  'lstm_9[55][2]',                \n",
            "                                                                  'concatenate_65[0][0]',         \n",
            "                                                                  'lstm_9[56][1]',                \n",
            "                                                                  'lstm_9[56][2]',                \n",
            "                                                                  'concatenate_66[0][0]',         \n",
            "                                                                  'lstm_9[57][1]',                \n",
            "                                                                  'lstm_9[57][2]',                \n",
            "                                                                  'concatenate_67[0][0]',         \n",
            "                                                                  'lstm_9[58][1]',                \n",
            "                                                                  'lstm_9[58][2]',                \n",
            "                                                                  'concatenate_68[0][0]',         \n",
            "                                                                  'lstm_9[59][1]',                \n",
            "                                                                  'lstm_9[59][2]',                \n",
            "                                                                  'concatenate_69[0][0]',         \n",
            "                                                                  'lstm_9[60][1]',                \n",
            "                                                                  'lstm_9[60][2]',                \n",
            "                                                                  'concatenate_70[0][0]',         \n",
            "                                                                  'lstm_9[61][1]',                \n",
            "                                                                  'lstm_9[61][2]',                \n",
            "                                                                  'concatenate_71[0][0]',         \n",
            "                                                                  'lstm_9[62][1]',                \n",
            "                                                                  'lstm_9[62][2]',                \n",
            "                                                                  'concatenate_72[0][0]',         \n",
            "                                                                  'lstm_9[63][1]',                \n",
            "                                                                  'lstm_9[63][2]',                \n",
            "                                                                  'concatenate_73[0][0]',         \n",
            "                                                                  'lstm_9[64][1]',                \n",
            "                                                                  'lstm_9[64][2]',                \n",
            "                                                                  'concatenate_74[0][0]',         \n",
            "                                                                  'lstm_9[65][1]',                \n",
            "                                                                  'lstm_9[65][2]',                \n",
            "                                                                  'concatenate_75[0][0]',         \n",
            "                                                                  'lstm_9[66][1]',                \n",
            "                                                                  'lstm_9[66][2]',                \n",
            "                                                                  'concatenate_76[0][0]',         \n",
            "                                                                  'lstm_9[67][1]',                \n",
            "                                                                  'lstm_9[67][2]',                \n",
            "                                                                  'concatenate_77[0][0]',         \n",
            "                                                                  'lstm_9[68][1]',                \n",
            "                                                                  'lstm_9[68][2]',                \n",
            "                                                                  'concatenate_78[0][0]',         \n",
            "                                                                  'lstm_9[69][1]',                \n",
            "                                                                  'lstm_9[69][2]',                \n",
            "                                                                  'concatenate_79[0][0]',         \n",
            "                                                                  'lstm_9[70][1]',                \n",
            "                                                                  'lstm_9[70][2]',                \n",
            "                                                                  'concatenate_80[0][0]',         \n",
            "                                                                  'lstm_9[71][1]',                \n",
            "                                                                  'lstm_9[71][2]',                \n",
            "                                                                  'concatenate_81[0][0]',         \n",
            "                                                                  'lstm_9[72][1]',                \n",
            "                                                                  'lstm_9[72][2]',                \n",
            "                                                                  'concatenate_82[0][0]',         \n",
            "                                                                  'lstm_9[73][1]',                \n",
            "                                                                  'lstm_9[73][2]',                \n",
            "                                                                  'concatenate_83[0][0]',         \n",
            "                                                                  'lstm_9[74][1]',                \n",
            "                                                                  'lstm_9[74][2]',                \n",
            "                                                                  'concatenate_84[0][0]',         \n",
            "                                                                  'lstm_9[75][1]',                \n",
            "                                                                  'lstm_9[75][2]',                \n",
            "                                                                  'concatenate_85[0][0]',         \n",
            "                                                                  'lstm_9[76][1]',                \n",
            "                                                                  'lstm_9[76][2]',                \n",
            "                                                                  'concatenate_86[0][0]',         \n",
            "                                                                  'lstm_9[77][1]',                \n",
            "                                                                  'lstm_9[77][2]',                \n",
            "                                                                  'concatenate_87[0][0]',         \n",
            "                                                                  'lstm_9[78][1]',                \n",
            "                                                                  'lstm_9[78][2]',                \n",
            "                                                                  'concatenate_88[0][0]',         \n",
            "                                                                  'lstm_9[79][1]',                \n",
            "                                                                  'lstm_9[79][2]',                \n",
            "                                                                  'concatenate_89[0][0]',         \n",
            "                                                                  'lstm_9[80][1]',                \n",
            "                                                                  'lstm_9[80][2]',                \n",
            "                                                                  'concatenate_90[0][0]',         \n",
            "                                                                  'lstm_9[81][1]',                \n",
            "                                                                  'lstm_9[81][2]',                \n",
            "                                                                  'concatenate_91[0][0]',         \n",
            "                                                                  'lstm_9[82][1]',                \n",
            "                                                                  'lstm_9[82][2]',                \n",
            "                                                                  'concatenate_92[0][0]',         \n",
            "                                                                  'lstm_9[83][1]',                \n",
            "                                                                  'lstm_9[83][2]',                \n",
            "                                                                  'concatenate_93[0][0]',         \n",
            "                                                                  'lstm_9[84][1]',                \n",
            "                                                                  'lstm_9[84][2]',                \n",
            "                                                                  'concatenate_94[0][0]',         \n",
            "                                                                  'lstm_9[85][1]',                \n",
            "                                                                  'lstm_9[85][2]',                \n",
            "                                                                  'concatenate_95[0][0]',         \n",
            "                                                                  'lstm_9[86][1]',                \n",
            "                                                                  'lstm_9[86][2]',                \n",
            "                                                                  'concatenate_96[0][0]',         \n",
            "                                                                  'lstm_9[87][1]',                \n",
            "                                                                  'lstm_9[87][2]',                \n",
            "                                                                  'concatenate_97[0][0]',         \n",
            "                                                                  'lstm_9[88][1]',                \n",
            "                                                                  'lstm_9[88][2]',                \n",
            "                                                                  'concatenate_98[0][0]',         \n",
            "                                                                  'lstm_9[89][1]',                \n",
            "                                                                  'lstm_9[89][2]',                \n",
            "                                                                  'concatenate_99[0][0]',         \n",
            "                                                                  'lstm_9[90][1]',                \n",
            "                                                                  'lstm_9[90][2]',                \n",
            "                                                                  'concatenate_100[0][0]',        \n",
            "                                                                  'lstm_9[91][1]',                \n",
            "                                                                  'lstm_9[91][2]',                \n",
            "                                                                  'concatenate_101[0][0]',        \n",
            "                                                                  'lstm_9[92][1]',                \n",
            "                                                                  'lstm_9[92][2]',                \n",
            "                                                                  'concatenate_102[0][0]',        \n",
            "                                                                  'lstm_9[93][1]',                \n",
            "                                                                  'lstm_9[93][2]',                \n",
            "                                                                  'concatenate_103[0][0]',        \n",
            "                                                                  'lstm_9[94][1]',                \n",
            "                                                                  'lstm_9[94][2]',                \n",
            "                                                                  'concatenate_104[0][0]',        \n",
            "                                                                  'lstm_9[95][1]',                \n",
            "                                                                  'lstm_9[95][2]',                \n",
            "                                                                  'concatenate_105[0][0]',        \n",
            "                                                                  'lstm_9[96][1]',                \n",
            "                                                                  'lstm_9[96][2]',                \n",
            "                                                                  'concatenate_106[0][0]',        \n",
            "                                                                  'lstm_9[97][1]',                \n",
            "                                                                  'lstm_9[97][2]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 1, 96)        0           ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dot_5[1][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7 (Sl  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_7[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dot_5[2][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8 (Sl  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_8[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dot_5[3][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9 (Sl  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_9[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dot_5[4][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_10[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[5][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_11[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[6][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_12[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[7][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_13 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_13[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[8][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_14 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_14[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[9][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_15 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_15[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[10][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_16 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_16[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[11][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_17 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_17[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[12][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[13][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[14][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[15][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_21 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_21[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[16][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_22 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_22[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[17][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_23 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_23[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[18][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[19][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[20][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[21][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_27 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_27[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[22][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_28 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_28[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[23][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_29 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_29[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[24][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_30 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_30[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[25][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_31 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_31[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[26][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_32 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_32[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[27][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_33 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_33[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[28][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_34 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_34[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[29][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_35 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_35[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[30][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_36 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_36[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[31][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_37 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_37[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[32][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_38 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_38[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[33][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_39 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_39[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[34][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_40 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_40[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[35][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_41 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_41[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[36][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_42 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_42[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[37][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_43 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_43[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[38][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_44 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_44[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[39][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_45 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_48 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_45[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[40][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_46 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_49 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_46[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[41][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_47 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_50 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_47[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[42][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_48 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_51 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_48[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[43][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_49 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_52 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_49[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[44][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_50 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_53 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_50[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[45][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_51 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_54 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_51[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[46][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_52 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_55 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_52[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[47][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_53 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_56 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_53[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[48][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_54 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_57 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_54[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[49][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_55 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_58 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_55[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[50][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_56 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_59 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_56[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[51][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_57 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_60 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_57[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[52][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_58 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_61 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_58[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[53][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_59 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_62 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_59[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[54][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_60 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_63 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_60[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[55][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_61 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_64 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_61[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[56][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_62 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_65 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_62[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[57][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_63 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_66 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_63[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[58][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_64 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_67 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_64[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[59][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_65 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_68 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_65[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[60][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_66 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_69 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_66[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[61][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_67 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_70 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_67[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[62][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_68 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_71 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_68[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[63][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_69 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_72 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_69[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[64][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_70 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_73 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_70[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[65][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_71 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_74 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_71[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[66][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_72 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_75 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_72[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[67][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_73 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_76 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_73[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[68][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_74 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_77 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_74[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[69][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_75 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_78 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_75[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[70][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_76 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_79 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_76[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[71][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_77 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_80 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_77[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[72][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_78 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_81 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_78[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[73][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_79 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_82 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_79[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[74][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_80 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_83 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_80[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[75][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_81 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_84 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_81[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[76][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_82 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_85 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_82[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[77][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_83 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_86 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_83[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[78][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_84 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_87 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_84[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[79][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_85 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_88 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_85[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[80][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_86 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_89 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_86[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[81][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_87 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_90 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_87[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[82][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_88 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_91 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_88[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[83][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_89 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_92 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_89[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[84][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_90 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_93 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_90[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[85][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_91 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_94 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_91[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[86][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_92 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_95 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_92[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[87][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_93 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_96 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_93[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[88][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_94 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_97 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_94[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[89][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_95 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_98 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_95[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[90][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_96 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_99 (Concatenate)   (None, 1, 96)        0           ['tf.__operators__.getitem_96[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[91][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_97 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_100 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_97[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[92][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_98 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_101 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_98[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[93][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_99 (S  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " concatenate_102 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_99[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'dot_5[94][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_100 (  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " concatenate_103 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_100[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'dot_5[95][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_101 (  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " concatenate_104 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_101[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'dot_5[96][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_102 (  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " concatenate_105 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_102[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'dot_5[97][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_103 (  (None, 1, 32)       0           ['embedding_9[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " concatenate_106 (Concatenate)  (None, 1, 96)        0           ['tf.__operators__.getitem_103[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'dot_5[98][0]']                 \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 16)           1040        ['lstm_9[0][1]',                 \n",
            "                                                                  'lstm_9[1][1]',                 \n",
            "                                                                  'lstm_9[2][1]',                 \n",
            "                                                                  'lstm_9[3][1]',                 \n",
            "                                                                  'lstm_9[4][1]',                 \n",
            "                                                                  'lstm_9[5][1]',                 \n",
            "                                                                  'lstm_9[6][1]',                 \n",
            "                                                                  'lstm_9[7][1]',                 \n",
            "                                                                  'lstm_9[8][1]',                 \n",
            "                                                                  'lstm_9[9][1]',                 \n",
            "                                                                  'lstm_9[10][1]',                \n",
            "                                                                  'lstm_9[11][1]',                \n",
            "                                                                  'lstm_9[12][1]',                \n",
            "                                                                  'lstm_9[13][1]',                \n",
            "                                                                  'lstm_9[14][1]',                \n",
            "                                                                  'lstm_9[15][1]',                \n",
            "                                                                  'lstm_9[16][1]',                \n",
            "                                                                  'lstm_9[17][1]',                \n",
            "                                                                  'lstm_9[18][1]',                \n",
            "                                                                  'lstm_9[19][1]',                \n",
            "                                                                  'lstm_9[20][1]',                \n",
            "                                                                  'lstm_9[21][1]',                \n",
            "                                                                  'lstm_9[22][1]',                \n",
            "                                                                  'lstm_9[23][1]',                \n",
            "                                                                  'lstm_9[24][1]',                \n",
            "                                                                  'lstm_9[25][1]',                \n",
            "                                                                  'lstm_9[26][1]',                \n",
            "                                                                  'lstm_9[27][1]',                \n",
            "                                                                  'lstm_9[28][1]',                \n",
            "                                                                  'lstm_9[29][1]',                \n",
            "                                                                  'lstm_9[30][1]',                \n",
            "                                                                  'lstm_9[31][1]',                \n",
            "                                                                  'lstm_9[32][1]',                \n",
            "                                                                  'lstm_9[33][1]',                \n",
            "                                                                  'lstm_9[34][1]',                \n",
            "                                                                  'lstm_9[35][1]',                \n",
            "                                                                  'lstm_9[36][1]',                \n",
            "                                                                  'lstm_9[37][1]',                \n",
            "                                                                  'lstm_9[38][1]',                \n",
            "                                                                  'lstm_9[39][1]',                \n",
            "                                                                  'lstm_9[40][1]',                \n",
            "                                                                  'lstm_9[41][1]',                \n",
            "                                                                  'lstm_9[42][1]',                \n",
            "                                                                  'lstm_9[43][1]',                \n",
            "                                                                  'lstm_9[44][1]',                \n",
            "                                                                  'lstm_9[45][1]',                \n",
            "                                                                  'lstm_9[46][1]',                \n",
            "                                                                  'lstm_9[47][1]',                \n",
            "                                                                  'lstm_9[48][1]',                \n",
            "                                                                  'lstm_9[49][1]',                \n",
            "                                                                  'lstm_9[50][1]',                \n",
            "                                                                  'lstm_9[51][1]',                \n",
            "                                                                  'lstm_9[52][1]',                \n",
            "                                                                  'lstm_9[53][1]',                \n",
            "                                                                  'lstm_9[54][1]',                \n",
            "                                                                  'lstm_9[55][1]',                \n",
            "                                                                  'lstm_9[56][1]',                \n",
            "                                                                  'lstm_9[57][1]',                \n",
            "                                                                  'lstm_9[58][1]',                \n",
            "                                                                  'lstm_9[59][1]',                \n",
            "                                                                  'lstm_9[60][1]',                \n",
            "                                                                  'lstm_9[61][1]',                \n",
            "                                                                  'lstm_9[62][1]',                \n",
            "                                                                  'lstm_9[63][1]',                \n",
            "                                                                  'lstm_9[64][1]',                \n",
            "                                                                  'lstm_9[65][1]',                \n",
            "                                                                  'lstm_9[66][1]',                \n",
            "                                                                  'lstm_9[67][1]',                \n",
            "                                                                  'lstm_9[68][1]',                \n",
            "                                                                  'lstm_9[69][1]',                \n",
            "                                                                  'lstm_9[70][1]',                \n",
            "                                                                  'lstm_9[71][1]',                \n",
            "                                                                  'lstm_9[72][1]',                \n",
            "                                                                  'lstm_9[73][1]',                \n",
            "                                                                  'lstm_9[74][1]',                \n",
            "                                                                  'lstm_9[75][1]',                \n",
            "                                                                  'lstm_9[76][1]',                \n",
            "                                                                  'lstm_9[77][1]',                \n",
            "                                                                  'lstm_9[78][1]',                \n",
            "                                                                  'lstm_9[79][1]',                \n",
            "                                                                  'lstm_9[80][1]',                \n",
            "                                                                  'lstm_9[81][1]',                \n",
            "                                                                  'lstm_9[82][1]',                \n",
            "                                                                  'lstm_9[83][1]',                \n",
            "                                                                  'lstm_9[84][1]',                \n",
            "                                                                  'lstm_9[85][1]',                \n",
            "                                                                  'lstm_9[86][1]',                \n",
            "                                                                  'lstm_9[87][1]',                \n",
            "                                                                  'lstm_9[88][1]',                \n",
            "                                                                  'lstm_9[89][1]',                \n",
            "                                                                  'lstm_9[90][1]',                \n",
            "                                                                  'lstm_9[91][1]',                \n",
            "                                                                  'lstm_9[92][1]',                \n",
            "                                                                  'lstm_9[93][1]',                \n",
            "                                                                  'lstm_9[94][1]',                \n",
            "                                                                  'lstm_9[95][1]',                \n",
            "                                                                  'lstm_9[96][1]',                \n",
            "                                                                  'lstm_9[97][1]',                \n",
            "                                                                  'lstm_9[98][1]']                \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 20000)        340000      ['dense_18[1][0]',               \n",
            "                                                                  'dense_18[3][0]',               \n",
            "                                                                  'dense_18[5][0]',               \n",
            "                                                                  'dense_18[7][0]',               \n",
            "                                                                  'dense_18[9][0]',               \n",
            "                                                                  'dense_18[11][0]',              \n",
            "                                                                  'dense_18[13][0]',              \n",
            "                                                                  'dense_18[15][0]',              \n",
            "                                                                  'dense_18[17][0]',              \n",
            "                                                                  'dense_18[19][0]',              \n",
            "                                                                  'dense_18[21][0]',              \n",
            "                                                                  'dense_18[23][0]',              \n",
            "                                                                  'dense_18[25][0]',              \n",
            "                                                                  'dense_18[27][0]',              \n",
            "                                                                  'dense_18[29][0]',              \n",
            "                                                                  'dense_18[31][0]',              \n",
            "                                                                  'dense_18[33][0]',              \n",
            "                                                                  'dense_18[35][0]',              \n",
            "                                                                  'dense_18[37][0]',              \n",
            "                                                                  'dense_18[39][0]',              \n",
            "                                                                  'dense_18[41][0]',              \n",
            "                                                                  'dense_18[43][0]',              \n",
            "                                                                  'dense_18[45][0]',              \n",
            "                                                                  'dense_18[47][0]',              \n",
            "                                                                  'dense_18[49][0]',              \n",
            "                                                                  'dense_18[51][0]',              \n",
            "                                                                  'dense_18[53][0]',              \n",
            "                                                                  'dense_18[55][0]',              \n",
            "                                                                  'dense_18[57][0]',              \n",
            "                                                                  'dense_18[59][0]',              \n",
            "                                                                  'dense_18[61][0]',              \n",
            "                                                                  'dense_18[63][0]',              \n",
            "                                                                  'dense_18[65][0]',              \n",
            "                                                                  'dense_18[67][0]',              \n",
            "                                                                  'dense_18[69][0]',              \n",
            "                                                                  'dense_18[71][0]',              \n",
            "                                                                  'dense_18[73][0]',              \n",
            "                                                                  'dense_18[75][0]',              \n",
            "                                                                  'dense_18[77][0]',              \n",
            "                                                                  'dense_18[79][0]',              \n",
            "                                                                  'dense_18[81][0]',              \n",
            "                                                                  'dense_18[83][0]',              \n",
            "                                                                  'dense_18[85][0]',              \n",
            "                                                                  'dense_18[87][0]',              \n",
            "                                                                  'dense_18[89][0]',              \n",
            "                                                                  'dense_18[91][0]',              \n",
            "                                                                  'dense_18[93][0]',              \n",
            "                                                                  'dense_18[95][0]',              \n",
            "                                                                  'dense_18[97][0]',              \n",
            "                                                                  'dense_18[99][0]',              \n",
            "                                                                  'dense_18[101][0]',             \n",
            "                                                                  'dense_18[103][0]',             \n",
            "                                                                  'dense_18[105][0]',             \n",
            "                                                                  'dense_18[107][0]',             \n",
            "                                                                  'dense_18[109][0]',             \n",
            "                                                                  'dense_18[111][0]',             \n",
            "                                                                  'dense_18[113][0]',             \n",
            "                                                                  'dense_18[115][0]',             \n",
            "                                                                  'dense_18[117][0]',             \n",
            "                                                                  'dense_18[119][0]',             \n",
            "                                                                  'dense_18[121][0]',             \n",
            "                                                                  'dense_18[123][0]',             \n",
            "                                                                  'dense_18[125][0]',             \n",
            "                                                                  'dense_18[127][0]',             \n",
            "                                                                  'dense_18[129][0]',             \n",
            "                                                                  'dense_18[131][0]',             \n",
            "                                                                  'dense_18[133][0]',             \n",
            "                                                                  'dense_18[135][0]',             \n",
            "                                                                  'dense_18[137][0]',             \n",
            "                                                                  'dense_18[139][0]',             \n",
            "                                                                  'dense_18[141][0]',             \n",
            "                                                                  'dense_18[143][0]',             \n",
            "                                                                  'dense_18[145][0]',             \n",
            "                                                                  'dense_18[147][0]',             \n",
            "                                                                  'dense_18[149][0]',             \n",
            "                                                                  'dense_18[151][0]',             \n",
            "                                                                  'dense_18[153][0]',             \n",
            "                                                                  'dense_18[155][0]',             \n",
            "                                                                  'dense_18[157][0]',             \n",
            "                                                                  'dense_18[159][0]',             \n",
            "                                                                  'dense_18[161][0]',             \n",
            "                                                                  'dense_18[163][0]',             \n",
            "                                                                  'dense_18[165][0]',             \n",
            "                                                                  'dense_18[167][0]',             \n",
            "                                                                  'dense_18[169][0]',             \n",
            "                                                                  'dense_18[171][0]',             \n",
            "                                                                  'dense_18[173][0]',             \n",
            "                                                                  'dense_18[175][0]',             \n",
            "                                                                  'dense_18[177][0]',             \n",
            "                                                                  'dense_18[179][0]',             \n",
            "                                                                  'dense_18[181][0]',             \n",
            "                                                                  'dense_18[183][0]',             \n",
            "                                                                  'dense_18[185][0]',             \n",
            "                                                                  'dense_18[187][0]',             \n",
            "                                                                  'dense_18[189][0]',             \n",
            "                                                                  'dense_18[191][0]',             \n",
            "                                                                  'dense_18[193][0]',             \n",
            "                                                                  'dense_18[195][0]',             \n",
            "                                                                  'dense_18[197][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,680,976\n",
            "Trainable params: 1,680,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}