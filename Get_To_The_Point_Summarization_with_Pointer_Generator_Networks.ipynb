{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/6YT6wh7llq0zHfjO+Tpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "J2_Q63jmdSp3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "x6QucUCZ0q2V"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "path_to_dailymail_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        "    extract=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr2kPGPSLi5m",
        "outputId": "29339e3b-8ad5-4652-ddc4-2413bf52464b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\n",
            "158577824/158577824 [==============================] - 3s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\n",
            "375893739/375893739 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories, path_to_dailymail_stories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uamve0operH0",
        "outputId": "7b214596-34e9-4c15-aca3-9754351c6cd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.keras/datasets/cnn_stories.tgz',\n",
              " '/root/.keras/datasets/dailymail_stories.tgz')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /root/.keras/datasets"
      ],
      "metadata": {
        "id": "WT84BmPll24S",
        "outputId": "a58205f0-fdfc-4041-f550-b1da33ae8bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 521960\n",
            "drwxr-xr-x 3 root root      4096 Jul 26 07:33 cnn\n",
            "-rw-r--r-- 1 root root 158577824 Jul 26 07:33 cnn_stories.tgz\n",
            "drwxr-xr-x 3 root root      4096 Jul 26 07:34 dailymail\n",
            "-rw-r--r-- 1 root root 375893739 Jul 26 07:34 dailymail_stories.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir = pathlib.Path('/root/.keras/datasets/cnn/stories')\n",
        "dailymail_stories_dir = pathlib.Path('/root/.keras/datasets/dailymail/stories')"
      ],
      "metadata": {
        "id": "ag5ubSt7l88l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir, dailymail_stories_dir"
      ],
      "metadata": {
        "id": "UmMCM--Yhm3B",
        "outputId": "17273a72-3060-4df7-a641-34d00df6888c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/root/.keras/datasets/cnn/stories'),\n",
              " PosixPath('/root/.keras/datasets/dailymail/stories'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_filenames(dir_path, num_files=5):\n",
        "  '''Prints the name of the files that are present at `dir_path`.\n",
        "  Maximum `num_files` number of files are shown.\n",
        "\n",
        "  Arguements:\n",
        "    dir_path: PosixPath, pointing to the directory of which the user\n",
        "              wants to prints the file names.\n",
        "    num_files: int, number of files user wants to print.\n",
        "\n",
        "  returns:\n",
        "    nothing\n",
        "  '''\n",
        "\n",
        "  count = 0\n",
        "  for f in dir_path.glob('*.story'):\n",
        "    print(f.name)\n",
        "    count += 1\n",
        "\n",
        "    if count == num_files:\n",
        "      break\n",
        "  else:\n",
        "    print(f\"Less than {num_files} is present!\")"
      ],
      "metadata": {
        "id": "dZFFXG7Pg86H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(cnn_stories_dir)"
      ],
      "metadata": {
        "id": "nv3g7-M316yL",
        "outputId": "6e3c32c9-8231-4dc8-8c9b-d56821d9a0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438411e10e1ef79b47cc48cd95296d85798c1e38.story\n",
            "e453e379e8a70af2d3dff1c75c41b0a35edbe9cc.story\n",
            "2079f35aca44978a7985afe0ddacdf02bedf98f2.story\n",
            "4702f28c198223157bb8f69665b039d560eebb0f.story\n",
            "db3e2ea79323a98379228b17cd3b9dec17dbd2cb.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(dailymail_stories_dir)"
      ],
      "metadata": {
        "id": "WcyW1wJa2EaN",
        "outputId": "9b194cc1-c5c8-461d-a904-0b4494c86077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f4ba18635997139c751311b9f2ad18f455dd7c98.story\n",
            "4a3ef32cff589c85ad0d22724e2ed747c0dacf87.story\n",
            "5375ed75939108c72001b043d3b4799c47f32be9.story\n",
            "fe9e57c21e21fb4ec26e394f0e92824f38d18a95.story\n",
            "6a544b5cdd2384be6cc657b265d7aa2de72a99e0.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
        "              dm_single_close_quote, dm_double_close_quote, \")\"]\n",
        "\n",
        "# Maximum stories to process from cnn and dailymail each\n",
        "MAX_STORIES = 50000\n",
        "\n",
        "# From the total data how to split into train, val and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VAL_SIZE = 0.1\n",
        "TEST_SIZE = 0.1\n",
        "\n",
        "# For tokenization\n",
        "VOCAB_SIZE = 20000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# For standardization\n",
        "PAD_TOKEN = '<PAD>'\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'"
      ],
      "metadata": {
        "id": "cubLAm1Q3SuG",
        "outputId": "4a4ff3c9-59ac-4465-c75f-9b31af6c1cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a sample .story file from cnn stories\n",
        "sample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\n",
        "sample_filedir = cnn_stories_dir\n",
        "\n",
        "sample_filepath = sample_filedir / sample_filename\n",
        "with open(sample_filepath, 'r') as f:\n",
        "  sample_story = f.read()\n",
        "\n",
        "print(sample_story)"
      ],
      "metadata": {
        "id": "Uw0kqchX3X7X",
        "outputId": "e27b4e70-bcd9-42db-fe8a-4baf4c68690f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\n",
            "\n",
            "The figure represents a 0.7% increase from last year, adding 2,250,129 people to the U.S. population since the start of 2011, and a 1.3% increase since Census Day, April 1, 2010.\n",
            "\n",
            "The agency estimates that beginning in January, one American will be born every eight seconds and one will die every 12 seconds.\n",
            "\n",
            "U.S.-bound immigrants are also expected to add one person every 46 seconds.\n",
            "\n",
            "That combination of births, deaths and migration is expected to add a single person to the U.S. population every 17 seconds, the Census Bureau said.\n",
            "\n",
            "Meanwhile, millions are set to ring in the new year.\n",
            "\n",
            "In New York, authorities are preparing for large crowds in Manhattan's Times Square, where Lady Gaga is expected to join Mayor Michael Bloomberg to push the button that drops the Waterford Crystal ball at 11:59 p.m. ET on New Year's Eve.\n",
            "\n",
            "\"And I'm so looking forward to performing on NYE+dropping the Ball with Mayor Bloomberg!\" the pop star posted on Twitter. \"What an honor as a New Yorker.\"\n",
            "\n",
            "Past guests have included Muhammad Ali, Rudy Giuliani, Colin Powell and Bill and Hillary Clinton.\n",
            "\n",
            "On Thursday, officials conducted New York's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above Times Square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square.\n",
            "\n",
            "The Big Apple this year edged out Las Vegas for the first time in seven years as the top travel U.S. destination for those celebrating the new year, according to a December travel booking website poll.\n",
            "\n",
            "Seven New York neighborhoods made the top 10 list, with two districts in Las Vegas and one in New Orleans making up the other three, according to the Priceline poll.\n",
            "\n",
            "\"It appears that New York City will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman Brian Ek.\n",
            "\n",
            "@highlight\n",
            "\n",
            "Census Bureau: U.S. population is expected to be 312.8 million on New Year's Day\n",
            "\n",
            "@highlight\n",
            "\n",
            "That figure represents a 0.7% increase from last year\n",
            "\n",
            "@highlight\n",
            "\n",
            "Lady Gaga, mayor to activate ball drop at Times Square on New Year's Eve\n",
            "\n",
            "@highlight\n",
            "\n",
            "NYC has supplanted Las Vegas as the top New Year's destination, Priceline poll says\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n",
        "\n",
        "These are the steps -\n",
        "1. Check if line contains `@highlight`, if True then just return the line.\n",
        "2. Check if line is empty, then return line as it is.\n",
        "3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n",
        "4. Only is none of the above conditions match then append `.` to the current line."
      ],
      "metadata": {
        "id": "iZPt8EVBzJ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_missing_period(line, end_tokens=END_TOKENS):\n",
        "  '''function to fix the missing periods for some story lines which do not end with\n",
        "  any of the end_tokens mentioned.\n",
        "\n",
        "  Arguements:\n",
        "    line: string, line of the story to fix the missing the period of.\n",
        "    end_tokens: list of strings, all the tokens that are considered as line end.\n",
        "\n",
        "  Returns:\n",
        "    new line with fixed the ending part by adding an ending token if not present.\n",
        "  '''\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  elif line == \"\":\n",
        "    return line\n",
        "  elif line[-1] in end_tokens:\n",
        "    return line\n",
        "\n",
        "  return line + '.'"
      ],
      "metadata": {
        "id": "i-S-Hss12TPk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fix_missing_period(sample_story.split('\\n')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L1hiNysxnmAO",
        "outputId": "d5cc58d6-7760-40ea-bc13-15871546b8df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `split_article_summary` which will split the story into article and summary parts.\n",
        "\n",
        "The function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n",
        "\n",
        "The steps to follow are -\n",
        "1. Split the story by new line `\\n`. I will get a list of lines.\n",
        "2. Strip the lines by using list comprehension.\n",
        "3. Use list comprehension to make lower case each line by using `.lower()`.\n",
        "4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n",
        "5. Make 2 empty list for `article` and `summary`.\n",
        "6. Go through each line. In each line, I need to check 4 things,\n",
        "  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n",
        "  * line is `\"\"` empty or not, if True then ignore.\n",
        "  * `next_highlight` is True or not, if True then append the line to `summary`.\n",
        "  * If non of the ebove then append to `article`.\n",
        "7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method."
      ],
      "metadata": {
        "id": "5-tqWtoowXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_article_summary(story):\n",
        "  '''Splits the story into 2 parts, one for article and other for summary of that\n",
        "  article. Returns the article and summary.\n",
        "\n",
        "  Arguements:\n",
        "    story: string file that contains both article and summary combiningly.\n",
        "\n",
        "  Returns:\n",
        "    article, summary seperately from the story.\n",
        "\n",
        "  '''\n",
        "  lines = story.split('\\n')\n",
        "  lines = [line.strip() for line in lines]\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  # Fix the ending period\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # List to contain the article and summary lines\n",
        "  article = []\n",
        "  summary = []\n",
        "\n",
        "  # Indicator of whether the next line is the summary or not\n",
        "  next_highlight = False\n",
        "\n",
        "  for line in lines:\n",
        "    if \"@highlight\" in line:\n",
        "      next_highlight = True\n",
        "    elif line==\"\":\n",
        "      continue\n",
        "    elif next_highlight:\n",
        "      summary.append(line)\n",
        "    else:\n",
        "      article.append(line)\n",
        "\n",
        "  article = ' '.join(article)\n",
        "  summary = ' '.join(summary)\n",
        "\n",
        "  return article, summary"
      ],
      "metadata": {
        "id": "-X4eMltQnf10"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_article_summary(sample_story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuUjOaN9orGU",
        "outputId": "64272077-d9c5-41c2-fe46-8bdcb2ea1cec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.',\n",
              " \"census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n",
        "\n",
        "This function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n",
        "\n",
        "The process is simple. We will follow this steps -\n",
        "1. Create 2 empty lists of `articles` and `summaries`.\n",
        "2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n",
        "3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n",
        "4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n",
        "5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n",
        "6. Return the 2 lists."
      ],
      "metadata": {
        "id": "oTEa0m3Huxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_summaries(stories_dir, max_stories):\n",
        "  '''stores the stories from stories_dir folder into a list and returns the list\n",
        "\n",
        "  Arguement:\n",
        "    stories_dir: Posix string, the directory where the stories are stored\n",
        "    max_stories: maximum number of stories to store\n",
        "\n",
        "  Returns:\n",
        "    list of stories.\n",
        "\n",
        "  '''\n",
        "  articles = []\n",
        "  summaries = []\n",
        "\n",
        "  count = 0\n",
        "  for f in stories_dir.glob(\"*.story\"):\n",
        "    count += 1\n",
        "    with open(f, 'r') as reader:\n",
        "      story = reader.read()\n",
        "\n",
        "      article, summary = split_article_summary(story)\n",
        "\n",
        "      articles.append(article)\n",
        "      summaries.append(summary)\n",
        "\n",
        "    if count == max_stories:\n",
        "      break\n",
        "\n",
        "  return articles, summaries"
      ],
      "metadata": {
        "id": "4VUmbYSpnjAr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "cnn\n",
        "  stories\n",
        "    438411e10e1ef79b47cc48cd95296d85798c1e38.story\n",
        "    e453e379e8a70af2d3dff1c75c41b0a35edbe9cc.story\n",
        "    2079f35aca44978a7985afe0ddacdf02bedf98f2.story\n",
        "    4702f28c198223157bb8f69665b039d560eebb0f.story\n",
        "    db3e2ea79323a98379228b17cd3b9dec17dbd2cb.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "dailymail\n",
        "  stories\n",
        "    f4ba18635997139c751311b9f2ad18f455dd7c98.story\n",
        "    4a3ef32cff589c85ad0d22724e2ed747c0dacf87.story\n",
        "    5375ed75939108c72001b043d3b4799c47f32be9.story\n",
        "    fe9e57c21e21fb4ec26e394f0e92824f38d18a95.story\n",
        "    6a544b5cdd2384be6cc657b265d7aa2de72a99e0.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them."
      ],
      "metadata": {
        "id": "Uk-BuCM4rIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)\n",
        "\n",
        "len(cnn_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9ZDQntpHQZ",
        "outputId": "bcb6bdb0-c5bd-4ff5-e72c-617db366cea9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q_i-69YqJnj",
        "outputId": "33ae5832-338f-4ccf-99da-fa5b76c45869"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year's eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan's times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year's eve. \"and i'm so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.\n",
            "\n",
            "\n",
            "The summary of this article: census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n",
        "                                                                 MAX_STORIES)"
      ],
      "metadata": {
        "id": "KT4Hrp6nqeKu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(dailymail_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {dailymail_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "id": "nkzwSP9kh4VK",
        "outputId": "f1717689-c270-44ed-f53a-6299d2051ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: by. damien gayle. published:. 19:51 est, 19 july 2013. |. updated:. 20:24 est, 19 july 2013. the families of some of the 22 primary school pupils who died after eating a school lunch contaminated with pesticides have buried the children in the school playground in protest at 'government negligence'. police have arrested the school cook following the poisoning which may have been part of a political feud between two branches of the same family, a local police chief said. nearly 50 people, the majority of whom were young children, became sick at the rural school in the eastern state of bihar after eating the food. almost half died. the incident followed recent provincial elections in which arsal khan khichi lost to his cousin jehanzaeb khan khichi, police chief sadiq dogar said late thursday. buried in the playground: villagers look on as three of the children who died after eating food contaminated with pesticide are buried in the grounds of the school. protest: villagers chose to bury the youngsters in the school playground in protest against what they describe as government negligence. tragic: villagers gather round as the bodies are buried following the poisoning. mass tragedy: villagers stand around the burial. mound of a child in front of the school where she was fed contaminated. food. police said the school's head had ignored warnings from the cook about the food. investigators believe that mustard oil used in the cooking had been contaminated with organophosphate pesticides. several of the parents buried their children in front of the school yesterday. madav ram, whose 12-year-old son rahul was one of the children who died, told the daily telegraph: \"we decided to bury our children in front of the school building to remind the government that they died because of their negligence. 'we also plan to raise a memorial in the memory of the dead children inside the school.' arsal khan khichi is accused of paying. a cook, mohammad rafiq, 50,000 rupees ($500) to poison food at his. rival's home on june 9, dogar said. nearly 50 people became sick and. were taken to the hospital, and 22 died. jehanzeb khan khichi was not at. home when the incident occurred, dogar said. rafiq has confessed to poisoning the. food, dogar said. police waited to arrest him until they received. medical reports that confirmed the dead had been poisoned. arsal khan. khichi is still on the run, and a murder case has been registered. against him as well, dogar said. it comes as it was claimed the. headmistress of the school is on the run, as police. claim she forced a reluctant cook to serve up the food. authorities say the cook had warned. meena devi, head of the rural school in the eastern state of bihar, that. the cooking oil may have been contaminated. she had complained that the oil looked strange and gave off a foil smell when heated, but her concerns were dismissed. grief: women mourn the death of their children who died after consuming the contaminated meals. family members of a school girl (not pictured) mourn her death: twenty-three youngsters died after eating the food, and dozens more remain in hospital after suffering sever poisoning. 'the headmistress said \"continue cooking and serve the food to the children\",' said police superintendent sujeet kumar. it is thought that many of the children who ate the meal may have been saved had the headmistress tasted the food before it was served, as required under the rules of the midday meal scheme. children are said to be far more vulnerable to this type of poisoning than adults as they have low body weight. soon. after the potato curry and rice was served at the school in rural. chappra, bihar, one of india's poorest states, pupils started. complaining of stomach cramps, then began to vomit and collapse. ninety. minutes later, the first victim, named as four-and-a-half year old anshu kumar, died on the way to hospital. death came. so quickly for some that they died in their parents' arms while being. taken to hospital. dozens of other children are being treated for food poisoning. authorities. discovered a container of pesticide in the school's cooking area next. to the vegetable oil and mustard oil, but it wasn't yet known if that. container was the source, said amarjeet sinha, a top official in bihar. villagers stand next to mass graves of the schoolchildren who died: authorities discovered a container of pesticide in the school's cooking area next to the vegetable oil and mustard oil. a villager walks past the graves: more answers are expected on friday, when a forensic laboratory is to issue the results of its tests on the dead children, the food and the uncooked grain stored by the teacher in her house. the clothes and shoes of a victim, placed by her family members on her grave: post-mortem reports on the children who died have confirmed that insecticide was either in the food or cooking oil. abandoned: a villager looks back towards the locked house of the headmistress of a school, who fled with her family after children started dying. the cooks said that the teacher controlled the food for the free daily lunch. 'it's. not a case of food poisoning. it's a case of poison in food in a large. quantity, going by the instant deaths,' mr sinha said. more answers are expected. when a forensic laboratory issues the results of its tests on the. dead children, the food and the uncooked grain stored by the. headmistress in her house, he said. police were searching for the principal, who fled after the students started falling sick, sinha said. local farmer ajay kumar's 5-year-old daughter was among the victims. he was in his house, which is about 100 metres from the school, when he heard screams from neighbours saying the children had fallen ill. 'i rushed there and all the kids were on their backs or clutching their stomachs or vomiting. i picked up my girl and took her to the local hospital right away,' ajay said. parents hitched rides or took public transport to get to the hospital, which is about nine miles from the village, he said. medical staff told them they had no medicines to give them. his daughter died shortly afterwards, writhing in pain on the floor. when reuters visited the village, there were at least 18 burial mounds, many in a large field opposite the school. some contained multiple bodies and villagers could not agree on how many children were buried in them. many parents said they buried their children's toys and clothes in the graves. the. cooks, manju devi and pano devi, told the associated press that the. principal controlled the food for the free daily lunch provided by the. government at the school. she. gave them rice, potatoes, soy and other ingredients needed to prepare. the meal for tuesday lunchtime and then went about her business. as the. children ate, they started fainting, the cooks said. the two cooks were not spared either. manju devi, 30, ate some of the food and fainted. her three children, ages five, eight and 13, fell ill as well. all were in stable condition thursday. while pano devi, 35, didn't eat the tainted food, her three children did. two of them died and the third, a four-year-old daughter, was in the hospital. 'i will stop cooking at the school,' she said. 'i am so horrified that i wouldn't grieve more if my only surviving child died.' mr sinha said one of the cooks told authorities that the cooking oil appeared different than usual, but the principal told her to use it anyway. doctors treating the children said they suspected the food had been contaminated with insecticide. media reports said the cooking oil may have been stored in an old pesticide container, but there was no independent confirmation of this. 'the minute the children were brought in, we smelled this foul odour of organophosphorus,' said dr. vinod mishra, a doctor in the medical team treating many of the children at patna medical college hospital in bihar's capital, patna. organophosphorus compounds are used as pesticides, which are widely available and are sold under a variety of different brands. dozens sick: schoolchildren receive treatment at a hospital after falling ill soon after eating free school meals. treatment: the meal was cooked in the school kitchen, but school authorities stopped serving it as children started vomiting. the free midday meal was served to the. children in gandamal village in masrakh block, 50 miles north. of patna, the bihar state capital. those. who survived the poison were unlikely to suffer from any serious after. effects from the tainted food, said patna medical college hospital. superintendent amarkant jha amar. 'there. will be no remnant effects on them. the effects of poisoning will be. washed after a certain period of time from the tissues,' mr amar said. he. said that the post-mortem reports on the children who died confirmed. that insecticide was either in the food or cooking oil. he said. authorities were waiting for lab results for more details on the. chemicals. horror: a father mourns as he holds his dead daughter today inside an ambulance, outside a hospital in patna, in the eastern indian state of bihar. location: it was not immediately clear how chemicals ended up in the food in a school in masrakh, near patna in bihar, although one official said the food may not have been properly washed before it was cooked. the mid-day. meal plan is one of the world's biggest school nutrition programs. state. governments have the freedom to decide on menus and timings of the. meals, depending on local conditions and availability of food rations. it. was first introduced in the sixties in southern india, where it was. seen as an incentive for poor parents to send their children to school. since. then, the program has spread across the country, covering some. 120million schoolchildren. it's part of an effort to address concerns. about malnutrition, which the government says nearly half of all indian. children suffer from. although. there have been complaints about the quality of the food served and the. lack of hygiene, the incident in bihar appeared to be unprecedented for. the massive food program.\n",
            "\n",
            "\n",
            "The summary of this article: teacher meena devi ran away with her family after children began to fall ill. she controlled the supplies for the school's daily free meal programme. cooks say they told her there seemed to be something wrong with the oil. doctors believe pupils were poisoned with organophosphorus pesticide.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1, 2] + [3, 4]"
      ],
      "metadata": {
        "id": "ToPn8sAtpyOX",
        "outputId": "262bd757-7748-473e-9afa-05df75d14d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0) # Keeps the shuffling same as before\n",
        "random.sample([1, 2, 3, 4, 5], 5)"
      ],
      "metadata": {
        "id": "eFZO1v0NqIJM",
        "outputId": "ffa0f320-b648-402b-dbb1-7423bc75ac2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article, sample_summary = split_article_summary(sample_story)"
      ],
      "metadata": {
        "id": "ZJCTOgIJmyXh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article"
      ],
      "metadata": {
        "id": "STljZePPnGy9",
        "outputId": "e9719097-9736-455e-aa06-60dd5c0de9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating another function -\n",
        "`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples."
      ],
      "metadata": {
        "id": "8ea-PhS3iIJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_size, val_size, test_size):\n",
        "  first_split = train_size\n",
        "  second_split = train_size+val_size\n",
        "  third_split = train_size+val_size+test_size\n",
        "  return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]"
      ],
      "metadata": {
        "id": "v-iZDbUCqkdC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize the 4 functions created above into one function called `make_datasets`. This function will -\n",
        "1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\n",
        "Objective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n",
        "```python\n",
        "[1, 2] + [3, 4] = [1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n",
        "```python\n",
        "  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n",
        "```\n",
        "4. Shuffle the dataset using `random.sample` method.\n",
        "```python\n",
        "random.seed(seed_value) # To make sure that everytime it gives the same shuffle\n",
        "random.sample(list_to_shuffle, len(list_to_shuffle))\n",
        "```\n",
        "5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`."
      ],
      "metadata": {
        "id": "FTB5eNzJrqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n",
        "  '''Create 3 datasets each for training, validation and testing respectively.\n",
        "  This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n",
        "  them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n",
        "  summaries using random.sample method (although we have a helper function for it). Finally we do the\n",
        "  splitting of the whole dataset. Remember here the returned values become tensors.\n",
        "\n",
        "  Arguements:\n",
        "    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n",
        "    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n",
        "    train_size: float, specifying how much fraction of the original dataset to take for training.\n",
        "    val_size: float, specifying how much fraction of the original dataset to take for validation.\n",
        "    test_size: float, specifying how much fraction of the original dataset to take for testing.\n",
        "\n",
        "  Returns:\n",
        "    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n",
        "    with the specified amount of data in it.\n",
        "    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n",
        "  '''\n",
        "  articles = cnn_stories[0] + dailymail_stories[0]\n",
        "  summaries = cnn_stories[1] + dailymail_stories[1]\n",
        "\n",
        "  articles = np.array(articles, dtype=object)\n",
        "  summaries = np.array(summaries, dtype=object)\n",
        "\n",
        "  dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n",
        "\n",
        "  random.seed(seed_value)\n",
        "  shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n",
        "\n",
        "  dataset = dataset[shuffled_indices, :]\n",
        "\n",
        "  train_size = int(train_fraction * dataset.shape[0])\n",
        "  val_size = int(val_fraction * dataset.shape[0])\n",
        "  test_size = dataset.shape[0] - (train_size + val_size)\n",
        "\n",
        "  training_samples, validation_samples, testing_samples = split_dataset(dataset,\n",
        "                                                                        train_size,\n",
        "                                                                        val_size,\n",
        "                                                                        test_size)\n",
        "\n",
        "  return (training_samples, validation_samples, testing_samples)"
      ],
      "metadata": {
        "id": "QDB0_32RrnHk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ],
      "metadata": {
        "id": "GTnXBwd6Sa-U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n",
        "\n",
        "print(f\"Training dataset shape: {train_dataset.shape}\")\n",
        "print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
        "print(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n",
        "\n",
        "print(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")"
      ],
      "metadata": {
        "id": "AyRVodwIhkuQ",
        "outputId": "e6f51838-25a8-4adf-d863-a029a6214fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the datasets: <class 'numpy.ndarray'>\n",
            "\n",
            "Training dataset shape: (80000, 2)\n",
            "Validation dataset shape: (10000, 2)\n",
            "Testing dataset shape: (10000, 2)\n",
            "\n",
            "First example in the training dataset looks like: \n",
            " [\"by. sean poulter. a skin cancer warning has been sounded following evidence some big brand sun creams do not offer the protection they claim to give. as temperatures are predicted to hit the mid-70s this weekend, products by malibu, piz buin and hawaiian tropic have been given a ‘don’t buy’ rating by independent reviewer which?. the consumer champion tested 15 popular sun creams that claimed to have a sun protection factor (spf) of 30, as well as helping to shield the skin from uva rays. the end for lotion? the new drinkable liquid offers up to factor 30 protection. the three. which failed were found to have a real spf level of less than 25, while. the malibu product also fell short of the required uva standard. sun exposure is linked to premature ageing and skin cancer, with evidence of 100,000 cases being diagnosed in the uk each year. which? found that paying a high price is no guarantee of better protection,. with the most expensive brand outperformed by a cheap rival. piz. buin ultra light dry touch sun fluid spf30, which failed the spf test,. was the most expensive cream tested, at £11.30 per 100ml. calypso sun. lotion spf30, which was £1.20 per 100ml, passed both tests. which? executive director richard lloyd said: ‘we’ve found three products that. failed the strict british standard tests and we want to see. manufacturers doing much more to make sure their sun creams live up to. the claims on the packaging.’ all the companies behind the brands criticised by which? insist the products meet the protection levels claimed on the packs. researchers. tested the creams on ten volunteers, who had the same amount of each. product applied to a small area of their back before lying under a lamp. to replicate the sun’s rays. their skin was checked for redness and. compared to the results of a lamp test with no cream. to test the suncream which? apply a small amount to an area on a person's back before a special lamp is shone on the patch and a lab assistant checks for redness. spf is a measure of how much longer someone will be protected from uvb rays than someone wearing no cream. if the sun reddens unprotected skin in ten minutes, for example, a person wearing spf15 can be exposed to the sun without reddening for 15 times longer – 150 minutes. while uvb rays are behind the redness of sunburn, uva rays damage the skin’s dna at a deeper layer. both radiation types can cause skin cancer. the eu recommendation is for sunscreens to offer a uva protection factor that is a third of their spf. the malibu product failed to meet this standard. the world health organisation advises applying 35ml of sun cream. that equates to seven teaspoons – one for the head and neck, one for each arm and leg, and one each for the chest and back. it suggests users re-apply every two hours. which? said: ‘a sunscreen that doesn’t offer the protection it claims could expose you to a greater risk of developing sunburn than one that meets its claim, as you will be getting less protection than you think.’ malibu said: ‘we stand by our testing\\u2009…\\u2009we have nothing to hide.’ piz buin said all of its products are assessed in compliance with eu regulations and that it is confident its products provide the protection stated. hawaiian tropic said it only sells products that meet the spf figure claimed on the pack. temperatures are expected to reach a sweltering 25c (77f) this weekend. pictured: plymouth hoe, devon. as summer arrives at last, nature is waking up to celebrate. the glorious weather expected this weekend comes after a mild and wet winter that has provided perfect conditions for plants and wildlife to thrive. as a result, many species are stirring earlier than expected. elderflowers are in flower, dog roses are blooming and hawthorn is at a peak – a situation not usually seen until june. far more butterflies have also hatched than would usually be seen at this time of year. most of britain will be bathed in sunshine at the weekend as the mercury hits its record for the year. there will be barely a cloud in the sky for long spells as temperatures reach a sweltering 25c (77f), warmer than many parts of the mediterranean and much hotter than the 16c (61f) average for may. the hot spell will last until monday but will break before the chelsea flower show starts on tuesday, with unsettled weather expected for much of next week. national trust naturalist matthew oates said: ‘it’s an early summer because of the mild, wet and stormy winter, and the whole thing’s rather jumped the gun.’ he added: ‘if the jet stream is kind to us, it could be a really wonderful summer.’\"\n",
            " \"products by malibu, piz buin and hawaiian tropic given 'don't buy' rating. consumer champion which? tested spf of 15 popular brand sun creams. study found more expensive lotions does not guarantee better protection.\"]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n",
        "1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n",
        "\n",
        "2. Replace the single and double opening and closing quotes like `‘ → \\u2018`, `’ → \\u2019`, `“ → \\u201c` and `” → \\u201d` by `'` and `\"` respectively.\n",
        "\n",
        "3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\n",
        "In this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n",
        "\n",
        "4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n",
        "\n",
        "`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n",
        "1. Standardize the text with `standardizer`.\n",
        "\n",
        "2. Remove unwanted spaces in between words.\n",
        "\n",
        "3. Split the text into words which are seperated by ' '.\n",
        "\n",
        "4. Strip each of the words in the sentence. Finally, return it."
      ],
      "metadata": {
        "id": "TcZCpJ9hCyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the text data\n",
        "def standardizer(text):\n",
        "  # Lower case the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Replace the special single and double opening and closing quotes\n",
        "  text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n",
        "  text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n",
        "\n",
        "  # Add space before punctuations and ignore floating point numbers.\n",
        "  text = re.sub(r'(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n",
        "                  r' \\1 ', text)\n",
        "\n",
        "  # Remove spaces after sentence end and other unwanted spaces from text\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# custom analyzer for the Tokenizer class\n",
        "def custom_analyzer(text):\n",
        "  text = standardizer(text)\n",
        "\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  words = text.split(' ')\n",
        "  words = [word.strip() for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "FD2h0OiAxJP_",
        "outputId": "33029258-6d55-401c-8279-5d70631695fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n",
        "                \"U.S won the world cup and bagged 1.78 million dollars.\",\n",
        "                \"India had M.S. Dhoni won made it this far.\",\n",
        "                \"My email address is arupjana7365@gmail.com.\",\n",
        "                \"It can take care of dailymail single opening quote’ also.\",\n",
        "                \"I have 10,000 Rs in my bank\"]\n",
        "\n",
        "[standardizer(text) for text in sample_texts]"
      ],
      "metadata": {
        "id": "AVi3kZhcLXS7",
        "outputId": "e5e83a4a-ca08-4c46-b86b-add974861e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i have been working on , but never did it in this way .',\n",
              " 'u . s won the world cup and bagged 1.78 million dollars .',\n",
              " 'india had m . s . dhoni won made it this far .',\n",
              " 'my email address is arupjana7365@gmail . com .',\n",
              " \"it can take care of dailymail single opening quote ' also .\",\n",
              " 'i have 10,000 rs in my bank']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[custom_analyzer(text) for text in sample_texts]"
      ],
      "metadata": {
        "id": "soJCwgTn3a6U",
        "outputId": "e78f1cee-58a6-4e81-f5d7-dc265d49cfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i',\n",
              "  'have',\n",
              "  'been',\n",
              "  'working',\n",
              "  'on',\n",
              "  ',',\n",
              "  'but',\n",
              "  'never',\n",
              "  'did',\n",
              "  'it',\n",
              "  'in',\n",
              "  'this',\n",
              "  'way',\n",
              "  '.'],\n",
              " ['u',\n",
              "  '.',\n",
              "  's',\n",
              "  'won',\n",
              "  'the',\n",
              "  'world',\n",
              "  'cup',\n",
              "  'and',\n",
              "  'bagged',\n",
              "  '1.78',\n",
              "  'million',\n",
              "  'dollars',\n",
              "  '.'],\n",
              " ['india',\n",
              "  'had',\n",
              "  'm',\n",
              "  '.',\n",
              "  's',\n",
              "  '.',\n",
              "  'dhoni',\n",
              "  'won',\n",
              "  'made',\n",
              "  'it',\n",
              "  'this',\n",
              "  'far',\n",
              "  '.'],\n",
              " ['my', 'email', 'address', 'is', 'arupjana7365@gmail', '.', 'com', '.'],\n",
              " ['it',\n",
              "  'can',\n",
              "  'take',\n",
              "  'care',\n",
              "  'of',\n",
              "  'dailymail',\n",
              "  'single',\n",
              "  'opening',\n",
              "  'quote',\n",
              "  \"'\",\n",
              "  'also',\n",
              "  '.'],\n",
              " ['i', 'have', '10,000', 'rs', 'in', 'my', 'bank']]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n",
        "\n",
        "When fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab."
      ],
      "metadata": {
        "id": "IiiWRFR64jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(texts, num_words, oov_token, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n",
        "  # Create the tokenizer usinf Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=num_words,\n",
        "                        filters=filters,\n",
        "                        oov_token=oov_token,\n",
        "                        analyzer=custom_analyzer)\n",
        "\n",
        "  # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n",
        "  texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n",
        "\n",
        "  # Fit the data with fit_on_texts method\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "tR1FD3SESd0G",
        "outputId": "71081da2-d981-46b5-f96b-91e1fea0e55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")"
      ],
      "metadata": {
        "id": "UsmErFoxqimM",
        "outputId": "15035484-faeb-45f9-b8a2-e99523968df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length the articles dataset: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)"
      ],
      "metadata": {
        "id": "151rEpqo7Pof",
        "outputId": "33cada55-6d05-4042-ce50-259ddd6f7c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\")\n",
        "print(f\"'<OOV>' word had index: {tokenizer.word_index['<OOV>']}\")\n",
        "print(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\")\n",
        "\n",
        "print(f\"Text to Sequence of the first article is {tokenizer.texts_to_sequences([train_dataset[0, 0]])}\")\n",
        "\n",
        "sample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\n",
        "print(f\"Sequence to Text of the first acrticle is {tokenizer.sequences_to_texts(sample_sequence)}\")"
      ],
      "metadata": {
        "id": "YUNreIjr8Kng",
        "outputId": "8647aeab-a9a5-46d0-a4f6-231072fb241a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary for the tokenizer has a length 251628\n",
            "'<OOV>' word had index: 1\n",
            "'teacher' word has index: 1571\n",
            "Text to Sequence of the first article is [[29, 2, 3483, 9984, 2, 7, 1468, 695, 1528, 32, 46, 6098, 309, 484, 89, 298, 1669, 1356, 16656, 96, 40, 1010, 3, 1597, 35, 872, 6, 371, 2, 24, 2311, 34, 3652, 6, 380, 3, 1, 38, 791, 4, 1328, 29, 14264, 4, 1, 1, 8, 12231, 1, 28, 46, 346, 7, 5, 157, 5, 51, 1124, 5, 4699, 29, 1496, 1, 56, 98, 2, 3, 2945, 1393, 2682, 1, 805, 1356, 16656, 14, 558, 6, 28, 7, 1356, 1597, 2506, 50, 19052, 49, 9, 1, 24, 149, 24, 1406, 6, 6143, 3, 1468, 30, 1, 7297, 2, 3, 230, 15, 1, 98, 3, 68, 1, 4983, 1953, 58, 6, 2506, 1, 1597, 2, 3, 108, 2, 56, 801, 45, 132, 6, 28, 7, 342, 19052, 663, 9, 433, 75, 1, 102, 2, 3, 14264, 1790, 67, 1105, 778, 9, 3, 1703, 1, 1997, 2, 1356, 3967, 18, 1679, 6, 7876, 9555, 8, 1468, 695, 4, 22, 484, 9, 1, 704, 97, 2150, 10, 3, 414, 270, 59, 2, 56, 98, 132, 14, 1827, 7, 186, 960, 18, 79, 4765, 9, 363, 1597, 4, 2, 22, 3, 114, 1969, 1669, 1, 29, 7, 4113, 2608, 2, 1, 2, 1, 6542, 744, 3273, 1787, 1356, 6632, 1, 56, 801, 3, 19052, 834, 4, 2, 16, 3, 114, 1969, 3959, 2682, 4, 26, 1, 232, 1, 2, 1, 1356, 2, 1, 1, 56, 16, 1, 232, 1, 4, 1070, 189, 1620, 2, 56, 98, 896, 475, 1253, 4860, 21, 23, 5, 42, 5, 211, 132, 108, 1328, 14, 2, 801, 3, 4085, 325, 1997, 1620, 8, 42, 182, 6, 165, 2, 5488, 443, 172, 57, 6, 134, 580, 44, 1356, 16656, 408, 58, 6, 2, 3, 538, 17, 3, 8875, 2, 5, 65, 3, 945, 364, 3, 4265, 3913, 29, 56, 98, 5530, 3, 1328, 947, 3, 1597, 1312, 558, 17, 3, 9700, 2, 1400, 2, 2682, 3, 16656, 17, 1225, 3254, 4, 41, 43, 3, 207, 1232, 9, 270, 2, 1790, 3331, 6, 7, 450, 321, 9, 44, 106, 94, 2426, 178, 7, 16480, 2, 6, 12126, 3, 1356, 5, 13, 7297, 2, 44, 1468, 16, 3759, 15, 1, 8, 2, 1484, 6, 3, 1057, 9, 7, 16480, 834, 22, 79, 3959, 2, 6, 834, 3, 1, 56, 98, 3160, 7, 450, 1232, 6, 37, 321, 17, 7, 399, 5, 13, 106, 94, 7, 678, 16480, 18, 17316, 17, 3, 7053, 8, 7, 4409, 1797, 3153, 15, 1, 2, 19052, 18, 7, 2029, 9, 111, 172, 860, 526, 48, 31, 3636, 30, 1, 7297, 75, 526, 962, 79, 3959, 2, 74, 3, 1356, 1, 1, 1468, 10, 1225, 459, 4, 15, 1127, 4, 7, 399, 962, 1, 66, 31, 2771, 6, 3, 1356, 271, 1, 15, 1, 245, 860, 239, 1, 459, 2, 102, 1, 7297, 34, 364, 3, 1, 9, 1, 4, 1, 7297, 1069, 3, 1468, 5, 13, 2374, 26, 7, 5733, 6958, 2, 189, 4849, 3319, 66, 762, 1468, 695, 2, 3, 1786, 8238, 18, 15, 1, 6, 1010, 7, 1, 1597, 2506, 14, 18, 7, 455, 9, 44, 19052, 2, 3, 14264, 1790, 801, 6, 947, 38, 1997, 2, 3, 95, 236, 3721, 9621, 7199, 1, 9, 1356, 3959, 2, 14, 1, 6, 490, 1, 239, 52, 15, 3, 292, 8, 2262, 4, 52, 15, 270, 1928, 8, 1598, 4, 8, 52, 270, 15, 3, 2907, 8, 106, 2, 19, 2263, 989, 146, 11, 3160, 227, 69, 315, 2, 56, 98, 21, 23, 5, 7, 18308, 14, 497, 5, 51, 1010, 3, 1597, 19, 538, 91, 8007, 53, 6, 7, 1579, 641, 9, 2327, 1, 75, 52, 14, 5644, 83, 872, 4, 24, 53, 48, 31, 448, 433, 1597, 75, 53, 163, 2, 5, 14264, 21, 23, 5, 42, 857, 29, 101, 1, 28, 566, 6, 3643, 2, 5, 1, 1, 21, 65, 9, 83, 1328, 34, 8346, 10, 8584, 22, 1786, 3497, 8, 14, 19, 18, 2250, 83, 1328, 836, 3, 1597, 2981, 2, 12231, 1, 21, 19, 105, 6389, 1328, 14, 947, 3, 19052, 1252, 558, 17, 3, 3851, 2, 2311, 34, 496, 6, 1079, 7, 19149, 1, 1, 49, 38, 791, 2, 254, 23, 8506, 1, 4, 5191, 2, 24, 662, 5133, 26, 80, 4, 1619, 18, 9151, 58, 6, 2157, 2, 3, 9467, 1040, 496, 38, 791, 519, 47, 7, 7423, 8, 4891, 1565, 14, 32, 1444, 1461, 1078, 15, 3529, 8, 2988, 6, 9855, 2, 24, 7, 707, 4, 123, 2356, 34, 13845, 356, 75, 496, 2, 1, 34, 10, 5768, 4, 909, 9994, 34, 1, 8, 1, 18, 26, 7, 3590, 239, 7, 726, 40, 1546, 237, 281, 578, 2, 333, 57, 18960, 28, 67, 16597, 75, 64, 1546, 31, 237, 26, 38, 71, 9, 59, 2, 114, 9, 522, 48, 31, 1, 10, 5999, 26, 3, 791, 24, 3, 8171, 3356, 83, 544, 15, 3, 59, 2, 62, 48, 31, 3548, 7, 4156, 10, 3, 1664, 15, 179, 9047, 24, 2311, 1079, 7, 19149, 1, 1, 49, 4, 8769, 75, 123, 1085, 9, 3, 5956, 8, 172, 18795, 75, 3, 1, 1, 49, 1047, 15, 130, 2, 3, 1503, 4610, 48, 80, 281, 262, 33, 48, 870, 94, 3, 773, 5768, 194, 2746, 17, 267, 4, 22, 15251, 1040, 496, 15, 172, 9, 196, 158, 2, 213, 1222, 1, 3006, 1, 21, 23, 5, 19, 5, 13, 37, 349, 662, 99, 9, 3, 7423, 4, 4891, 8, 13798, 1565, 4, 8, 3, 832, 451, 5, 13, 738, 3184, 3, 765, 2, 5, 20, 223, 23, 5, 74, 3, 2402, 5166, 18, 618, 6, 167, 4, 19, 91, 31, 7, 217, 2639, 662, 2, 5]]\n",
            "Sequence to Text of the first acrticle is [\"by . sean poulter . a skin cancer warning has been sounded following evidence some big brand sun creams do not offer the protection they claim to give . as temperatures are predicted to hit the <OOV> this weekend , products by malibu , <OOV> <OOV> and hawaiian <OOV> have been given a ' don ' t buy ' rating by independent <OOV> which ? . the consumer champion tested <OOV> popular sun creams that claimed to have a sun protection factor ( spf ) of <OOV> as well as helping to shield the skin from <OOV> rays . the end for <OOV> ? the new <OOV> liquid offers up to factor <OOV> protection . the three . which failed were found to have a real spf level of less than <OOV> while . the malibu product also fell short of the required <OOV> standard . sun exposure is linked to premature ageing and skin cancer , with evidence of <OOV> cases being diagnosed in the uk each year . which ? found that paying a high price is no guarantee of better protection , . with the most expensive brand <OOV> by a cheap rival . <OOV> . <OOV> ultra light dry touch sun fluid <OOV> which failed the spf test , . was the most expensive cream tested , at <OOV> per <OOV> . <OOV> sun . <OOV> <OOV> which was <OOV> per <OOV> , passed both tests . which ? executive director richard lloyd said : ' we ' ve found three products that . failed the strict british standard tests and we want to see . manufacturers doing much more to make sure their sun creams live up to . the claims on the packaging . ' all the companies behind the brands criticised by which ? insist the products meet the protection levels claimed on the packs . researchers . tested the creams on ten volunteers , who had the same amount of each . product applied to a small area of their back before lying under a lamp . to replicate the sun ' s rays . their skin was checked for <OOV> and . compared to the results of a lamp test with no cream . to test the <OOV> which ? apply a small amount to an area on a person ' s back before a special lamp is shone on the patch and a lab assistant checks for <OOV> . spf is a measure of how much longer someone will be protected from <OOV> rays than someone wearing no cream . if the sun <OOV> <OOV> skin in ten minutes , for example , a person wearing <OOV> can be exposed to the sun without <OOV> for <OOV> times longer – <OOV> minutes . while <OOV> rays are behind the <OOV> of <OOV> , <OOV> rays damage the skin ' s dna at a deeper layer . both radiation types can cause skin cancer . the eu recommendation is for <OOV> to offer a <OOV> protection factor that is a third of their spf . the malibu product failed to meet this standard . the world health organisation advises applying <OOV> of sun cream . that <OOV> to seven <OOV> – one for the head and neck , one for each arm and leg , and one each for the chest and back . it suggests users re - apply every two hours . which ? said : ' a sunscreen that doesn ' t offer the protection it claims could expose you to a greater risk of developing <OOV> than one that meets its claim , as you will be getting less protection than you think . ' malibu said : ' we stand by our <OOV> have nothing to hide . ' <OOV> <OOV> said all of its products are assessed in compliance with eu regulations and that it is confident its products provide the protection stated . hawaiian <OOV> said it only sells products that meet the spf figure claimed on the pack . temperatures are expected to reach a sweltering <OOV> <OOV> ) this weekend . pictured : plymouth <OOV> , devon . as summer arrives at last , nature is waking up to celebrate . the glorious weather expected this weekend comes after a mild and wet winter that has provided perfect conditions for plants and wildlife to thrive . as a result , many species are stirring earlier than expected . <OOV> are in flower , dog roses are <OOV> and <OOV> is at a peak – a situation not usually seen until june . far more butterflies have also hatched than would usually be seen at this time of year . most of britain will be <OOV> in sunshine at the weekend as the mercury hits its record for the year . there will be barely a cloud in the sky for long spells as temperatures reach a sweltering <OOV> <OOV> ) , warmer than many parts of the mediterranean and much hotter than the <OOV> <OOV> ) average for may . the hot spell will last until monday but will break before the chelsea flower show starts on tuesday , with unsettled weather expected for much of next week . national trust <OOV> matthew <OOV> said : ' it ' s an early summer because of the mild , wet and stormy winter , and the whole thing ' s rather jumped the gun . ' he added : ' if the jet stream is kind to us , it could be a really wonderful summer . '\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0, 0]"
      ],
      "metadata": {
        "id": "aUrMtDvpeMrR",
        "outputId": "f813f5be-1f18-44a5-a02c-c1643904d5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"by. sean poulter. a skin cancer warning has been sounded following evidence some big brand sun creams do not offer the protection they claim to give. as temperatures are predicted to hit the mid-70s this weekend, products by malibu, piz buin and hawaiian tropic have been given a ‘don’t buy’ rating by independent reviewer which?. the consumer champion tested 15 popular sun creams that claimed to have a sun protection factor (spf) of 30, as well as helping to shield the skin from uva rays. the end for lotion? the new drinkable liquid offers up to factor 30 protection. the three. which failed were found to have a real spf level of less than 25, while. the malibu product also fell short of the required uva standard. sun exposure is linked to premature ageing and skin cancer, with evidence of 100,000 cases being diagnosed in the uk each year. which? found that paying a high price is no guarantee of better protection,. with the most expensive brand outperformed by a cheap rival. piz. buin ultra light dry touch sun fluid spf30, which failed the spf test,. was the most expensive cream tested, at £11.30 per 100ml. calypso sun. lotion spf30, which was £1.20 per 100ml, passed both tests. which? executive director richard lloyd said: ‘we’ve found three products that. failed the strict british standard tests and we want to see. manufacturers doing much more to make sure their sun creams live up to. the claims on the packaging.’ all the companies behind the brands criticised by which? insist the products meet the protection levels claimed on the packs. researchers. tested the creams on ten volunteers, who had the same amount of each. product applied to a small area of their back before lying under a lamp. to replicate the sun’s rays. their skin was checked for redness and. compared to the results of a lamp test with no cream. to test the suncream which? apply a small amount to an area on a person's back before a special lamp is shone on the patch and a lab assistant checks for redness. spf is a measure of how much longer someone will be protected from uvb rays than someone wearing no cream. if the sun reddens unprotected skin in ten minutes, for example, a person wearing spf15 can be exposed to the sun without reddening for 15 times longer – 150 minutes. while uvb rays are behind the redness of sunburn, uva rays damage the skin’s dna at a deeper layer. both radiation types can cause skin cancer. the eu recommendation is for sunscreens to offer a uva protection factor that is a third of their spf. the malibu product failed to meet this standard. the world health organisation advises applying 35ml of sun cream. that equates to seven teaspoons – one for the head and neck, one for each arm and leg, and one each for the chest and back. it suggests users re-apply every two hours. which? said: ‘a sunscreen that doesn’t offer the protection it claims could expose you to a greater risk of developing sunburn than one that meets its claim, as you will be getting less protection than you think.’ malibu said: ‘we stand by our testing\\u2009…\\u2009we have nothing to hide.’ piz buin said all of its products are assessed in compliance with eu regulations and that it is confident its products provide the protection stated. hawaiian tropic said it only sells products that meet the spf figure claimed on the pack. temperatures are expected to reach a sweltering 25c (77f) this weekend. pictured: plymouth hoe, devon. as summer arrives at last, nature is waking up to celebrate. the glorious weather expected this weekend comes after a mild and wet winter that has provided perfect conditions for plants and wildlife to thrive. as a result, many species are stirring earlier than expected. elderflowers are in flower, dog roses are blooming and hawthorn is at a peak – a situation not usually seen until june. far more butterflies have also hatched than would usually be seen at this time of year. most of britain will be bathed in sunshine at the weekend as the mercury hits its record for the year. there will be barely a cloud in the sky for long spells as temperatures reach a sweltering 25c (77f), warmer than many parts of the mediterranean and much hotter than the 16c (61f) average for may. the hot spell will last until monday but will break before the chelsea flower show starts on tuesday, with unsettled weather expected for much of next week. national trust naturalist matthew oates said: ‘it’s an early summer because of the mild, wet and stormy winter, and the whole thing’s rather jumped the gun.’ he added: ‘if the jet stream is kind to us, it could be a really wonderful summer.’\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\n",
        "NO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that."
      ],
      "metadata": {
        "id": "crn5t_zk6iBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.keys())[21000]"
      ],
      "metadata": {
        "id": "qhttL-heeP-P",
        "outputId": "b43dd482-1dbe-49c3-80d8-9476b40f577c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'resounding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This example is to test the above fact with the word `resounding`\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "S_Dnkcig7SIX",
        "outputId": "f0658315-fbd9-42a3-e2c6-758ead23e0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This example is to test the above fact with the word `resounding`\n",
            "\n",
            "\n",
            "Tokenized text: ['this example is to test the above fact with the word ` <OOV> `']\n",
            "Sequence: [[38, 1127, 18, 6, 834, 3, 764, 539, 22, 3, 1302, 16926, 1, 16926]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although `resounding` word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`."
      ],
      "metadata": {
        "id": "xvNeazXb-Yq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add a number 2.1 in this sentence!\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ng5MZ_DZ84kk",
        "outputId": "42bf2048-59e4-4f63-bae1-ce34526e0f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add a number 2.1 in this sentence!\n",
            "\n",
            "\n",
            "Tokenized text: ['what happens when i add a number <OOV> in this sentence !']\n",
            "Sequence: [[70, 2087, 54, 27, 1951, 7, 260, 1, 10, 38, 1074, 301]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add parenthesis (I am inside it!).\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ImlMHniH-Jnt",
        "outputId": "04294a94-e26a-427e-a3ea-6e86c64bb191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add parenthesis (I am inside it!).\n",
            "\n",
            "\n",
            "Tokenized text: ['what happens when i add <OOV> ( i am inside it ! ) .']\n",
            "Sequence: [[70, 2087, 54, 27, 1951, 1, 50, 27, 320, 514, 19, 301, 49, 2]]\n"
          ]
        }
      ]
    }
  ]
}