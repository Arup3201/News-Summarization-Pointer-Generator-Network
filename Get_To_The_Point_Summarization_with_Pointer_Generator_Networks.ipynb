{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQR3+wOYWYHN7blvCwp6Yv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/Summarization-Project-using-Pointer-Gen/blob/main/Get_To_The_Point_Summarization_with_Pointer_Generator_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "J2_Q63jmdSp3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x6QucUCZ0q2V",
        "outputId": "0d89463b-17be-4644-988d-ac1d51c8fc93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pathlib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "path_to_dailymail_stories = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\",\n",
        "    extract=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "hr2kPGPSLi5m",
        "outputId": "3ba33df5-c599-49d2-b024-c89cbcf0acaf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/cnn_stories.tgz\n",
            "158577824/158577824 [==============================] - 3s 0us/step\n",
            "Downloading data from https://huggingface.co/datasets/cnn_dailymail/resolve/main/data/dailymail_stories.tgz\n",
            "375893739/375893739 [==============================] - 7s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cnn_stories, path_to_dailymail_stories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uamve0operH0",
        "outputId": "13e8f0cf-6a5b-45a4-c286-934afa17223b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.keras/datasets/cnn_stories.tgz',\n",
              " '/root/.keras/datasets/dailymail_stories.tgz')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /root/.keras/datasets"
      ],
      "metadata": {
        "id": "WT84BmPll24S",
        "outputId": "f6636273-0c1d-45e5-aae3-295461746c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 521960\n",
            "drwxr-xr-x 3 root root      4096 Jul 27 13:54 cnn\n",
            "-rw-r--r-- 1 root root 158577824 Jul 27 13:54 cnn_stories.tgz\n",
            "drwxr-xr-x 3 root root      4096 Jul 27 13:55 dailymail\n",
            "-rw-r--r-- 1 root root 375893739 Jul 27 13:55 dailymail_stories.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir = pathlib.Path('/root/.keras/datasets/cnn/stories')\n",
        "dailymail_stories_dir = pathlib.Path('/root/.keras/datasets/dailymail/stories')"
      ],
      "metadata": {
        "id": "ag5ubSt7l88l",
        "outputId": "aab64faa-f054-4ed4-8a38-d4df73c4eb67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_stories_dir, dailymail_stories_dir"
      ],
      "metadata": {
        "id": "UmMCM--Yhm3B",
        "outputId": "502fc763-225a-4dc2-c228-c8f9951e03d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/root/.keras/datasets/cnn/stories'),\n",
              " PosixPath('/root/.keras/datasets/dailymail/stories'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_filenames(dir_path, num_files=5):\n",
        "  '''Prints the name of the files that are present at `dir_path`.\n",
        "  Maximum `num_files` number of files are shown.\n",
        "\n",
        "  Arguements:\n",
        "    dir_path: PosixPath, pointing to the directory of which the user\n",
        "              wants to prints the file names.\n",
        "    num_files: int, number of files user wants to print.\n",
        "\n",
        "  returns:\n",
        "    nothing\n",
        "  '''\n",
        "\n",
        "  count = 0\n",
        "  for f in dir_path.glob('*.story'):\n",
        "    print(f.name)\n",
        "    count += 1\n",
        "\n",
        "    if count == num_files:\n",
        "      break\n",
        "  else:\n",
        "    print(f\"Less than {num_files} is present!\")"
      ],
      "metadata": {
        "id": "dZFFXG7Pg86H",
        "outputId": "637dc86b-dedc-4c8c-917b-a96aea59abd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(cnn_stories_dir)"
      ],
      "metadata": {
        "id": "nv3g7-M316yL",
        "outputId": "7b54ab64-2653-45f5-9a0f-495b8c097f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "866fd53d950795b6bfd67201bcbfc71b2fc0760e.story\n",
            "c8a5597ea3044f53bc9f473b5eef2f46ab6d7d93.story\n",
            "6feb1c384ef044e13fa496b421a7d8294bd5d14f.story\n",
            "17329a436a5f99bb018a6faca885d8d3bd1896f8.story\n",
            "1539cd6df413b33a006ded9abdf017fcc22bf52e.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_filenames(dailymail_stories_dir)"
      ],
      "metadata": {
        "id": "WcyW1wJa2EaN",
        "outputId": "bcf6762e-31cb-4c1f-c52c-1cedd0c6a44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8f3f6f9ec6a29a35e611b90cd9a7f19b09b2b460.story\n",
            "a690d9ef911b5e7cd1a8a4eaab228f1654f8b6a3.story\n",
            "1035b0731144ced5c6dc61891cfe4f2f01449511.story\n",
            "7c2855b2f276b36c7e0f3c29e219333ed0759a41.story\n",
            "c21407d7fc1c01e93dd184d7013893869fa92061.story\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
        "              dm_single_close_quote, dm_double_close_quote, \")\"]\n",
        "\n",
        "# Maximum stories to process from cnn and dailymail each\n",
        "MAX_STORIES = 50000\n",
        "\n",
        "# From the total data how to split into train, val and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VAL_SIZE = 0.1\n",
        "TEST_SIZE = 0.1\n",
        "\n",
        "# For tokenization\n",
        "VOCAB_SIZE = 20000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# For standardization\n",
        "PAD_TOKEN = '<PAD>'\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "# For the number of tokens to use in representing articles and summaries\n",
        "MAX_ARTICLE_TOKENS = 400\n",
        "MAX_SUMMARY_TOKENS = 100"
      ],
      "metadata": {
        "id": "cubLAm1Q3SuG",
        "outputId": "cac095a1-ced0-4a54-908e-ca9395ab7db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a sample .story file from cnn stories\n",
        "sample_filename = \"438411e10e1ef79b47cc48cd95296d85798c1e38.story\"\n",
        "sample_filedir = cnn_stories_dir\n",
        "\n",
        "sample_filepath = sample_filedir / sample_filename\n",
        "with open(sample_filepath, 'r') as f:\n",
        "  sample_story = f.read()\n",
        "\n",
        "print(sample_story)"
      ],
      "metadata": {
        "id": "Uw0kqchX3X7X",
        "outputId": "d3715148-cd84-4e8d-b0f5-941b075db590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\n",
            "\n",
            "The figure represents a 0.7% increase from last year, adding 2,250,129 people to the U.S. population since the start of 2011, and a 1.3% increase since Census Day, April 1, 2010.\n",
            "\n",
            "The agency estimates that beginning in January, one American will be born every eight seconds and one will die every 12 seconds.\n",
            "\n",
            "U.S.-bound immigrants are also expected to add one person every 46 seconds.\n",
            "\n",
            "That combination of births, deaths and migration is expected to add a single person to the U.S. population every 17 seconds, the Census Bureau said.\n",
            "\n",
            "Meanwhile, millions are set to ring in the new year.\n",
            "\n",
            "In New York, authorities are preparing for large crowds in Manhattan's Times Square, where Lady Gaga is expected to join Mayor Michael Bloomberg to push the button that drops the Waterford Crystal ball at 11:59 p.m. ET on New Year's Eve.\n",
            "\n",
            "\"And I'm so looking forward to performing on NYE+dropping the Ball with Mayor Bloomberg!\" the pop star posted on Twitter. \"What an honor as a New Yorker.\"\n",
            "\n",
            "Past guests have included Muhammad Ali, Rudy Giuliani, Colin Powell and Bill and Hillary Clinton.\n",
            "\n",
            "On Thursday, officials conducted New York's annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above Times Square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square.\n",
            "\n",
            "The Big Apple this year edged out Las Vegas for the first time in seven years as the top travel U.S. destination for those celebrating the new year, according to a December travel booking website poll.\n",
            "\n",
            "Seven New York neighborhoods made the top 10 list, with two districts in Las Vegas and one in New Orleans making up the other three, according to the Priceline poll.\n",
            "\n",
            "\"It appears that New York City will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman Brian Ek.\n",
            "\n",
            "@highlight\n",
            "\n",
            "Census Bureau: U.S. population is expected to be 312.8 million on New Year's Day\n",
            "\n",
            "@highlight\n",
            "\n",
            "That figure represents a 0.7% increase from last year\n",
            "\n",
            "@highlight\n",
            "\n",
            "Lady Gaga, mayor to activate ball drop at Times Square on New Year's Eve\n",
            "\n",
            "@highlight\n",
            "\n",
            "NYC has supplanted Las Vegas as the top New Year's destination, Priceline poll says\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `fix_missing_period` where I am taking 2 arguements, one for the `line` for which I am checking and fixing the period and other is `end_tokens` which is a list that has all the tokens that I should consider as ending of a sentence.\n",
        "\n",
        "These are the steps -\n",
        "1. Check if line contains `@highlight`, if True then just return the line.\n",
        "2. Check if line is empty, then return line as it is.\n",
        "3. Check is line ends with any of the `end_tokens`, if so then return line as it is.\n",
        "4. Only is none of the above conditions match then append `.` to the current line."
      ],
      "metadata": {
        "id": "iZPt8EVBzJ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_missing_period(line, end_tokens=END_TOKENS):\n",
        "  '''function to fix the missing periods for some story lines which do not end with\n",
        "  any of the end_tokens mentioned.\n",
        "\n",
        "  Arguements:\n",
        "    line: string, line of the story to fix the missing the period of.\n",
        "    end_tokens: list of strings, all the tokens that are considered as line end.\n",
        "\n",
        "  Returns:\n",
        "    new line with fixed the ending part by adding an ending token if not present.\n",
        "  '''\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  elif line == \"\":\n",
        "    return line\n",
        "  elif line[-1] in end_tokens:\n",
        "    return line\n",
        "\n",
        "  return line + '.'"
      ],
      "metadata": {
        "id": "i-S-Hss12TPk",
        "outputId": "3e5b16b0-d048-4bb8-b37e-9fe041c263e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fix_missing_period(sample_story.split('\\n')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L1hiNysxnmAO",
        "outputId": "d0a75d26-cad0-4095-92c8-b4819141ff52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"New York (CNN) -- The U.S. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on New Year's Eve, according to new census data released Thursday.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `split_article_summary` which will split the story into article and summary parts.\n",
        "\n",
        "The function takes only 1 arguement and that is the `story` which will be splitted into article and summary.\n",
        "\n",
        "The steps to follow are -\n",
        "1. Split the story by new line `\\n`. I will get a list of lines.\n",
        "2. Strip the lines by using list comprehension.\n",
        "3. Use list comprehension to make lower case each line by using `.lower()`.\n",
        "4. Fix each line by adding period if there is none in that line using `fix_missing_period` function.\n",
        "5. Make 2 empty list for `article` and `summary`.\n",
        "6. Go through each line. In each line, I need to check 4 things,\n",
        "  * line contains `@highlight` or not, if True then set `next_highlight` to `True` because the next to next line is going to be a summary line.\n",
        "  * line is `\"\"` empty or not, if True then ignore.\n",
        "  * `next_highlight` is True or not, if True then append the line to `summary`.\n",
        "  * If non of the ebove then append to `article`.\n",
        "7. After done with filling the `article` and `summary` list with lines, join those sentences to make the whole article and summary. Here, I am using `.join()` method."
      ],
      "metadata": {
        "id": "5-tqWtoowXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_article_summary(story):\n",
        "  '''Splits the story into 2 parts, one for article and other for summary of that\n",
        "  article. Returns the article and summary.\n",
        "\n",
        "  Arguements:\n",
        "    story: string file that contains both article and summary combiningly.\n",
        "\n",
        "  Returns:\n",
        "    article, summary seperately from the story.\n",
        "\n",
        "  '''\n",
        "  lines = story.split('\\n')\n",
        "  lines = [line.strip() for line in lines]\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  # Fix the ending period\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # List to contain the article and summary lines\n",
        "  article = []\n",
        "  summary = []\n",
        "\n",
        "  # Indicator of whether the next line is the summary or not\n",
        "  next_highlight = False\n",
        "\n",
        "  for line in lines:\n",
        "    if \"@highlight\" in line:\n",
        "      next_highlight = True\n",
        "    elif line==\"\":\n",
        "      continue\n",
        "    elif next_highlight:\n",
        "      summary.append(line)\n",
        "    else:\n",
        "      article.append(line)\n",
        "\n",
        "  article = ' '.join(article)\n",
        "  summary = ' '.join(summary)\n",
        "\n",
        "  return article, summary"
      ],
      "metadata": {
        "id": "-X4eMltQnf10",
        "outputId": "d5d20ac9-e160-4f5e-c173-02480577119b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_article_summary(sample_story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nuUjOaN9orGU",
        "outputId": "8eb67bec-e5db-4bb1-c921-2368570e760d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.',\n",
              " \"census bureau: u.s. population is expected to be 312.8 million on new year's day. that figure represents a 0.7% increase from last year. lady gaga, mayor to activate ball drop at times square on new year's eve. nyc has supplanted las vegas as the top new year's destination, priceline poll says.\")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating a function `get_articles_summaries` which will process each of the stories present in the directory of cnn and dailymail and return the articles, summaries in the form of list.\n",
        "\n",
        "This function will take 2 arguements. One will be the `stories_dir` which is a Posix format string from `pathlib` library and another arguement is of `max_stories` which is the maximum number of stories that we will extract from those directories.\n",
        "\n",
        "The process is simple. We will follow this steps -\n",
        "1. Create 2 empty lists of `articles` and `summaries`.\n",
        "2. Loop through all the files present in the directory `stories_dir` using `.glob` generator method.\n",
        "3. Make a `count` variable which will count the number of processed strories and when it hits `max_stories`, break from the loop.\n",
        "4. Inside the loop, you will open the file in `r` reading format, then just use `.read()` method to read the story.\n",
        "5. Everytime after reading the story, split the article and summary part from it and then append them inside the `articles` and `summaries` list.\n",
        "6. Return the 2 lists."
      ],
      "metadata": {
        "id": "oTEa0m3Huxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_summaries(stories_dir, max_stories):\n",
        "  '''stores the stories from stories_dir folder into a list and returns the list\n",
        "\n",
        "  Arguement:\n",
        "    stories_dir: Posix string, the directory where the stories are stored\n",
        "    max_stories: maximum number of stories to store\n",
        "\n",
        "  Returns:\n",
        "    list of stories.\n",
        "\n",
        "  '''\n",
        "  articles = []\n",
        "  summaries = []\n",
        "\n",
        "  count = 0\n",
        "  for f in stories_dir.glob(\"*.story\"):\n",
        "    count += 1\n",
        "    with open(f, 'r') as reader:\n",
        "      story = reader.read()\n",
        "\n",
        "      article, summary = split_article_summary(story)\n",
        "\n",
        "      articles.append(article)\n",
        "      summaries.append(summary)\n",
        "\n",
        "    if count == max_stories:\n",
        "      break\n",
        "\n",
        "  return articles, summaries"
      ],
      "metadata": {
        "id": "4VUmbYSpnjAr",
        "outputId": "52933d86-bc07-4e0b-f62f-281148827b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "cnn\n",
        "  stories\n",
        "    438411e10e1ef79b47cc48cd95296d85798c1e38.story\n",
        "    e453e379e8a70af2d3dff1c75c41b0a35edbe9cc.story\n",
        "    2079f35aca44978a7985afe0ddacdf02bedf98f2.story\n",
        "    4702f28c198223157bb8f69665b039d560eebb0f.story\n",
        "    db3e2ea79323a98379228b17cd3b9dec17dbd2cb.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "dailymail\n",
        "  stories\n",
        "    f4ba18635997139c751311b9f2ad18f455dd7c98.story\n",
        "    4a3ef32cff589c85ad0d22724e2ed747c0dacf87.story\n",
        "    5375ed75939108c72001b043d3b4799c47f32be9.story\n",
        "    fe9e57c21e21fb4ec26e394f0e92824f38d18a95.story\n",
        "    6a544b5cdd2384be6cc657b265d7aa2de72a99e0.story\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "Out of all available .story files, we will only take `MAX_STORIES` number of files and then open them."
      ],
      "metadata": {
        "id": "Uk-BuCM4rIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles, cnn_summaries = get_articles_summaries(cnn_stories_dir, MAX_STORIES)\n",
        "\n",
        "len(cnn_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aa9ZDQntpHQZ",
        "outputId": "0095b95e-0691-4598-e4f0-05f959f9bf57"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(cnn_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {cnn_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {cnn_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "-q_i-69YqJnj",
        "outputId": "5225b97e-74d1-4e18-d296-5ecc0dd2b3b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: (cnn)  -- in trendy neighborhoods of tokyo customers are lining up for vitamin injections that promise to improve health and beauty. a japanese woman receives an intravenous vitamin supplement at the tenteki cafe in tokyo. these intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in japan: the iv cafe. each drip pack contains saline solution and specific vitamins and minerals to target a particular health ailment or beauty concern. \"i used to take vitamin supplements, but changed to the iv drip because i feel the effects more quickly,\" a 20-something woman at the tenteki 10 café told cnn. she said she receives specific injections to get better skin, burn fat and boost her energy. there are 10 different varieties to choose from at tenteki. the \"orange\" variety touts anti-aging properties, loaded with antioxidants. the \"placenta pack\" is said to help rejuvenate and ease muscle stiffness. prices range from $20-$30 per injection, and nurses see about 30-40 people each day. their most common patients are japanese businessmen who work in the same office building. \"i see a lot of businessmen who say they don't have time to sleep. they can't take a break from working and get the vitamin drip for an extra kick of energy,\" a tenteki nurse told cnn. \"blue\" is the most requested vitamin pack among these men: a concoction of b1 and vitamin e that claims to offer relief from exhaustion. registered nurses and doctors administer the drips at tenteki, but there's no conclusive medical evidence to back up the health claims. many nutritionists actually caution against using injectable vitamin supplements because the quantities are not regulated. \"more is not necessarily better...some vitamins and minerals can be toxic in high doses,\" particularly the fat-soluble ones which the body stores like vitamins a, d, e and k, explained claire williamson, nutrition scientist at the british nutrition foundation. in europe and the united states vitamin shots are popular among celebrities with hectic lifestyles and little time to sleep, particularly vitamin b 12. former spice girl geri halliwell and singer robbie williams have both confirmed they've used the shots as part of their diets to maintain stamina during tours. dermatological injections of vitamin c are also popular among women hoping to keep their skin looking young. former supermodel cindy crawford has admitted using such injections to keep her skin firm and wrinkle-free. according to williamson, it does not matter if supplements are injected into the vein or into the skin. \"at the end of the day it will go into the blood stream,\" she said. most of these nutrients we can get sufficient from foods, nutrients tend to be better absorbed by the body if they are consumed in foods.\"\n",
            "\n",
            "\n",
            "The summary of this article: intravenous vitamin boosts are the latest health fad in japan. vitamin injections target specific health aliments and beauty concerns. nutritionists warn of vitamin overdose from high quantities of supplements. vitamin injections are popular among celebrities and businessmen for fatigue.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailymail_articles, dailymail_summaries = get_articles_summaries(dailymail_stories_dir,\n",
        "                                                                 MAX_STORIES)"
      ],
      "metadata": {
        "id": "KT4Hrp6nqeKu",
        "outputId": "05ef3597-cb52-4230-c42c-037caac3cabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total no of cnn stories captured are {len(dailymail_articles)}\\n\\n\")\n",
        "print(f\"One of the CNN articles: {dailymail_articles[0]}\\n\\n\")\n",
        "print(f\"The summary of this article: {dailymail_summaries[0]}\\n\\n\")"
      ],
      "metadata": {
        "id": "nkzwSP9kh4VK",
        "outputId": "94163e26-74d5-413c-8472-3fb2c9975e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of cnn stories captured are 50000\n",
            "\n",
            "\n",
            "One of the CNN articles: it's a brave politician indeed who is willing to expose their own battle with their squeezed middle for the cameras. as the election gets underway the gloves have come off between the main parties, and for three frontbenchers the tops have come off as well. tory health minister dan poulter, lib dem minister tom brake and labour frontbencher gavin shuker imposed strict cuts on their calories and put themselves at the mercy of a personal trainer for men's health magazine. scroll down for video. health minister dan poulter saw his weight fell from 105kg to 102.5kg, losing two inches off his 35in waist. his body fat fell from 24 per cent to 18 per cent. cooped up in the department of health, he admitted it is difficult to practice what he preaches on keeping healthy. skipping breakfast, grabbing lunch on the go and late nights in parliament's bars and restaurants can take its toll on mps when they arrive in westminster. after five years of the coalition government, the three mps agreed to take part in the feature for the men's health magazine, which is better known for its front covers of rippling muscles and toned six packs. each was given strict orders to eat more healthily, do more exercise and take better care of their bodies. there was the inevitable unflattering 'before' photo shoot, followed by a full-length portrait to show off their new physique. mr poulter, a gp first elected in 2010, could be expected to already know what it takes to stay healthy. but cooped up in the department of health, he admitted it is difficult to practice what he preaches. 'i have an erratic diary, working 15-hour days, so it does require real dedication to get gym sessions in', he said. his personal trainer stephen white put him on a regime of fat-burning intervals and muscle-building resistance training. he said: 'dan really struggled at first because of his back – we focused on loosening and developing it to pull his shoulders back and correct his terrible posture.' in the end mr poulter's weight fell from 105kg to 102.5kg, losing two inches off his 35in waist. his body fat fell from 24 per cent to 18 per cent, and he boasted of feeling 'fitter, stronger, more awake and motivated in my work'. lib dem deputy commons leader tom brake was told to build his upper-body strength and after six weeks of training could swim the 750m of the sprint distance triathlon without reverting to breast stroke. as deputy leader of the commons, mr brake's job involved a lot of sitting on the green leather benches listening to debates. the lib dem mp and keen marathon runner could have been forgiven for thinking he was fit. but after casting an eye over his 'before' physique, personal trainer daniel marshall declared: 'tom neglected every muscle from the waist up.' the 52-year-old was told to build his upper-body strength and after six weeks of training could swim the 750m of the sprint distance triathlon without reverting to breast stroke. he lost only a kilo of his 83.5kg starting weight, but his chest size grew from 38in to 48 in, while he lost an inch from his waist. mr brake said: 'i used to think going to the gym was a waste of time, and that you were much better off going for a run. not anymore.' labour mp gavin shuker embarked on an intense weight loss regime, forced to shun carbs and alcohol when being wined and dined in westminster. labour mp gavin shuker was elected as mp for luton south in 2010, after incumbent margaret moran was forced out over her expenses. a shadow international development minister, he says that he succumbed to the 'parliamentary stone' - the extra 14 pounds new mps put on in their first year after arriving in westminster. in fact he went even further, and put on three stone within 12 months. men's health rather unkindly remarked: 'the fact that the shadow minister hadn’t done any exercise since his school days compounded the problem – the only internationally perceptible development being around his waistline. 'the only internationally perceptible development being around his waistline.' he embarked on an intense weight loss regime, forced to shun carbs and alcohol when being wined and dined in westminster. 'i’m sure at one point an mp’s life was all big boozy lunches and champagne dinner receptions,' he said. 'the reality now is that if you eat lunch at all, it’ll be out of a box at your desk, like most people.' surprisingly, he was told to eat more, not less, but make it healthy. the 33-year-old had got into the habit of not eating during the day and gorging at night. for the first time in decades he started eating porridge for breakfast. in the end he managed to lose more than 10kg, taking him down to 85kg. his waist slimmed down from 44in to 37in, while his biceps expanded from 32cm to 35cm. the magazine suggested the spectacle of three mps naked from the waist up could inspire its readers to get fit themselves. it said: 'if they can juggle work and fitness, you sure as hell can too. it may not be easy, but then in this age of austerity, the biggest cuts never are.'\n",
            "\n",
            "\n",
            "The summary of this article: tory dan poulter, lib dem tom brake and labour's gavin shuker get fit. three agreed to put themselves at the mercy of a personal trainer. strict diet and exercise regime saw them shed pounds and gain strength. full interview appears in the may 2015 issue of men’s health, on sale thursday 2 april. also available as a digital edition.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1, 2] + [3, 4]"
      ],
      "metadata": {
        "id": "ToPn8sAtpyOX",
        "outputId": "37e81d12-070b-4654-ac2e-2512c34cd745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0) # Keeps the shuffling same as before\n",
        "random.sample([1, 2, 3, 4, 5], 5)"
      ],
      "metadata": {
        "id": "eFZO1v0NqIJM",
        "outputId": "7b49be7e-3c16-4852-dab8-bb3fc4ea19bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article, sample_summary = split_article_summary(sample_story)"
      ],
      "metadata": {
        "id": "ZJCTOgIJmyXh",
        "outputId": "eaadb0f3-aaf5-45ed-d3c5-a44ce48c41f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article"
      ],
      "metadata": {
        "id": "STljZePPnGy9",
        "outputId": "5483425d-47b8-4fbe-ac20-270b216696d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'new york (cnn) -- the u.s. population is expected to top out at close to 312.8 million people just around the time crowds gather to watch the ball drop on new year\\'s eve, according to new census data released thursday. the figure represents a 0.7% increase from last year, adding 2,250,129 people to the u.s. population since the start of 2011, and a 1.3% increase since census day, april 1, 2010. the agency estimates that beginning in january, one american will be born every eight seconds and one will die every 12 seconds. u.s.-bound immigrants are also expected to add one person every 46 seconds. that combination of births, deaths and migration is expected to add a single person to the u.s. population every 17 seconds, the census bureau said. meanwhile, millions are set to ring in the new year. in new york, authorities are preparing for large crowds in manhattan\\'s times square, where lady gaga is expected to join mayor michael bloomberg to push the button that drops the waterford crystal ball at 11:59 p.m. et on new year\\'s eve. \"and i\\'m so looking forward to performing on nye+dropping the ball with mayor bloomberg!\" the pop star posted on twitter. \"what an honor as a new yorker.\" past guests have included muhammad ali, rudy giuliani, colin powell and bill and hillary clinton. on thursday, officials conducted new york\\'s annual \"airworthiness test\" -- a process in which confetti is tossed by handfuls above times square -- in preparation for the annual city tradition of dumping one ton of confetti over revelers in the iconic square. the big apple this year edged out las vegas for the first time in seven years as the top travel u.s. destination for those celebrating the new year, according to a december travel booking website poll. seven new york neighborhoods made the top 10 list, with two districts in las vegas and one in new orleans making up the other three, according to the priceline poll. \"it appears that new york city will be helped this year by a weather forecast that calls for warmer than usual temperatures over the holiday weekend,\" said company spokesman brian ek.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creating another function -\n",
        "`split_dataset(train_size, val_size, test_size)`: I am creating this function to split the original 1,00,000 examples into 80,000 training samples, 10,000 val samples and 10,000 test samples."
      ],
      "metadata": {
        "id": "8ea-PhS3iIJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_size, val_size, test_size):\n",
        "  first_split = train_size\n",
        "  second_split = train_size+val_size\n",
        "  third_split = train_size+val_size+test_size\n",
        "  return dataset[:first_split, :], dataset[first_split:second_split, :], dataset[second_split:third_split, :]"
      ],
      "metadata": {
        "id": "v-iZDbUCqkdC",
        "outputId": "362a2e77-036c-4a95-dd92-b417d087fcee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize the 4 functions created above into one function called `make_datasets`. This function will -\n",
        "1. This functions will have many argumenets and among them 2 argumenets `cnn_stories` and `dailymail_stories` are lists which has list of articles and summaries at 0 and 1 index. It means `cnn_stories[0]` is articles of cnn news and `cnn_stories[1]` is summaries of cnn news. It applies to `dailymail_stories` as well.\n",
        "Objective of this step is to concatenate the cnn articles with dailymail articles and cnn summaries with dailymail summaries.\n",
        "```python\n",
        "[1, 2] + [3, 4] = [1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "3. Convert the articles and summaries list into tensors and then concatenate them along a new axis. To create new axis I can use `tf.newaxis` in the indexing. E.g.\n",
        "```python\n",
        "  np.concatenate([articles[:, tf.newaxis], summaries[:, tf.newaxis]], axis=-1)\n",
        "```\n",
        "4. Shuffle the dataset using `random.sample` method.\n",
        "```python\n",
        "random.seed(seed_value) # To make sure that everytime it gives the same shuffle\n",
        "random.sample(list_to_shuffle, len(list_to_shuffle))\n",
        "```\n",
        "5. Split the dataset into 3 parts, one for training, other for validation and last one for testing. All the tensors are of shape `(num_samples, 2)`."
      ],
      "metadata": {
        "id": "FTB5eNzJrqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets(cnn_stories, dailymail_stories, train_fraction, val_fraction, test_fraction, seed_value=0):\n",
        "  '''Create 3 datasets each for training, validation and testing respectively.\n",
        "  This function concatenates the articles, summaries of cnn and dailymail news. After that it will tokenize\n",
        "  them one by one in a loop. After it is done with the tokenization, it will shuffle the articles and\n",
        "  summaries using random.sample method (although we have a helper function for it). Finally we do the\n",
        "  splitting of the whole dataset. Remember here the returned values become tensors.\n",
        "\n",
        "  Arguements:\n",
        "    cnn_stories: list of 2 values, one for cnn articles and other for cnn summaries.\n",
        "    dailymail_stories: list of 2 values, one for dailymail articles and other for dailymail summaries.\n",
        "    train_size: float, specifying how much fraction of the original dataset to take for training.\n",
        "    val_size: float, specifying how much fraction of the original dataset to take for validation.\n",
        "    test_size: float, specifying how much fraction of the original dataset to take for testing.\n",
        "\n",
        "  Returns:\n",
        "    returns a tuple with 3 values inside it, `training_data`, `validation_data` and `testing_data`\n",
        "    with the specified amount of data in it.\n",
        "    Each one of them are tensor with shape `(num_samples, 2)`. `shape[1]=2` for article and summary.\n",
        "  '''\n",
        "  articles = cnn_stories[0] + dailymail_stories[0]\n",
        "  summaries = cnn_stories[1] + dailymail_stories[1]\n",
        "\n",
        "  articles = np.array(articles, dtype=object)\n",
        "  summaries = np.array(summaries, dtype=object)\n",
        "\n",
        "  dataset = np.concatenate((articles[:, tf.newaxis], summaries[:, tf.newaxis]), axis=-1)\n",
        "\n",
        "  random.seed(seed_value)\n",
        "  shuffled_indices = random.sample(list(range(dataset.shape[0])), dataset.shape[0])\n",
        "\n",
        "  dataset = dataset[shuffled_indices, :]\n",
        "\n",
        "  train_size = int(train_fraction * dataset.shape[0])\n",
        "  val_size = int(val_fraction * dataset.shape[0])\n",
        "  test_size = dataset.shape[0] - (train_size + val_size)\n",
        "\n",
        "  training_samples, validation_samples, testing_samples = split_dataset(dataset,\n",
        "                                                                        train_size,\n",
        "                                                                        val_size,\n",
        "                                                                        test_size)\n",
        "\n",
        "  return (training_samples, validation_samples, testing_samples)"
      ],
      "metadata": {
        "id": "QDB0_32RrnHk",
        "outputId": "87a34cf0-c49c-4733-c5e2-4a51c076571d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = make_datasets([cnn_articles, cnn_summaries], [dailymail_articles, dailymail_summaries], TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
      ],
      "metadata": {
        "id": "GTnXBwd6Sa-U",
        "outputId": "48f85a49-8fd4-473d-cfd1-8893a4006430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of the datasets: {type(train_dataset)}\\n\")\n",
        "\n",
        "print(f\"Training dataset shape: {train_dataset.shape}\")\n",
        "print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
        "print(f\"Testing dataset shape: {test_dataset.shape}\\n\")\n",
        "\n",
        "print(f\"First example in the training dataset looks like: \\n {train_dataset[0]}\\n\")"
      ],
      "metadata": {
        "id": "AyRVodwIhkuQ",
        "outputId": "66ce57bb-4e78-45de-b01b-7b6d8724b66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the datasets: <class 'numpy.ndarray'>\n",
            "\n",
            "Training dataset shape: (80000, 2)\n",
            "Validation dataset shape: (10000, 2)\n",
            "Testing dataset shape: (10000, 2)\n",
            "\n",
            "First example in the training dataset looks like: \n",
            " [\"by. snejana farberov. 'monster': samuel cabrera, 27, is accused of murdering his 3-month-old son by beating the child to death. a philadelphia man has been arrested after police say he beat his 3-month-old son to death and then tried to make it look like an accident. samuel cabrera, 27, has been charged with murder in the death of samuel cabrera jr, who passed away after suffering a slew of internal injuries. last tuesday, the toddler was rushed from his family's home in the 600 block of north 63rd street to the children's hospital in critical condition. doctors say all of the boy's internal organs had been crushed, and the toddler also had two broken bones and a ruptured spleen, nbc news reported. when mr cabrera and the victim’s mother had been questioned by police, the couple's stories did not match up. the mother, who has not been charged in the killing, allegedly told investigators that she and her boyfriend found little samuel pale and unconscious. according to the woman, the boy suffered bruises after she and cabrera pounded on his chest while performing cpr. the 27-year-old father, however, offered a different version of events, telling police that he was trying to get the family dog off he coach when he accidentally hit the baby. crime scene: the tragedy unfolded in cabrera's family home in the 600 block of north 63rd street. cover-up: when he was initially questioned by police, mr cabrera said he accidentally hit the boy while trying to get the family dog (pictured) off the couch. nbc 10 sources claimed that cabrera eventually confessed to killing the boy. besides little samuel, the couple have been living with cabrera's 15-year-old daughter and his girlfriend’s three children from a previous relationship. all four children are currently staying with relatives. detectives say they spotted bruises on the suspect’s teenage daughter and are now trying to determine if she was also abused by her father. mr cabrera is due in court for a preliminary hearing may 1. heartbreaking outcome: little samuel passed away at the children's hospital from what doctors described as crushed internal organs and ruptured spleen.\"\n",
            " \"samuel cabrera, 27, initially told police he accidentally hit boy while trying to get dog off a couch. police investigate if cabrera's 15-year-old daughter also had been abused by him.\"]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the tokenization, we need to preprocess the text data so that it can be properly tokenized. In this step we need to choose whether we want to keep punctuations or not, whether we should keep the numbers or not and so on. There are 2 functions I will create, one for simple `standardize` and other to feed the Tokenizer class when creating the `tokenizer`. `standardize` function implements the following steps -\n",
        "\n",
        "1. Lower case the strings passed to it. It is already done but for user data it might not be the case so, we will still perform this step.\n",
        "2. Replace the single and double opening and closing quotes like `‘ → \\u2018`, `’ → \\u2019`, `“ → \\u201c` and `” → \\u201d` by `'` and `\"` respectively.\n",
        "3. Replace the punctutations ``['.', '?', '!', ',', ':', '-', ''', '\"', '_', '(', ')', '{', '}', '[', ']', '`', ';', '...']`` by `[SPACE]punctutations`.\n",
        "In this process we need to make sure that the floating point numbers like `1.78` do not become `1 .78`. To do that the correct regex expression is ``(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)``.\n",
        "4. Strip the texts from extra starting or ending spaces. Finally, remove extra spaces using regex expression like `\\s{2,}`.\n",
        "\n",
        "`custom_analyzer` function which will be feed to the Tokenizer as the value for `analyzer`, has some more steps to implement -\n",
        "1. Remove the `START_TOKEN` and `END_TOKEN` from the text. So that tokenizer does not standardize them.\n",
        "2. Standardize the text with `standardizer`.\n",
        "3. Add back the `START_TOKEN` and `END_TOKEN` because you want your tokenizer to learn them.\n",
        "4. Remove unwanted spaces in between words.\n",
        "5. Split the text into words which are seperated by ' '.\n",
        "6. Strip each of the words in the sentence. Finally, return it."
      ],
      "metadata": {
        "id": "TcZCpJ9hCyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the text data\n",
        "def standardizer(text):\n",
        "  '''Standardize the text provided to the function\n",
        "  The text is lower cased. Then, the opening and closing quotes are removed. I add spaces before the\n",
        "  punctuations like `don't` becomes `don ' t`, ignoring the numerical values so that `1.78` does not become\n",
        "  `1 . 78`. Finally, it strips the text and removes any type of unwanted spaces in it.\n",
        "\n",
        "  Arguement:\n",
        "    text: str, the text to standardize\n",
        "\n",
        "  Returns:\n",
        "    returns the standadized text\n",
        "  '''\n",
        "\n",
        "  # Lower case the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Replace the special single and double opening and closing quotes\n",
        "  text = re.sub(r'[\\u2019\\u2018]', \"'\", text)\n",
        "  text = re.sub(r'[\\u201c\\u201d]', '\"', text)\n",
        "\n",
        "  # Add space before punctuations and ignore floating point numbers.\n",
        "  text = re.sub(r'(?<!\\d)\\s*([!\"#$£%&\\'\\(\\)*+,-./:;<=>?@\\[\\]\\\\^_`{|}~])\\s*(?!\\d)',\n",
        "                  r' \\1 ', text)  # It used to also remove commas after numbers like '27,' will be removed\n",
        "\n",
        "  # Remove spaces after sentence end and other unwanted spaces from text\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# custom analyzer for the Tokenizer class\n",
        "def custom_analyzer(text):\n",
        "  '''Custom analyzer to provide to the `Tokenizer` class when creating the tokenizer.\n",
        "\n",
        "  Arguements:\n",
        "    text: str, the text that will be tokenized\n",
        "\n",
        "  Returns:\n",
        "    returns the splitted sentence\n",
        "  '''\n",
        "  # Remove START and END before standardizing\n",
        "  if START_TOKEN in text:\n",
        "    text = re.sub(f'{START_TOKEN} ', '', text)\n",
        "  if END_TOKEN in text:\n",
        "    text = re.sub(f'{END_TOKEN} ', '', text)\n",
        "\n",
        "  # Standardize the text first\n",
        "  text = standardizer(text)\n",
        "\n",
        "  # Add back the START and END tokens\n",
        "  text = ' '.join([START_TOKEN, text, END_TOKEN])\n",
        "\n",
        "  # Split the sentence into words to tokenize\n",
        "  words = text.split(' ')\n",
        "  words = [word.strip() for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "FD2h0OiAxJP_",
        "outputId": "4da7e5cd-a4f5-4ad9-be92-7f53d29ce7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\"I have been working on, \\nbut \\tnever did it in this way.\",\n",
        "                \"U.S won the world cup and bagged 1.78 million dollars.\",\n",
        "                \"India had M.S. Dhoni won made it this far.\",\n",
        "                \"My email address is arupjana7365@gmail.com.\",\n",
        "                \"It can take care of dailymail single opening quote’ also.\",\n",
        "                \"I have 10,000 Rs in my bank\",\n",
        "                \"This sentence has , after a number 12,\",\n",
        "                \"This sentence contains <START> token and <END> token.\"]\n",
        "\n",
        "print(f\"After Standardizing the sample texts:\\n{[standardizer(text) for text in sample_texts]}\")\n",
        "print(f\"After applying custom analyzer on sample texts:\\n{[custom_analyzer(text) for text in sample_texts]}\")"
      ],
      "metadata": {
        "id": "AVi3kZhcLXS7",
        "outputId": "c51909ff-63ab-4a1a-e6df-edf6e6a83f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Standardizing the sample texts:\n",
            "['i have been working on , but never did it in this way .', 'u . s won the world cup and bagged 1.78 million dollars .', 'india had m . s . dhoni won made it this far .', 'my email address is arupjana7365@gmail . com .', \"it can take care of dailymail single opening quote ' also .\", 'i have 10,000 rs in my bank', 'this sentence has , after a number 12,', 'this sentence contains < start > token and < end > token .']\n",
            "After applying custom analyzer on sample texts:\n",
            "[['<START>', 'i', 'have', 'been', 'working', 'on', ',', 'but', 'never', 'did', 'it', 'in', 'this', 'way', '.', '<END>'], ['<START>', 'u', '.', 's', 'won', 'the', 'world', 'cup', 'and', 'bagged', '1.78', 'million', 'dollars', '.', '<END>'], ['<START>', 'india', 'had', 'm', '.', 's', '.', 'dhoni', 'won', 'made', 'it', 'this', 'far', '.', '<END>'], ['<START>', 'my', 'email', 'address', 'is', 'arupjana7365@gmail', '.', 'com', '.', '<END>'], ['<START>', 'it', 'can', 'take', 'care', 'of', 'dailymail', 'single', 'opening', 'quote', \"'\", 'also', '.', '<END>'], ['<START>', 'i', 'have', '10,000', 'rs', 'in', 'my', 'bank', '<END>'], ['<START>', 'this', 'sentence', 'has', ',', 'after', 'a', 'number', '12,', '<END>'], ['<START>', 'this', 'sentence', 'contains', 'token', 'and', 'token', '.', '<END>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to find the tokens from the articles. I need to use only training articles not any other and also I will not use summaries data because that will be my target and I won't know what type of words I will encounter when summarizing the source article. So, the only words that I know will be from the articles of training dataset. Here, I am going to use the `tensorflow.keras.preprocessing.text.Tokenizer` in short `Tokenizer` to find the tokens from the articles and then finally converting the articles into sequence of integers. One thing to remember is here we are going to use `oov_token` arguement of `Tokenizer` to mention the token we want to use for out-of-vocabulary words.\n",
        "\n",
        "When fiting the texts on `tokenizer` make sure to remove floating point and integer numbers using the regex expression - `[+-]?[0-9]*[.]?[0-9]+`. I am making sure that tokenizer does learn the numbers because it can always be taken from the original articles data and we do not to remember them in vocab."
      ],
      "metadata": {
        "id": "IiiWRFR64jTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(texts, num_words, oov_token=None, filters = '#*+/:<=>@[\\\\]/^{|}~\\t\\n'):\n",
        "  '''This will create the tokenizer needed for the task in hand.\n",
        "  The tokenizer will be trained on the `texts`. Tokenizer will have vocabulary length `num_words`.\n",
        "  The `oov_token` will be used as the token represent the out-of-vocabulary words. The `filters` are\n",
        "  the ones which the tokenizer will remove when tokenizing any sentence given to it. The returned\n",
        "  tokenizer is using a custom analyzer that can standardize the sentence before tokenizing using the\n",
        "  `standardizer` function and then splits the sentence into words. After that it tokenizes the sentence.\n",
        "  As for the vocabulary, the returned tokenizer's vocabulary does not contain any number, as I have removed\n",
        "  them before feeding them into `Tokenizer.fit_on_texts` method.\n",
        "\n",
        "  Arguements:\n",
        "    texts: list of strings, the tokenizer will be trained on this strings\n",
        "    num_words: int, number of vocabulary words the tokenizer will consider\n",
        "    oov_token: str, token to represent out-of-vocabulary words\n",
        "    filters: str, all the characters that the tokenizer will remove before tokenizing\n",
        "\n",
        "  Returns:\n",
        "    tokenzier of the `Tokenizer` class after learning vocabulary from `texts`\n",
        "  '''\n",
        "\n",
        "  # Create the tokenizer usinf Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=num_words,\n",
        "                        filters=filters,\n",
        "                        oov_token=oov_token,\n",
        "                        analyzer=custom_analyzer)\n",
        "\n",
        "  # Remove the numbers from the dataset so that tokenizer does not add them inside vocabulary\n",
        "  texts = [re.sub(r\"[+-]?[0-9]*[.]?[0-9]+\", \"\", text) for text in texts]\n",
        "\n",
        "  # Fit the data with fit_on_texts method\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "tR1FD3SESd0G",
        "outputId": "6e59de8e-495d-4709-af3c-8435d6704c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length the articles dataset: {len(list(train_dataset[:, 0]))}\")"
      ],
      "metadata": {
        "id": "UsmErFoxqimM",
        "outputId": "47329d34-ef67-4bdc-9b87-0237d3e85bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length the articles dataset: 80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `tokenizer` using the articles from training dataset by using `train_dataset[:, 0]`, with a vocabulary size of `VOCAB_SIZE` and use `OOV_TOKEN` token to represent out-of-vocabulary words."
      ],
      "metadata": {
        "id": "6KBXcUV2pYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(list(train_dataset[:, 0]), VOCAB_SIZE, OOV_TOKEN)"
      ],
      "metadata": {
        "id": "151rEpqo7Pof",
        "outputId": "564c0558-3c0c-404b-df8a-7ef2115e4b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary for the tokenizer has a length {len(tokenizer.word_index.keys())}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"{OOV_TOKEN} word has index: {tokenizer.word_index[OOV_TOKEN]}\")\n",
        "print(f\"{START_TOKEN} word has index: {tokenizer.word_index[START_TOKEN]}\")\n",
        "print(f\"{END_TOKEN} word has index: {tokenizer.word_index[END_TOKEN]}\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"'teacher' word has index: {tokenizer.word_index['teacher']}\\n\")\n",
        "\n",
        "print(f\"Text:\\n{train_dataset[0, 0]}\\n\\n\")\n",
        "sample_sequence = tokenizer.texts_to_sequences([train_dataset[0, 0]])\n",
        "print(f\"Text to Sequence of the first article:\\n{sample_sequence}\\n\")\n",
        "print(f\"Sequence to Text of the first acrticle:\\n{tokenizer.sequences_to_texts(sample_sequence)}\")"
      ],
      "metadata": {
        "id": "YUNreIjr8Kng",
        "outputId": "96871078-94cb-40a6-b975-4bee568d57f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary for the tokenizer has a length 252445\n",
            "<OOV> word has index: 1\n",
            "<START> word has index: 80\n",
            "<END> word has index: 81\n",
            "'teacher' word has index: 1606\n",
            "Text to Sequence of the first article is [[80, 29, 2, 1, 1, 2, 5, 6104, 5, 24, 6070, 19035, 4, 1, 18, 567, 9, 5973, 25, 1, 11, 105, 316, 29, 2360, 3, 310, 6, 201, 2, 7, 3133, 160, 32, 46, 475, 47, 93, 159, 20, 1160, 25, 1, 11, 105, 316, 6, 201, 8, 120, 577, 6, 142, 19, 278, 97, 37, 1196, 2, 6070, 19035, 4, 1, 32, 46, 654, 22, 592, 10, 3, 201, 9, 6070, 19035, 2649, 4, 41, 1072, 227, 47, 1391, 7, 12205, 9, 2615, 989, 2, 85, 274, 4, 3, 4942, 16, 2701, 30, 25, 121, 5, 13, 112, 10, 3, 1, 2027, 9, 267, 1, 489, 6, 3, 154, 5, 13, 262, 10, 1730, 868, 2, 820, 159, 66, 9, 3, 609, 5, 13, 2615, 5792, 43, 46, 6183, 4, 8, 3, 4942, 68, 43, 70, 1509, 3990, 8, 7, 13474, 1, 4, 2636, 220, 270, 2, 54, 118, 19035, 8, 3, 779, 5, 13, 216, 43, 46, 2223, 29, 93, 4, 3, 413, 5, 13, 1552, 149, 40, 535, 58, 2, 3, 216, 4, 41, 32, 40, 46, 654, 10, 3, 799, 4, 836, 86, 1095, 14, 38, 8, 36, 2005, 136, 272, 6070, 8749, 8, 4915, 2, 131, 6, 3, 271, 4, 3, 609, 849, 7847, 47, 38, 8, 19035, 17969, 17, 25, 2674, 107, 3292, 9501, 2, 3, 1, 11, 105, 296, 4, 273, 4, 1158, 7, 383, 1560, 9, 981, 4, 1306, 93, 14, 20, 16, 398, 6, 115, 3, 121, 897, 125, 20, 1073, 54, 20, 5162, 377, 3, 642, 2, 736, 627, 24, 3, 1919, 8425, 10, 19035, 5, 13, 121, 112, 10, 3, 1, 2027, 9, 267, 1, 489, 2, 1290, 11, 58, 24, 54, 20, 16, 1664, 2223, 29, 93, 4, 118, 19035, 21, 20, 5162, 377, 3, 609, 107, 398, 6, 115, 3, 121, 897, 50, 257, 49, 125, 3, 7771, 2, 2636, 1, 1572, 557, 14, 19035, 1088, 4548, 6, 799, 3, 609, 2, 6088, 272, 6070, 4, 3, 413, 28, 46, 550, 22, 19035, 5, 13, 1, 11, 105, 388, 8, 25, 1566, 5, 13, 113, 154, 30, 7, 958, 817, 2, 66, 180, 154, 34, 763, 2479, 22, 2074, 2, 2934, 159, 35, 1961, 7847, 17, 3, 1212, 5, 13, 2903, 388, 8, 34, 98, 398, 6, 2123, 74, 38, 16, 68, 3529, 29, 36, 296, 2, 118, 19035, 18, 572, 10, 153, 15, 7, 4310, 804, 134, 1, 6726, 3349, 24, 272, 6070, 1072, 227, 26, 3, 154, 5, 13, 262, 30, 71, 820, 603, 23, 6183, 2615, 5792, 8, 13474, 1, 2, 81]]\n",
            "Sequence to Text of the first acrticle is [\"<START> by . <OOV> <OOV> . ' monster ' : samuel cabrera , <OOV> is accused of murdering his <OOV> - old son by beating the child to death . a philadelphia man has been arrested after police say he beat his <OOV> - old son to death and then tried to make it look like an accident . samuel cabrera , <OOV> has been charged with murder in the death of samuel cabrera jr , who passed away after suffering a slew of internal injuries . last tuesday , the toddler was rushed from his family ' s home in the <OOV> block of north <OOV> street to the children ' s hospital in critical condition . doctors say all of the boy ' s internal organs had been crushed , and the toddler also had two broken bones and a ruptured <OOV> , nbc news reported . when mr cabrera and the victim ' s mother had been questioned by police , the couple ' s stories did not match up . the mother , who has not been charged in the killing , allegedly told investigators that she and her boyfriend found little samuel pale and unconscious . according to the woman , the boy suffered bruises after she and cabrera pounded on his chest while performing cpr . the <OOV> - old father , however , offered a different version of events , telling police that he was trying to get the family dog off he coach when he accidentally hit the baby . crime scene : the tragedy unfolded in cabrera ' s family home in the <OOV> block of north <OOV> street . cover - up : when he was initially questioned by police , mr cabrera said he accidentally hit the boy while trying to get the family dog ( pictured ) off the couch . nbc <OOV> sources claimed that cabrera eventually confessed to killing the boy . besides little samuel , the couple have been living with cabrera ' s <OOV> - old daughter and his girlfriend ' s three children from a previous relationship . all four children are currently staying with relatives . detectives say they spotted bruises on the suspect ' s teenage daughter and are now trying to determine if she was also abused by her father . mr cabrera is due in court for a preliminary hearing may <OOV> heartbreaking outcome : little samuel passed away at the children ' s hospital from what doctors described as crushed internal organs and ruptured <OOV> . <END>\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oddness you might see if you are that much familiar with `Tokenizer` class is, even though I have specified that `num_words=VOCAB_SIZE` which is `20,000` still the length of the `word_index` is more that that. Does that mean we are doing something wrong?\n",
        "NO, here although tokenizer computes the word_index of all other words apart from those first 20000 words, it will not use them when we convert them into sequence. Let's look at one example to understand that."
      ],
      "metadata": {
        "id": "crn5t_zk6iBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index.keys())[21000]"
      ],
      "metadata": {
        "id": "qhttL-heeP-P",
        "outputId": "b09c7777-afc5-44f7-f5b9-fcc8c98e9746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'salvaged'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This example is to test the above fact with the word `resounding`\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "S_Dnkcig7SIX",
        "outputId": "fa36767b-0d0b-4c41-dfca-c0a3cc024b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This example is to test the above fact with the word `resounding`\n",
            "\n",
            "\n",
            "Tokenized text: ['this example is to test the above fact with the word ` <OOV> `']\n",
            "Sequence: [[39, 1118, 18, 6, 837, 3, 778, 540, 22, 3, 1291, 14377, 1, 14377]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although `resounding` word was present in the `word_index` mapping still tokenizer represented it with `<OOV>`."
      ],
      "metadata": {
        "id": "xvNeazXb-Yq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add a number 2.1 in this sentence!\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ng5MZ_DZ84kk",
        "outputId": "5f816876-5cdf-4fb3-a50c-dc131f8f1a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add a number 2.1 in this sentence!\n",
            "\n",
            "\n",
            "Tokenized text: ['what happens when i add a number <OOV> in this sentence !']\n",
            "Sequence: [[70, 2057, 54, 27, 1950, 7, 260, 1, 10, 39, 1111, 290]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"What happens when I add parenthesis (I am inside it!).\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"Text: {sample_text}\\n\\n\")\n",
        "print(f\"Tokenized text: {tokenizer.sequences_to_texts(sample_sequence)}\")\n",
        "print(f\"Sequence: {sample_sequence}\")"
      ],
      "metadata": {
        "id": "ImlMHniH-Jnt",
        "outputId": "687117dd-c993-40b1-cab0-9c8abd260c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: What happens when I add parenthesis (I am inside it!).\n",
            "\n",
            "\n",
            "Tokenized text: ['what happens when i add <OOV> ( i am inside it ! ) .']\n",
            "Sequence: [[70, 2057, 54, 27, 1950, 1, 50, 27, 327, 513, 19, 290, 49, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the `tokenizer` to tokenize the articles and summaries. We need to pad those sequences to fit the requirements.\n",
        "\n",
        "In the paper, the articles are limited to have 400 tokens and summary has, 100 tokens at training and 120 tokens for testing.\n",
        "\n",
        "I will be using `pad_sequences` method to pad or truncate the articles and summaries based on their length.\n",
        "\n",
        "NOTE: I am using same tokenizer for article and summary. But, later I might change that to 2 different tokenizers each having different `num_words`."
      ],
      "metadata": {
        "id": "jza9oQYKXB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pad(texts, tokenizer, padding, truncating, maxlen):\n",
        "  '''Tokenize the `texts` using the tokenizer. Then, pad the sequences or truncate the sequences\n",
        "  depending the length. If the length exceeds `maxlen` then it will be truncated and if not then it will be\n",
        "  padded. The padding and truncating can happend at the beginning or at the end of the sequence depending\n",
        "  on the value of `padding` and `truncating` respectively.\n",
        "\n",
        "  Arguements:\n",
        "    texts: list of strings, the sentences to tokenize and pad\n",
        "    tokenizer: Tokenizer class object, helps in tokenizing the `texts`\n",
        "    padding: str, can take 2 values `pre` or `post`. If `pre` then padding will happen at the beginning,\n",
        "    if `post` then padding will happen at the end.\n",
        "    truncating: str, can take 2 values `pre` or 'truncating`, works the same as `padding`\n",
        "    maxlen: int, maximum length after padding or truncating\n",
        "\n",
        "  Returns:\n",
        "    returns the tokenized and padded sentences\n",
        "  '''\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "id": "Alo70WYjW-tA",
        "outputId": "667634be-eafa-406b-d928-8a479d1f762f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts"
      ],
      "metadata": {
        "id": "rttUwCiT4FzJ",
        "outputId": "5012a3d0-7aba-443a-9a93-af239e20e7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have been working on, \\nbut \\tnever did it in this way.',\n",
              " 'U.S won the world cup and bagged 1.78 million dollars.',\n",
              " 'India had M.S. Dhoni won made it this far.',\n",
              " 'My email address is arupjana7365@gmail.com.',\n",
              " 'It can take care of dailymail single opening quote’ also.',\n",
              " 'I have 10,000 Rs in my bank',\n",
              " 'This sentence has , after a number 12,']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_pad(sample_texts, tokenizer, padding=\"post\", truncating=\"post\", maxlen=20)"
      ],
      "metadata": {
        "id": "QNuBKpTq4lZW",
        "outputId": "f36e6873-fd7a-43be-e851-64a54fab8303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   80,    27,    28,    46,   338,    17,     4,    33,   203,\n",
              "          149,    19,    10,    39,   146,     2,    81,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,   114,     2,    13,   298,     3,   101,   399,     8,\n",
              "        16990,     1,   167,  2127,     2,    81,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,  1076,    43,   140,     2,    13,     2, 18665,   298,\n",
              "          126,    19,    39,   328,     2,    81,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    90,  2569,  1225,    18,     1,     2,   612,     2,\n",
              "           81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    19,    67,   157,   361,     9, 18088,   859,  1099,\n",
              "         7709,     5,    68,     2,    81,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    27,    28,     1, 16447,    10,    90,   855,    81,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    39,  1115,    32,     4,    47,     7,   264,     1,\n",
              "           81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [   80,    39,  1115,  3970, 18100,     8, 18100,     2,    81,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Geneator function that generates the source and target for training the model."
      ],
      "metadata": {
        "id": "ZUtzIfXm6UnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example(inputs, targets, input_tokenizer, target_tokenizer, input_len, target_len):\n",
        "  '''Generates examples for the model. Processes the `inputs` and `targets` with their respective\n",
        "  tokenizers and tokenize them to `input_len` and `target_len` length.\n",
        "\n",
        "  Arguements:\n",
        "    inputs: list of input sentences\n",
        "    targets: list of target sentences\n",
        "    input_tokenizer: Tokenizer class object, tokenizer for inputs\n",
        "    target_tokenizer: Tokenizer class object, tokenizer for targets\n",
        "    input_len: int, the length of the tokenization for inputs\n",
        "    target_len: int, the length of the tokenization for targets\n",
        "\n",
        "  Returns:\n",
        "    returns 2 values, a tuple containing 2 numpy arrays (input_tokens, target_tokens[:-1]) and\n",
        "    another numpy array target_tokens[1:]\n",
        "  '''\n",
        "\n",
        "  for inp, tar in zip(inputs, targets):\n",
        "    inp_tokens = tokenize_pad([inp],\n",
        "                              input_tokenizer,\n",
        "                              padding=\"post\",\n",
        "                              truncating=\"post\",\n",
        "                              maxlen=input_len)\n",
        "\n",
        "    tar_tokens = tokenize_pad([tar],\n",
        "                 target_tokenizer,\n",
        "                 padding=\"post\",\n",
        "                 truncating=\"post\",\n",
        "                 maxlen=target_len)\n",
        "\n",
        "    yield (inp_tokens[0], tar_tokens[0][:-1]), tar_tokens[0][1:]"
      ],
      "metadata": {
        "id": "8647M-ey6n19",
        "outputId": "7410d8e8-16f4-4fcc-95c0-a51400e7bc89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example generated by the generator :\")\n",
        "\n",
        "# (inp_art_tokens, inp_sum_tokens), tar_sum_tokens = generate_example(list(train_dataset[:, 0]),\n",
        "example_gen = generate_example(list(train_dataset[:, 0]),\n",
        "                               list(train_dataset[:, 1]),\n",
        "                               input_tokenizer=tokenizer,\n",
        "                               target_tokenizer=tokenizer,\n",
        "                               input_len=MAX_ARTICLE_TOKENS,\n",
        "                               target_len=MAX_SUMMARY_TOKENS)\n",
        "\n",
        "inps, tar = next(example_gen)\n",
        "print(f\"Inputs:\\n{inps[0]}\\n{inps[1]}\\n\\n\")\n",
        "print(f\"Target:\\n{tar}\")"
      ],
      "metadata": {
        "id": "BKKyIeVa5B20",
        "outputId": "5cc755bc-92a0-4980-f769-e5edbbc67161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example generated by the generator :\n",
            "Inputs:\n",
            "[   80    29     2     1     1     2     5  6104     5    24  6070 19035\n",
            "     4     1    18   567     9  5973    25     1    11   105   316    29\n",
            "  2360     3   310     6   201     2     7  3133   160    32    46   475\n",
            "    47    93   159    20  1160    25     1    11   105   316     6   201\n",
            "     8   120   577     6   142    19   278    97    37  1196     2  6070\n",
            " 19035     4     1    32    46   654    22   592    10     3   201     9\n",
            "  6070 19035  2649     4    41  1072   227    47  1391     7 12205     9\n",
            "  2615   989     2    85   274     4     3  4942    16  2701    30    25\n",
            "   121     5    13   112    10     3     1  2027     9   267     1   489\n",
            "     6     3   154     5    13   262    10  1730   868     2   820   159\n",
            "    66     9     3   609     5    13  2615  5792    43    46  6183     4\n",
            "     8     3  4942    68    43    70  1509  3990     8     7 13474     1\n",
            "     4  2636   220   270     2    54   118 19035     8     3   779     5\n",
            "    13   216    43    46  2223    29    93     4     3   413     5    13\n",
            "  1552   149    40   535    58     2     3   216     4    41    32    40\n",
            "    46   654    10     3   799     4   836    86  1095    14    38     8\n",
            "    36  2005   136   272  6070  8749     8  4915     2   131     6     3\n",
            "   271     4     3   609   849  7847    47    38     8 19035 17969    17\n",
            "    25  2674   107  3292  9501     2     3     1    11   105   296     4\n",
            "   273     4  1158     7   383  1560     9   981     4  1306    93    14\n",
            "    20    16   398     6   115     3   121   897   125    20  1073    54\n",
            "    20  5162   377     3   642     2   736   627    24     3  1919  8425\n",
            "    10 19035     5    13   121   112    10     3     1  2027     9   267\n",
            "     1   489     2  1290    11    58    24    54    20    16  1664  2223\n",
            "    29    93     4   118 19035    21    20  5162   377     3   609   107\n",
            "   398     6   115     3   121   897    50   257    49   125     3  7771\n",
            "     2  2636     1  1572   557    14 19035  1088  4548     6   799     3\n",
            "   609     2  6088   272  6070     4     3   413    28    46   550    22\n",
            " 19035     5    13     1    11   105   388     8    25  1566     5    13\n",
            "   113   154    30     7   958   817     2    66   180   154    34   763\n",
            "  2479    22  2074     2  2934   159    35  1961  7847    17     3  1212\n",
            "     5    13  2903   388     8    34    98   398     6  2123    74    38\n",
            "    16    68  3529    29    36   296     2   118 19035    18   572    10\n",
            "   153    15     7  4310]\n",
            "[   80  6070 19035     4     1  1664    86    93    20  5162   377   609\n",
            "   107   398     6   115   897   125     7  7771     2    93  2656    74\n",
            " 19035     5    13     1    11   105   388    68    43    46  3529    29\n",
            "    75     2    81     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "\n",
            "Target:\n",
            "[ 6070 19035     4     1  1664    86    93    20  5162   377   609   107\n",
            "   398     6   115   897   125     7  7771     2    93  2656    74 19035\n",
            "     5    13     1    11   105   388    68    43    46  3529    29    75\n",
            "     2    81     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zj-H_fEh5Zu9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}